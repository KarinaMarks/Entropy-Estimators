\documentclass{article}
\usepackage[utf8]{inputenc}
\title{Literature Review}
\author{Karina Marks}

\usepackage{amsmath, amsfonts, graphicx, listings, booktabs, amstext}
\usepackage[format=plain,
            textfont=it]{caption}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Condition}

\begin{document}
\maketitle
\section{Pre N.N-Estimator}
\subsection{1992 - Entropy Optimization Principles with Applications (J.N.Kapur and H.K.Kesavan) 8}

This gives an overview of entropy, some principles on the optimisation of entropy, the interrelationships among these principles and applications of entropy and these principles.

This paper defines entropy as the probabilistic uncertainty - the uncertainty associated with the probability of outcomes.

Not useful for estimation of entropy, but shows the applications of exact entropies, the entropy optimisation principles considered are;
\begin{itemize}
\item 
\end{itemize}

\subsection{1998 - Limit Theorems for Non-Parametric Sample Entropy Estimators (K-S.Song) 6}

Vasicek's sample entropy estimator
conditions, theorems and proofs of this


\section{Other Estimators}

\subsection{2001 - Nonparametric Entropy Estimation: An Overview (J.Beirlant, E.Dudewicz, L.Gyorfi, E.van der Muelen) 10}

There are several estimation methods for the nonparametric estimation of the Shannon entropy of a continuous random sample. The paper \textit{Nonparametric Entropy Estimation: An Overview} (J.Beirlant, E.Dudewicz, L.Gyorfi, E.van der Muelen, 2001) \cite{paper10}, gives an overview of the properties of these various methods. I will outline a summary below to the types of estimators, which will lead us to understand why we choose the Kozachenko-Leonenko estimator for entropy. 

First, I must set out the types of consistency, so we can see more obviously how it compares to the K-L estimator, for $X_{1}, ..., X_{N}$ a i.i.d sample from the distribution $f(X)$, where $H_{N}$ is the estimator of $H(f)$. Then we have (as $N \to \infty$);
\begin{itemize}
\item Weak Consistency 
\begin{equation}
H_{N} \xrightarrow{p} H(f)
\end{equation}

\item Mean Square Consistency
\begin{equation}
\mathbb{E}\{(H_{N} - H(f))^2\} \to 0
\end{equation}

\item Strong Consistency
\begin{align}
\sqrt{N}(\hat{H}_{N, k} - H) &\xrightarrow{d} N(0, \sigma^2) \\
N \mathbb{E}{(\hat{H}_{N, k} - H)^2} &\xrightarrow{} \sigma^2 \\ \nonumber
\end{align}
This is the type of consistency shown with the K-L estimator in Theorem \ref{efficient}.

\end{itemize}

The types of nonparametric estimators can be split into 3 categories; plug-in estimates, estimates based on sample-spacings and estimates based on nearest neighbour distances. The latter is the Kozachenko-Leonenko estimator, which is the main focus of this paper and will be explored in more detail in section TODO.

The plug-in estimates are based upon a consistent density estimate $f_{N}$, of density $f$, which depends on the sample $X_{1}, ..., X_{N}$, I will consider two of these; the most obvious estimator of this type if the integral estimate of entropy. Given by;
\begin{equation}
H_{N} = - \int_{A_{N}} f_{N}(x) log ( f_{N}(x) )dx
\end{equation}
where the set $A_{N}$ excludes the tail values of $f_{N}$. When the sample is from a 1-dimensional distribution, Dmitriev and Tarasenko, \cite{intest1} for $A_{N} = [-b_{N}, b_{N}]$ and $f_{N}$ the kernel density estimator; proved a strong consistency for this estimator. However, if $f_{N}$ is not estimated in this form, due to the numeric integration, for dimensions $d \geq 2$, Joe \cite{intest2} points out that this estimator is not practical and thus proposed the next plug-in estimator for entropy - the resubstitution estimator.

The resubstitution estimate is of the form;
\begin{equation}
H_{N} = - \frac{1}{N}\sum_{i=1}^{N}  log ( f_{N}(X_{i}) )dx
\end{equation}
which was first proposed in 1976, by Ahmad and Lin \cite{resest1} who showed the mean-square consistency of this estimator, where  $f_{N}$ is a kernel density estimate. Joe \cite{intest2} then went on to obtain the asymptotic bias and variance, and whilst satisfying certain conditions reduced the mean square error. Moreover, Hall and Morton \cite{resest2} went on to say that under more restrictive conditions we have strong consistency for 1-dimensional distributions; however, when $d=2$ the root-n consistent estimator will have significant bias.

There are also two more plug-in estimates discussed in this paper; the splitting data and cross-validation estimates. Where in the first estimator, strong consistency is shown for a general dimension $d$, under some conditions on $f$. And in the latter estimator, strong consistency holds for a kernel estimate of $f$ and for other estimates of $f$ under some conditions we have root-n consistency when $1 \leq d \leq 3$.

Thus, so far the estimates for entropy looked at are only consistent whilst under strong conditions on $f$ and $f_{N}$ and mostly for a 1-dimensional distribution. So it is important to look at the next category of estimates - estimates of entropy based on sample-spacings; namely the m-spacing estimate. Sometimes it in not practical to estimate $f_{N}$, so this estimate is found based on spacings between the sample observations. 

This estimator is only defined for samples of 1-dimension, where we assume $X_{1}, ..., X_{N}$ are an i.i.d sample, and let $X_{N, 1} \leq X_{N, 2} \leq ... \leq X_{N, N}$ be the corresponding ordered sample, then $X_{N, i+m} - X_{N, m}$ is the m-spacing.

Firstly we look at this estimator of the form, with fixed $m$;
\begin{equation}
H_{m, N} = \frac{1}{N} \sum_{i=1}^{N-m} log \left(\frac{N}{m} (X_{N, i+m} - X_{n, i}) \right) - \Psi(m) + log(m)
\end{equation}
where $\Psi(x)$ is the digamma function - more detailed explanation in section \ref{TODO}. For a sample from a uniform distribution this estimator has been shown to be consistent; proved by Tarasenko \cite{spacest1}. Under some conditions on $f$, on its boundedness, the weak consistency and asymptotic normality was shown by Hall \cite{spacest2}.

To decrease the asymptotic variance of the estimator, we consider the estimator when $m_{N} \to \infty$, which is defined slightly differently;
\begin{equation}
H_{m, N} = \frac{1}{N} \sum_{i=1}^{N-m_{N}} log \left(\frac{N}{m_{N}} (X_{N, i+m_{N}} - X_{n, i}) \right)
\end{equation}
for this estimator the weak and strong consistencies are proved under the assumption that as $N \to \infty$, $m_{N} \to \infty$ and $\frac{m_{N}}{N} \to 0$, for densities with bounded support.








\section{N.N Entropy Estimator}

\subsection{1987 - Problems of Information Transmission - On statistical estimation of entropy of random vector (N.Leonenko) 1}

This paper is a primary view at the estimator and at how conditions for asymptotic unbiasedness and consistency of the estimator can be established. Here we are looking at estimating the entropy of an absolutely continuous random sample of independent observations, with unknown probability density $f(x), x \in \mathbb{R}^{d}$, given by;
\begin{equation}
H = - \int_{\mathbb{R}^{d}} f (x) \log f(x) dx < \infty
\end{equation}
As $f(x)$ is unknown this is not easily estimated for a random sample, and by just estimating the density $\hat{f}(x)$ to replace the actual density $f(x)$ in the formula for the entropy we get highly restrictive consistency conditions. Thus the following simple estimator for entropy was proposed;
\begin{equation}
H_{N} = d \log(\bar{\rho } ) + \log (c(d)) + \log (\gamma) + \log (N-1)
\end{equation}
where the sample $X_{1}, X_{2}, ..., X_{N}$, $N\geq 2$ is taken from the space $\mathbb{R}^{d}, d \geq 1$ and we have the following defined;
\begin{itemize}
\item Metric $\rho (x_{1}, x_{2}) = \left[ \sum_{j=1}^{d} (x_{1}^{(j)} - x_{2}^{(j)} )^{2} \right]^{\frac{1}{2}}$, where $x = (x^{(1)}, x^{(2)}, ..., x^{(d)}) \in \mathbb{R}^{d}$
\item Volume of the d-dimensional unit ball, $v(y, r) = \{ x \in \mathbb{R}^{d} : \rho (x, y) < r \}$, is given by $c(d) = |v(y, r)| = \frac{\pi^{\frac{d}{2}}}{\Gamma ( \frac{d}{2} + 1 )}$
\item $\bar{\rho} = \left[ \prod_{i=1}^{N} \rho_{i} \right]^{\frac{1}{N}}$, where $\rho_{i} = min(\rho (X_{i}, X_{j}) : j \in \{1, 2, ..., N\} \backslash \{i\})$
\item Euler constant $\gamma = \exp \left[ - \int_{0}^{\infty} e^{-t} \log(t) dt \right]$
\end{itemize}

Henceforth, here Leonenko establishes some conditions under which the estimator is asymptotically unbiased and consistent. The two main results about this estimator are, as follows;
\begin{theorem} \label{paper1_T1}
For exact entropy $H$, Kozachenko-Leonenko estimator $\hat{H}_{N,k}$, and density function $f(x)$, for some $\epsilon > 0$ if both
\begin{equation} \label{paper1_T1_eq1}
\int_{\mathbb{R}^{d}} | log(f(x))|^{1 + \epsilon} f(x) dx < \infty 
\end{equation}
and
\begin{equation} \label{paper1_T1_eq2}
\int_{\mathbb{R}^{d}} \int_{\mathbb{R}^{d}} | log(\|x-y\|)|^{1+ \epsilon} f(x) f(y) dx dy < \infty
\end{equation}
Then we have that 
\begin{equation} 
\lim_{N \to \infty} \mathbb{E} (\hat{H}_{N, k}) = H \nonumber
\end{equation}
Thus $\hat{H}_{N, k}$ is an asymptotically unbiased estimator of $H$.
\end{theorem}

\begin{theorem} \label{paper1_T2}
For exact entropy $H$, Kozachenko-Leonenko estimator $\hat{H}_{N,k}$, and density function $f(x)$, for some $\epsilon > 0$ if both
\begin{equation} \label{paper1_T2_eq1}
\int_{\mathbb{R}^{d}} | log(f(x))|^{2 + \epsilon} f(x) dx < \infty \nonumber
\end{equation}
and
\begin{equation} \label{paper1_T2_eq2}
\int_{\mathbb{R}^{d}} \int_{\mathbb{R}^{d}} | log(\|x-y\|)|^{2+ \epsilon} f(x) f(y) dx dy < \infty \nonumber
\end{equation}
Then $\hat{H}_{N, k}$ for $N \to \infty$ is a consistent estimator of H.

(An estimator is consistent if the probability that it is in error by more than a given amount tends to zero as the sample become large $\Leftrightarrow $ for error $\delta > 0$, we have $\lim_{N \to \infty} \mathbb{P}(|\hat{H}_{N,k} - H| < \delta) = 1$) 
\end{theorem}

In this paper, the estimator is in its simplest form, which is later developed into something more sophisticated, using the nearest neighbour method, where the consistency and asymptotic unbias of the estimator holds under less constrained conditions.


\subsubsection{Summary - paper 1}

This paper looks at estimating the Shannon entropy of an absolutely continuous random sample of independent observations, with unknown probability density $f(x), x \in \mathbb{R}^{d}$. As $f(x)$ is unknown this is not easily estimated accurately for a random sample, and by just estimating the density $\hat{f}(x)$ to replace the actual density $f(x)$ in the formula for the entropy we get highly restrictive consistency conditions. 

Therefore, the following estimator was proposed for the Shannon entropy of a random sample $X_{1}, X_{2}, ..., X_{N}$ of d-dimensional observations;
\begin{equation}
H_{N} = d \log(\bar{\rho } ) + \log (c(d)) + \log (\gamma) + \log (N-1)
\end{equation}
where $c(d) = \frac{\pi^{\frac{d}{2}}}{\Gamma ( \frac{d}{2} + 1 )}$ is the volume of the d-dimensional unit ball, the Euler constant is $\gamma = \exp \left[ - \int_{0}^{\infty} e^{-t} \log(t) dt \right]$ and $\bar{\rho} = \left[ \prod_{i=1}^{N} \rho_{i} \right]^{\frac{1}{N}}$, with $\rho_{i}$ the nearest neighbour distance from $X_{i}$ to another member of the sample $X_{j}$, $i \neq j$. 

Under some strong conditions on the density function, this estimator is asymptotically unbiased and a consistent estimator for the Shannon entropy. 

The estimator here is in a simple form, which is later developed into something more sophisticated, using the nearest neighbour method, but considering larger values of $k$ (here $k=1$). This estimator is developed so that the consistency and asymptotic unbias of the estimator holds under less constrained conditions.




 


\subsection{2006 - Causality Detection Based on Information-Theoretic Approaches in Time Series Analysis (K.H-Schindler, M.Palus, M.Vejmelka, J.Bhattacharya) 7}

considers different types of estimators including NN






\subsection{2007 - a Class of Renyi Information Estimators for Mulitdimensional densities (N.Leonenko, L.Pronzato, V.Savani) 2}

(Shows that entropy can be estimated consistently with minimal assumptions on the density of the distribution, $f$. Moreover, this can be extended to estimate the statistical difference between two distributions using one i.i.d sample from each.)

This paper looks at estimating both the R\'enyi and Tsallis entropies for a random vector $X \in \mathbb{R}^d$ with density function $f$, defined as;

R\'enyi entropy
\begin{equation}
H_{q}^{*} = \frac{1}{1-q} log(I_{q}) \quad  \quad (q \neq 1) \label{RenEnt} 
\end{equation}

Tsallis entropy
\begin{equation}
H_{q} = \frac{1}{q-1} (1 -  I_{q}) \quad  \quad (q \neq 1) \label{TsaEnt} 
\end{equation}
where in both the above $I_{q} = \int_{\mathbb{R}^d} f^q (x) dx$.

When the order of the entropy $q \to 1$, both the R\'enyi, (\ref{RenEnt}), and Tsallis, (\ref{TsaEnt}), entropies tend to the Shannon entropy. The Shannon entropy is a special case for when $q=1$, defined by;
\begin{equation} 
H = - \int_{x : f(x) > 0} f(x) log(f(x)) dx \label{ShaEnt} 
\end{equation} 

For $q \neq 1$, the construction of the estimator of entropy relies upon the estimation of the integral $I_{q}$, which is given by;
\begin{equation}
\hat{I}_{N, k, q} = \frac{1}{N} \sum_{i=1}^{N} (\zeta_{N, k, q})^{1-q}
\end{equation}
where we have taken a sample $X_{1}, X_{2}, ..., X_{N}$ with all $X_{i} \in \mathbb{R}^{d}$ , and k is the size of the nearest neighbour method to be used. We have also defined;
\begin{itemize}
\item $\zeta_{N, k ,q} = (N-1)C_{k}V_{d}(\rho_{k, N-1}^{(i)})^d$
\item Volume of d-dimensional unit ball $V_{d} = \frac{\pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2} + 1 )}$
\item $C_{k} = \left[ \frac{\Gamma(k)}{\Gamma(k+1-q)} \right]^{\frac{1}{1-q}}$
\item $\rho_{k, N-1}^{(i)}$ is the kth nearest neighbour distance from the observation $X_{i}$ to some other $X_{j}$
\end{itemize}

Thus, here it is also shown that the estimator $\hat{I}_{N, k, q}$ for $I_{q}$ is asymptotically unbiased and consistent, given certain conditions;
\begin{theorem} \label{paper2_T1}
The estimator $\hat{I}_{N, k, q}$, given above satisfies;
\begin{equation}
\mathbb{E} (\hat{I}_{N, k, q}) \to I_{q}, \quad \quad (N \to \infty)
\end{equation}
for $q > 1$ provided that $I_{q}$ exists, and for any $q \in (1, k+1)$ if f is bounded.
Thus, $\hat{I}_{N, k, q}$ is an asymptotically unbiased estimator for $I_{q}$.
\end{theorem}

\begin{theorem} \label{paper2_T2}
The estimator $\hat{I}_{N, k, q}$, given above satisfies;
\begin{equation}
\hat{I}_{N, k, q} \to^{L_{2}} I_{q}, \quad \quad (N \to \infty)
\end{equation}
(and thus $\hat{I}_{N, k, q} \to^{p} I_{q},  (N \to \infty)$) for $q>1$, provided that $I_{2q-1}$ exists and for any $q \in (1, \frac{k+1}{2})$ when $ k \geq 2$ if f is bounded.
Thus, $\hat{I}_{N, k, q}$ is a consistent estimator for $I_{q}$.
\end{theorem}

Therefore, under the conditions of Theorem \ref{paper2_T2} we have the estimators for the following entropies;

R\'enyi entropy
\begin{equation}
\hat{H}_{q}^{*} = \frac{1}{1-q} log(\hat{I}_{N, k, q}) \quad  \quad (q \neq 1) \label{RenEnt} 
\end{equation}

Tsallis entropy
\begin{equation}
\hat{H}_{q} = \frac{1}{q-1} (1 - \hat{I}_{N, k, q}) \quad  \quad (q \neq 1) \label{TsaEnt} 
\end{equation}
which are both consistent estimators of the R\'enyi and Tsallis entropies of the sample.

Moreover, this paper also goes on to discuss the estimator for the Shannon entropy, given by;
\begin{equation} \label{ShaEntEst}
\hat{H}_{N, k, 1} = \frac{1}{N} \sum_{i=1}^{N} \log (\xi_{N, i, k})
\end{equation}
where we have taken a sample $X_{1}, X_{2}, ..., X_{N}$ with all $X_{i} \in \mathbb{R}^{d}$ , and k is the size of the nearest neighbour method to be used. We have also defined;
\begin{itemize}
\item $\xi_{N, i, k} = (N-1)\exp[-\Psi(k)]V_{d}(\rho_{k, N-1}^{(i)})^{d}$
\item $V_{d}$ and $\rho_{k, N-1}^{(i)}$ are as defined above
\item Digamma function $\Psi(z) = \frac{\Gamma'(z)}{\Gamma(z)}$
\end{itemize}
Then, we also have the following Theorem, for the Shannon entropy;
\begin{theorem} \label{paper2_T3}
Suppose that f is bounded and that $I_{q_{1}}$ exists for some $q_{1} < 1$. Then $H_{1}$ exists and the estimator \ref{ShaEntEst} satisfies $\hat{H}_{N, k, 1} \to^{L_{2}} H_{1}$ as $N \to \infty$.
\end{theorem}

In this paper, we consistently consider a fixed value of $k$, and assume that $q$ depends upon this $k$ for Theorems \ref{paper2_T1} and \ref{paper2_T2} to hold. I wish to examine whether or not a fixed value of $k$ is appropriate for this estimator.

\subsubsection{Summary - paper 2}

This paper looks at estimating both the R\'enyi ($H_{q}^{*}$) and Tsallis ($H_{q}$) entropies for a random vector $X \in \mathbb{R}^d$ with density function $f(x)$, when $q \neq 1$, by using the kth nearest neighbour method, with a fixed values of k. This is achieved by considering the integral  $I_{q} = \int_{\mathbb{R}^d} f^q (x) dx$, and generating its estimator, which is defined as $\hat{I}_{N, k, q} = \frac{1}{N} \sum_{i=1}^{N} (\zeta_{N, k, q})^{1-q}$. Where, $\zeta_{N, k ,q} = (N-1)C_{k}V_{d}(\rho_{k, N-1}^{(i)})^d$,  $V_{d} = \frac{\pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2} + 1 )}$ is the volume of d-dimensional unit ball, $C_{k} = \left[ \frac{\Gamma(k)}{\Gamma(k+1-q)} \right]^{\frac{1}{1-q}}$ and $\rho_{k, N-1}^{(i)}$ is the kth nearest neighbour distance from the observation $X_{i}$ to some other $X_{j}$.

The estimator $\hat{I}_{N, k, q}$, provided $q>1$ and $I_{q}$ exists - and for any $q \in (1, k+1)$ if f is bounded - is thus found to be an asymptotically unbiased estimator for $I_{q}$. Also, provided  $q>1$ and $I_{2q-1}$ exists -  and for any $q \in (1, \frac{k+1}{2})$, when $k \geq 2$ if f is bounded - $\hat{I}_{N, k, q}$ is thus a consistent estimator for $I_{q}$. Moreover, by simple formulas both the R\'enyi and Tsallis entropies can be written in terms of this estimated value; 
\begin{align}
\hat{H}_{q}^{*} &= \frac{1}{1-q} log(\hat{I}_{N, k, q}) \\
\hat{H}_{q} &= \frac{1}{q-1} (1 - \hat{I}_{N, k, q})
\end{align}
thus, under the latter conditions, provide consistent estimates of these entropies as $N \to \infty$.

Furthermore, this paper goes on to discuss an estimator for the Shannon entropy, $H_{1}$ by taking the limit of the estimator for the Tsallis entropy, $\hat{H}_{N, k, q}$ as $q \to 1$, again with a fixed value of $k$. This estimator is given by $\hat{H}_{N, k, 1} =  \frac{1}{N} \sum_{i=1}^{N} \log (\xi_{N, i, k})$, where $\xi_{N, i, k} = (N-1)\exp[-\Psi(k)]V_{d}(\rho_{k, N-1}^{(i)})^{d}$, with $V_{d}$ and $\rho_{k, N-1}^{(i)}$ defined as in the estimation of $I_{q}$ and the Digamma function $\Psi(z) = \frac{\Gamma'(z)}{\Gamma(z)}$. Under the following conditions; f is bounded, $I_{q_{1}}$ exists for some $q_{1} > 1$; the $H_{1}$ exists and the estimator $\hat{H}_{N, k, 1}$ is a consistent estimator for the Shannon entropy.




\subsection{2009 - Statistical Inference of $\epsilon$ -Entropy and the Quadratic Renyi Entropy (N.Leonenko, O.Seleznjev) 5}

Show asymptotic properties of the nn-estimator an the quadratic one?

\subsection{Feb 2016 - On Kozachenko-Leonenko Entropy Estimator (S.Delattre, N.Fournier) 3}

Studies the bias (which is found to be $O(N^{\frac{-2}{d}})$, for dimension $d$) and variance of the estimator, prove CLT for $d=1,2$ and find explicit asymptotic confidence intervals.














\subsection{Jul 2016 - Efficient Multivariate Entropy Estimation via k-Nearest Neighbour Distances (T.B.Berrett, R.J.Samworth, M.Yuan) 4} THIS ONE

Under certain conditions (considering k dependent of n) the k-nearest neighbour estimator only works for dimension $d \leq 3$, for a higher dimension gives problems. They introduced a new estimator which is formed as a weighted average of k-nearest neighbour estimators for different values of k.

They also show that the bias of the K-L estimator is dependent on $\alpha$ and $\beta$ from certain conditions which imply for $d \leq 3$,  that the bias is $O\left(max \left\{ \frac{k^{\frac{\alpha }{\alpha + d} + \epsilon}}{n^{\frac{\alpha}{\alpha + d} + \epsilon}}, \frac{k^{\frac{\beta}{d}}}{n^{\frac{\beta}{d}}} \right\} \right)$.



From paper 4 (TODO reference), we have the following, conditions which will be used in Theorems \ref{paper4_T4} and \ref{paper4_T5};
\begin{remark} ($\beta$) \label{A1}
For density $f$ bounded, denoting $m := \lfloor \beta \rfloor$ and $\eta := \beta -m$, we have that $f$ is $m$ times continuously differentiable and there exists $r_{*} > 0$ and a Borel measurable function $g_{*}$ such that for each $t = 1, 2, ... , m$ and $\|y-x\| \leq r_{*}$, we have;
\begin{equation}
\| f^{(t)} (x) \| \leq g_{*}(x)f(x) \nonumber
\end{equation},
\begin{equation}
\| f^{(m)} (y) - f^{(m)} (x) \| \leq g_{*}(x)f(x) \|y-x\|^{\eta} \nonumber
\end{equation}
and $sup_{x:f(x)\geq \delta} g_{*}(x) = o(\delta^{-\epsilon})$ as $\delta \downarrow 0$, for each $\epsilon > 0$.
\end{remark}

\begin{remark} ($\alpha$) \label{A2}
For density $f(x)$ and dimension $d$, we have;
\begin{equation}
\int_{\mathbb{R}^{d}} \| x \|^{\alpha} f(x) dx < \infty \nonumber
\end{equation}
\end{remark}

\begin{remark} \label{A3}
Assume that condition \ref{A1} holds for $\beta = 2$ and condition \ref{A2} holds for some $\alpha > d$. Let $k_{0}^{*} = k_{0, N}^{*}$ and $k_{1}^{*} = k_{1, N}^{*}$ denote two deterministic sequences of positive integers with $k_{0}^{*} \leq k_{1}^{*}$, with $\frac{k_{0}^{*}}{\log^{5}{N}} \to \infty$ and with $k_{1}^{*} = O(N^{\tau})$, where
\begin{equation}
\tau < \min \left\{ \frac{2 \alpha}{5 \alpha + 3d} , \frac{\alpha - d}{2 \alpha} , \frac{4}{4 + 3d} \right\} \nonumber
\end{equation}
\end{remark}

\begin{theorem} \label{paper4_T1}
Assume that conditions \ref{A1} and \ref{A2} hold for some $\beta, \alpha > 0$. Let  $k^{*} = k_{N}^{*}$ denote a deterministic sequence of positive integers with $k^{*} = O(N^{1-\epsilon})$ as $N \to \infty$  for some $\epsilon > 0$. Then, for $d \leq 2$ (or $d \geq 3$) with either $\beta \leq 2$ or $\alpha \in (0, \frac{2d}{d-2})$, then for every $\epsilon >0$ we have;
\begin{equation}
\mathbb{E} ( \hat{H}_{N} ) - H = O \left( max \left\{ \frac{k^{\frac{\alpha}{\alpha + d} - \epsilon}}{N^{\frac{\alpha}{\alpha + d} - \epsilon}}, \frac{k^{\frac{\beta}{d}}}{N^{\frac{\beta}{d}}} \right\} \right)
\end{equation}
uniformly for $k \in \{ a, ..., k^{*}\}$, as $N \to \infty$.
\end{theorem}


\begin{theorem}\label{paper4_T4}
Assume that condition \ref{A1} holds for $\beta = 2$ and condition \ref{A2} holds for some $\alpha > d$, then by condition \ref{A3}, for the estimator $\hat{H}_{N, k}$ we have;
\begin{equation}
Var(\hat{H}_{N, k}) = \frac{Var(\log f(x))}{N} + o(\frac{1}{N}) \nonumber
\end{equation}
as $N \to \infty$, uniformly for $k \in \{ k_{0}^{*}, ...,  k_{1}^{*} \}$.
\end{theorem}

\begin{theorem} \label{paper4_T5}
Assume that $d \leq 3$ and the conditions of Theorem \ref{paper4_T4} are satisfied (where if $d=3$, we additionally assume $k_{1}^{*} = o(n^{\frac{1}{4}})$. Then, for the estimator $\hat{H}_{N, k}$ we have;
\begin{equation} \label{est_dist}
\sqrt{N}(\hat{H}_{N, k} - H) \xrightarrow{d} N(0, \sigma^2)
\end{equation}
and 
\begin{equation} \label{est_consist}
N \mathbb{E}{(\hat{H}_{N, k} - H)^2} \xrightarrow{} \sigma^2
\end{equation}
as $N \to \infty$ uniformly for $k \in \{ k_{0}^{*}, ...,  k_{1}^{*} \}$, where $\sigma^2 = Var(log(f(x))$, for density function $f(x)$.

(This combines Theorem \ref{paper1_T1} with stronger conditions; hence we can now say that $\hat{H}_{N, k}$ is an consistent and asymptotically unbiased estimator of exact entropy $H$.)
\end{theorem}

\subsubsection{Summary - paper 4}

The last main paper, whose results I will be exploring is \textit{Efficient Multivariate Entropy Estimation via k-Nearest Neighbour Distances} (T.Berrett, R.Samworth, M.Yuan, 2016), which initially studies the K-L estimator, and the conditions under which it is efficient and asymptotically unbiased (for a value of $k$ depending on the sample size $N$). 

Considering dimensions $d \leq 3$, and a sample size $N$ from distribution with density $f(x)$, they defined the k-nearest neighbour estimator of entropy - just as in section \ref{fixed_k} - to be;
\begin{equation}
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(k),i}^{d} V_{d} (N-1)}{e^{\Psi(k)}} \right]
\end{equation}
where $\rho_{(k),i}$, $V_{d}$ and $\Psi(k)$ are all defined as in the 2007 paper. However, the difference here is in the conditions under which the estimator is consistent and asymptotically unbiased.

Here, some conditions on the finiteness of the $\alpha$ moment of $f$ and the continuity and differentaibility of $f$ are proposed, with $k \in \{1, ..., O(N^{1-\epsilon})\}$, for some $\epsilon > 0$, we have asymptotic unbias of the estimator; where the bias can be expressed as;
\begin{equation}
\mathbb{E} ( \hat{H}_{N} ) - H = O \left( max \left\{ \frac{k^{\frac{\alpha}{\alpha + d} - \epsilon}}{N^{\frac{\alpha}{\alpha + d} - \epsilon}}, \frac{k^{\frac{\beta}{d}}}{N^{\frac{\beta}{d}}} \right\} \right) \quad \quad N \to \infty
\end{equation}

Also, they considered the asymptotic normality of the estimator, given the $\alpha$ moment of $f$ is finite (for $\alpha > d$), and some conditions on the continuity and differentaibility of $f$ hold and with $k \in \{k_{0}, ..., k_{1}\}$. Then the variance of the estimator is given by;
\begin{equation}
Var(\hat{H}_{N, k}) = \frac{\sigma^2}{N} + o(\frac{1}{N})
\end{equation}
as $N \to \infty$, where $\sigma^2 = Var(log(f(x))$, and we define $k_{0}, k_{1}$ such that $\frac{k_{0}}{log^5(N)} \to \infty$ and $k_{1} = O(N^{\tau})$, where $\tau < \min \left\{ \frac{2 \alpha}{5 \alpha + 3d} , \frac{\alpha - d}{2 \alpha} , \frac{4}{4 + 3d} \right\}$. 

Moreover, T.Berrett, R.Samsworth and M.Yuan also go on to show that a consequence of the variance, given the dimension of the sample $d \leq 3$, with the same conditions, we have the asymptotic normality;
\begin{equation}
\sqrt{N}(\hat{H}_{N, k} - H) \xrightarrow{d} N(0, \sigma^2)
\end{equation}
and 
\begin{equation} 
N \mathbb{E}{(\hat{H}_{N, k} - H)^2} \xrightarrow{} \sigma^2
\end{equation}
where the estimator is asymptotically efficient and the asymptotic variance here is the best possible.

It is important to note that for higher dimensions ($d > 3$), these results do not necessarily hold; since I am just considering the specific dimensions $d=1$ and $d=2$, there is no need to detail this. However, they do then go on to discuss a more appropriate estimator for higher dimensions, given sufficient smoothness, which is efficient in arbitrary dimensions, which was previously mentioned in section TODO.


\subsection{Aug 2016 - Demystifying Fixed k-Nearest Neighbour Information Estimators (W.Gao, S.Oh, P.Viswanath) 9}

Now considering k is independent of n, the kl estimator has bias  $O(N^{\frac{-1}{d}})$



\end{document}