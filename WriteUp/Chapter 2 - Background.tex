\documentclass{article}
\usepackage[utf8]{inputenc}
\title{Chapter 2 - Background}
\author{Karina Marks}

\usepackage{amsmath, amsfonts, graphicx, listings, booktabs, amstext}
\usepackage[format=plain,
            textfont=it]{caption}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Condition}

\begin{document}
\maketitle


\section{Properties of Entropy} \label{entropyProperties}

Kapur and Kesavan's book on \textit{Entropy Optimization Principles with Applications} \cite{paper8}, gives an account of some of the properties of the Shannon entropy $H$, which I will mention. First recall the definition of Shannon entropy (equation (\ref{ShaEnt});
\begin{equation}
H = - \int_{x : f(x) > 0} f(x) log(f(x)) dx \nonumber
\end{equation}
where $f$ is the density of the distribution of $x$. Some properties are as follows; 
\begin{itemize}
\item $H$ is permutationally symmetric
\item For $f(x)$ continuous on some interval; H is also continuous everywhere in the same interval
\item Entropy doesn't change by the inclusion of an impossible event
\item $H > 0$ for all circumstances unless if $f$ is any of the $N$ degenerate distributions; where $f(x_{i}) = 1$ if $i = k, k \in [1, N]$ otherwise $f(x_{i}) = 0$, then $H=0$
\item H is a concave function, \textit{for R\'enyi and Tsallis entropies, when $q > 0$ they're concave; however, if $q<0$ then they're convex}
\item The maximum value of $H$ is attained when $f$ is the uniform distribution
\item For two independent distributions ($f_{X}(x)$ and $f_{Y}(y)$), the entropy of their joint distribution ($f_{X,Y}(x,y)$) is just the sum of the entropies of the two distributions; $H(f_{X,Y}) = H(f_{X}) + H(f_{Y})$
\end{itemize}


\section{Applications of Entropy}

Entropy began as a concept in thermodynamics, about the idea of that within any irreversible system, a small amount of heat energy is aways lost. Entropy has more recently found application in the field of information theory, where it describes a similar loss, this time of missing information or data in systems of information transmission. Thus, entropy has many applications across both these areas.

I will be concentrating on Shannon entropy - also mentioning R\'enyi and Tsallis entropies - which concern information theory; therefore the application thus considered are appropriate to this. I will give a short overview of some of its applications; however, this is not an exhaustive list, since the application of entropy are extensive.

Wikipedia gives an appropriate overview of the applications, that the estimation of Shannon entropy is useful in ''various science/engineering applications, such as independent component analysis, image analysis, genetic analysis, speech recognition, manifold learning, and time delay estimation'' \cite{wiki1}.

Independent component analysis (ICA), in signal processing, is a computational method for decomposing large, often very complex, multivariate data to find underlying/hidden factors or components. The computation of ICA depends on knowing the entropy of the sample; and in most cases this must be estimated, as an exact entropy is not always known. Kraskov, St\"{o}gbauer and Grassberger \cite{ICA1} discussed how estimating the mutual information (MI) using entropy estimators is useful for assessing the independence of components from ICA. Learned-Miller and Fisher \cite{ICA2} also presented another example of how to use estimation of entropy to obtain a new algorithm for the ICA problem. 

Image analysis is the investigation of an image and the extraction of useful information. Hero and Michel \cite{IM2} first discuss the applications of R\'enyi entropy in image processing, then Neemuchwala, Hero and Carson \cite{IM1} discuss how in image analysis an important task is that of image retrieval, which uses entropy estimation to compute entropic similarities that are used to match a reference image to another image.  Moreover Du, Wang, Guo and Thouin, \cite{IM3} considered the importance of entropy-based image thresholding; using both Shannon and relative entropy.

Genetic analysis is the study and research of genes and molecules to find information on biological systems. Statistical analysis of specific cells can help us understand how genomic entropy can help diagnose diseases and cancers. Wieringen and Vaart, \cite{gen1} discuss how chromosomal disorganisation increases as cancer progresses, they mention how the K-L estimator can be used to help find this disorganisation/entropy; thus finding that "as cancer evolves, and the genomic entropy increases, the transcriptomic entropy is also expected to surge". 

''Speech recognition (SR) is the inter-disciplinary sub-field of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers'' \cite{wiki2}. Shen, Hung and Lee \cite{speech1} discuss how an entropy based algorithm can conduct accurate SR in noisy environments. Moreover, Kuo and Gao \cite{speech2} focus on a method where the probability of a state or word sequence given an observation sequence is computed directly from the maximum entropy direct model.

There are many more applications of entropy, it is important to note the statistical applications of entropy; there are some tests on goodness-of-fit established by the estimation of entropy. Vasicek explored the test for normality; that its entropy exceeds that of any other distributions with the same variance \cite{stat1}. Dudewicz and van der Meulen \cite{stat2} discussed the property mentioned in section \ref{entropyProperties}, that the uniform distribution maximises the entropy. Moreover, others have explored different distributions and their entropic properties; see \cite{stat3, stat4}
 



\section{Other Estimators of Entropy} \label{otherest}

There are several estimation methods for the nonparametric estimation of the Shannon entropy of a continuous random sample. The paper \textit{Nonparametric Entropy Estimation: An Overview} (J.Beirlant, E.Dudewicz, L.Gyorfi, E.van der Muelen, 2001) \cite{paper10}, gives an overview of the properties of these various methods. Also, the paper \textit{Causality detection based on information-theoretic approaches in time series analysis} (K.Hlav\'{a}\v{c}kov\'{a}, M.Palu\v{s}, M.Vejmelka, and J.Bhattacharya, 2007) gives a more detailed look into these different types of estimators. I will outline a summary below to the types of estimators, which will lead us to understand why we choose the Kozachenko-Leonenko estimator for entropy. 

First, I must set out the types of consistency, so we can see more obviously how it compares to the K-L estimator, for $X_{1}, ..., X_{N}$ a i.i.d sample from the distribution $f(X)$, where $H_{N}$ is the estimator of $H(f)$. Then we have (as $N \to \infty$);
\begin{itemize}
\item Weak Consistency 
\begin{equation}
H_{N} \xrightarrow{p} H(f)
\end{equation}

\item Mean Square Consistency
\begin{equation}
\mathbb{E}\{(H_{N} - H(f))^2\} \to 0
\end{equation}

\item Strong Consistency
\begin{align}
\sqrt{N}(\hat{H}_{N, k} - H) &\xrightarrow{d} N(0, \sigma^2) \\
N \mathbb{E}{(\hat{H}_{N, k} - H)^2} &\xrightarrow{} \sigma^2 \\ \nonumber
\end{align}
This is the type of consistency shown with the K-L estimator in Theorem \ref{efficient}.

\end{itemize}

The types of nonparametric estimators can be split into 3 categories; plug-in estimates, estimates based on sample-spacings and estimates based on nearest neighbour distances. The latter is the Kozachenko-Leonenko estimator, which is the main focus of this paper and will be explored in more detail in section \ref{motivation}.

The plug-in estimates \cite{paper10}, \cite{paper7} are based upon a consistent density estimate $f_{N}$, of density $f$, which depends on the sample $X_{1}, ..., X_{N}$, I will consider two of these; the most obvious estimator of this type if the integral estimate of entropy. Given by;
\begin{equation}
H_{N} = - \int_{A_{N}} f_{N}(x) log ( f_{N}(x) )dx
\end{equation}
where the set $A_{N}$ excludes the tail values of $f_{N}$. When the sample is from a 1-dimensional distribution, Dmitriev and Tarasenko, \cite{intest1} for $A_{N} = [-b_{N}, b_{N}]$ and $f_{N}$ the kernel density estimator; proved a strong consistency for this estimator. However, if $f_{N}$ is not estimated in this form, due to the numeric integration, for dimensions $d \geq 2$, Joe \cite{intest2} points out that this estimator is not practical and thus proposed the next plug-in estimator for entropy - the resubstitution estimator.

The resubstitution estimate is of the form;
\begin{equation}
H_{N} = - \frac{1}{N}\sum_{i=1}^{N}  log ( f_{N}(X_{i}) )dx
\end{equation}
which was first proposed in 1976, by Ahmad and Lin \cite{resest1} who showed the mean-square consistency of this estimator, where  $f_{N}$ is a kernel density estimate. Joe \cite{intest2} then went on to obtain the asymptotic bias and variance, and whilst satisfying certain conditions reduced the mean square error. Moreover, Hall and Morton \cite{resest2} went on to say that under more restrictive conditions we have strong consistency for 1-dimensional distributions; however, when $d=2$ the root-n consistent estimator will have significant bias.

There are also two more plug-in estimates discussed in this paper; the splitting data and cross-validation estimates. Where in the first estimator, strong consistency is shown for a general dimension $d$, under some conditions on $f$. And in the latter estimator, strong consistency holds for a kernel estimate of $f$ and for other estimates of $f$ under some conditions we have root-n consistency when $1 \leq d \leq 3$.

Thus, so far the estimates for entropy looked at are only consistent whilst under strong conditions on $f$ and $f_{N}$ and mostly for a 1-dimensional distribution. So it is important to look at the next category of estimates - estimates of entropy based on sample-spacings; namely the m-spacing estimate. Sometimes it in not practical to estimate $f_{N}$, so this estimate is found based on spacings between the sample observations. 

This estimator is only defined for samples of 1-dimension, where we assume $X_{1}, ..., X_{N}$ are an i.i.d sample, and let $X_{N, 1} \leq X_{N, 2} \leq ... \leq X_{N, N}$ be the corresponding ordered sample, then $X_{N, i+m} - X_{N, m}$ is the m-spacing.

Firstly we look at this estimator of the form, with fixed $m$;
\begin{equation}
H_{m, N} = \frac{1}{N} \sum_{i=1}^{N-m} log \left(\frac{N}{m} (X_{N, i+m} - X_{n, i}) \right) - \Psi(m) + log(m)
\end{equation}
where $\Psi(x)$ is the digamma function - more detailed explanation in section \ref{motivation}. For a sample from a uniform distribution this estimator has been shown to be consistent; proved by Tarasenko \cite{spacest1}. Under some conditions on $f$, on its boundedness, the weak consistency and asymptotic normality was shown by Hall \cite{spacest2}.

To decrease the asymptotic variance of the estimator, we consider the estimator when $m_{N} \to \infty$, which is defined slightly differently;
\begin{equation}
H_{m, N} = \frac{1}{N} \sum_{i=1}^{N-m_{N}} log \left(\frac{N}{m_{N}} (X_{N, i+m_{N}} - X_{n, i}) \right)
\end{equation}
for this estimator the weak and strong consistencies are proved under the assumption that as $N \to \infty$, $m_{N} \to \infty$ and $\frac{m_{N}}{N} \to 0$, for densities with bounded support.

The last category of estimators discussed by Beirlant, Dudewicz, Gyorfi and Muelen are those based on nearest neighbour distances. The main focus of my paper is on the Kozachenko-Leonenko estimator for entropy; which is the estimator covered in this section of their paper. I will not go into detail for this estimator now; however, I will mention that strong consistency holds for dimension $d \leq 3$, but higher dimensions can cause problems. Thus, it is important to note that recently a new estimator has been proposed by Berrerrt, Samworth and Yuan \cite{paper4}, formed as a weighted average of k-nearest neighbour estimators for different values of k. This estimator has shown promising results in higher dimensions, where under the same assumptions as for the K-L estimator, the strong consistency condition holds.


\end{document}