\documentclass{article}
\usepackage[utf8]{inputenc}
\title{Statistical Inference for Entropy}
\author{Karina Marks}

\usepackage{amsmath, amsfonts, graphicx, listings, Booktabs}
\usepackage[format=plain,
            textfont=it]{caption}

\newtheorem{theorem}{Theorem}

\begin{document}
\maketitle
\section{Introduction}



\section{Entropies and Properties}

Entropy can be thought of as a representation of the average information content of an observation; sometimes referred to as a measure of unpredictability or disorder. 

\subsection{Shannon Entropy}
The Shannon entropy of a random vector X with density function f is given by;
\begin{align} 
H &= - \mathbb{E} \{log(f(x))\} \nonumber \\
&= - \int_{x : f(x) > 0} f(x) log(f(x)) dx \nonumber \\
&= - \sum_{x \in \mathbb{R}^{d}} f(x) log(f(x)) \label{ShaEnt}
\end{align} 

\subsection{ R\'enyi and Tsallis Entropy}
These entropies are for the order $q \neq 1$ and the construction of them relies upon the generalisation of the Shannon entropy \ref{ShaEnt}. For a random vector $X \in \mathbb{R}^d$ with density function f, we define;

R\'enyi entropy
\begin{align} 
H_{q}^{*} &= \frac{1}{1-q} log \left( \int_{\mathbb{R}^d} f^q (x) dx \right) \quad  \quad (q \neq 1) \label{RenEnt} \\
&=  \frac{1}{1-q} log \left( \sum_{x \in \mathbb{R}^{d}} f^q (x) \right) \nonumber 
\end{align}

Tsallis entropy
\begin{align} 
H_{q} &= \frac{1}{q-1} \left(1 - \int_{\mathbb{R}^d} f^q (x) dx \right)  \quad  \quad (q \neq 1) \label{TsaEnt} \\
&=  \frac{1}{q-1} \left(1 - \sum_{x \in \mathbb{R}^d} f^q (x) \right) \nonumber 
\end{align}

When the order of the entropy $q \to 1$, both the R\'enyi, (\ref{RenEnt}), and Tsallis, (\ref{TsaEnt}), entropies tend to the Shannon entropy, (\ref{ShaEnt}), this is a special case for when $q=1$. There are also other special cases, sometimes the R\'enyi entropy is considered for the special case, $q=2$, and known as the quadratic R\'enyi entropy;
\begin{align} 
H_{2}^{*} &= - log\left( \int_{\mathbb{R}^{d}} f^2(x) dx \right) \label{QuadRenEnt} \\
&= - log \left( \sum_{x \in \mathbb{R}^d} f^2 (x) \right) \nonumber 
\end{align}
As $q \to \infty$, the limit of the R\'enyi entropy exists, and is defined as the minimum entropy, since it's the smallest possible value of $H_{q}^{*}$;
\begin{equation}
H_{\infty}^{*} = - \log \sup_{x \in \mathbb{R}^d} f (x) \nonumber
\end{equation}
Thus, it follows that; $H_{\infty}^{*} \leq H_{2}^{*} \leq 2H_{\infty}^{*}$.

There is also an approximate relationship between the Shannon entropy and the quadratic R\'enyi entropy;
\begin{equation}
H_{2}^{*} \leq H \leq \log(d) + \frac{1}{d} - e^{-H_{2}^{*}} \nonumber
\end{equation}
where $H_{2}^{*}$ is the quadratic R\'enyi entropy (\ref{QuadRenEnt}), H is the Shannon entropy (\ref{ShaEnt}) and d is the dimension of the distribution.

\section{Estimation of Entropy}

\subsection{Kozachenko-Leonenko Estimator}

We now wish to introduce the Kozachenko-Leonenko estimator of the entropy H. Let $X_{1}, X_{2}, ... ,X_{N}$, $N \geq 1$ be independent and identically distributed random vectors in $\mathbb{R}^{d}$, and denote $\|.\|$ the Euclidean norm on $\mathbb{R}^{d}$.
 
\begin{itemize}

\item For $i = 1, 2, ..., N$, let $X_{(1), i}, X_{(2), i}, .., X_{(N-1), i}$ denote an order of the $X_{k}$ for $k = \{1, 2, ..., N\} \setminus \{i\}$, such that $\| X_{(1), i} - X_{i}\| \leq \cdots \leq \|  X_{(N-1), i} - X_{i}\| $. Let the metric $\rho$, defined as;
\begin{equation} \label{Rho}
\rho_{(k), i} = \| X_{(k), i} - X_{i}\|
\end{equation} denote the kth nearest neighbour or $X_{i}$.

\item  For dimension d, the volume of the unit d-dimensional Euclidean ball is defined as;
\begin{equation} \label{Volume}
V_{d} = \frac{\pi^\frac{d}{2}}{\Gamma(1 + \frac{d}{2})}
\end{equation}

\item For the kth nearest neighbour, the digamma function is defined as;
\begin{equation} \label{Psi}
\Psi(k) = -\gamma + \sum_{j=1}^{k-1} \frac{1}{j}
\end{equation}
where $\gamma = 0.577216$ is the Euler-Mascheroni constant (where the digamma function is chosen so that $\frac{e^{\Psi(k)}}{k}\to1$ as $k \to \infty$).

\end{itemize} Then the Kozachenko-Leonenko estimator for entropy, H, is given by;
\begin{equation} \label{KLest}
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(k),i}^{d} V_{d} (N-1)}{e^{\Psi(k)}} \right]
\end{equation} where, $\rho_{(k),i}^{d}$ is defined in (\ref{Rho}), $V_{d}$ is defined in (\ref{Volume}) and $\Psi(k)$ is defined in (\ref{Psi}).This estimator for entropy, when $d \leq 3$, under a wide range of k and some regularity conditions, satisfies;
\begin{equation} \label{efficiency}
N \mathbb{E} {(\hat{H}_{N, k} - H)^2} \to 0 \quad  (N \to \infty)
\end{equation} so $\hat{H}_{N, k}$ is efficient in the sense that the asymptotic variance is the best attainable; $N^{\frac{1}{2}}(\hat{H}_{N, k} - H) \xrightarrow{d} N(0, Var[log(f(x))])$, the normal distribution with 0 mean and variance as shown.

Later, I will further discuss this estimator for the specific dimensions $d=1$ and $d=2$; however, it is important to note that for larger dimensions this estimator is not accurate. When $d=4$, equation( \ref{efficiency}) no longer holds but the estimator $\hat{H}_{N, k}$, defined by (\ref{KLest}), is still root-N consistent, provided k is bounded. Also, when $d \geq 5$ there is a non trivial bias, regardless of the choice of k. 

There is a new proposed estimator, formed as a weighted average of $\hat{H}_{N, k}$ for different values of k, explored in ...SOMEONE... . Moreover, this will not be examined here as this paper focuses only on the 1-dimensional samples, with estimator fo the form;
\begin{equation} \nonumber
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(k),i} V_{1} (N-1)}{e^{\Psi(k)}} \right]
\end{equation}
and the 2-dimensional samples, with estimator of the form;
\begin{equation} \nonumber
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(k),i}^{2} V_{2} (N-1)}{e^{\Psi(k)}} \right]
\end{equation}



\subsubsection{Bias of the K-L estimator}
$\hat{H}_{N, k}$ is approximately an unbiased estimator for H; we wish to explore how approximate this is, by considering the bias of the estimator for entropy;
\begin{equation} \label{Bias}
Bias(\hat{H}_{N, k} ) = \mathbb{E}(\hat{H}_{N, k}) - H = \mathbb{E}(\hat{H}_{N, k} - H)
\end{equation}
To do this we consider the consistency and asymptotic bias of the estimator $\hat{H}_{N, k}$, ...SOMEONE... has explored this in detail thus the following theorems hold.

DON'T KNOW WHAT TO DO HERE?

\begin{theorem} \label{Paper1Thm1}
For some $\epsilon > 0$, let
\begin{equation}
\int_{\mathbb{R}^{d}} | log(f(x))|^{1 + \epsilon} f(x) dx < \infty
\end{equation}
and
\begin{equation}
\int_{\mathbb{R}^{d}} \int_{\mathbb{R}^{d}} | log(\|x-y\|)|^{2+ \epsilon} f(x) f(y) dx dy < \infty
\end{equation}
Then
\begin{equation}
\lim_{N \to \infty} \mathbb{E} (\hat{H}_{N, k}) = H
\end{equation}
\end{theorem}

\begin{theorem} \label{Paper1Thm2-Consistency}
For some $\epsilon > 0$, let
\begin{equation}
\int_{\mathbb{R}^{d}} | log(f(x))|^{2 + \epsilon} f(x) dx < \infty
\end{equation}
and
\begin{equation}
\int_{\mathbb{R}^{d}} \int_{\mathbb{R}^{d}} | log(\|x-y\|)|^{2+ \epsilon} f(x) f(y) dx dy < \infty
\end{equation}
Then $\hat{H}_{N, k}$ for $N \to \infty$ is a consistent estimator of H.
\end{theorem}


\subsection{Other Estimators}
\begin{itemize}
\item The estimator for $H_{2}^{*}$ from paper 5

\item The estimator for higher dimensions $d$, from paper 4
\end{itemize}

\section{Monte-Carlo Simulations}

In this section I will explore simulations of the bias of estimator (\ref{KLest}) in comparison to the size of the sample estimated from, with respect to different values of k; firstly exploring 1-dimensional distributions and then progressing onto 2-dimensional.

The motivation for these simulations is to explore the consistency of this estimator for different values of $k$; the relationship between the size of the bias of the estimator $\hat{H}_{N, k}$, $Bias(\hat{H}_{N, k})$,  and the sample size, $N$. Throughout this analysis we will be considering the absolute value of this bias, since when considering its logarithm, we need a positive value. We believe the relationship between these two variables is of the form;
\begin{equation} \label{bias}
|Bias(\hat{H}_{N, k})| = \frac{c}{N^a}
\end{equation}
for $a, c > 0$. By taking the logarithm of this, we can see that this relationship is in fact linear;
\begin{equation} \label{logbias}
log|Bias(\hat{H}_{N, k})| = log(c) - a [log(N)]
\end{equation}
I will investigate the consistency of this estimator for a sample from the normal distribution, dependent on the value of $k$. I wish to find the optimum value of $k$ for which $|Bias(\hat{H}_{N, k})| \to 0$ for $N \to \infty$. For the relationship in (\ref{bias}), this will happen for large values of $a$ and relatively small $c$. I will also examine the dependence of the value of $c$ on the value of $k$.

As I wish to consider the difference in accuracy of the estimator when using different values of k, let us denote the approximate values for $a$ and $c$ dependent on $k$ as $a_{k}$ and $c_{k}$.


\subsection{1-dimensional Normal Distribution}

I will begin by exploring entropy of samples from the normal distribution $N(0, \sigma^2)$, where without loss of generality we can use the mean $\mu = 0$ and change the variance $\sigma^2$ as needed. The normal distribution has an exact formula to work out the entropy, given the variance $\sigma^2$. Using equation (\ref{ShaEnt}) and the density function for the normal distribution $f(x) = \frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)}$ for $x \in \mathbb{R}$, given $\mu = 0$. We can write the exact entropy for the normal distribution, using equation (\ref{ShaEnt});
\begin{align}
H &= - \int_{x : f(x) > 0} f(x) log(f(x)) dx \nonumber \\
&= - \int_{\mathbb{R}} \frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)} log \left[\frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)} \right] dx \nonumber \\
&=  \int_{\mathbb{R}} \frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)} \left( log(\sqrt{(2\pi)}\sigma) +  \frac{x^2}{2\sigma^2} \right) \nonumber \\
&= \frac{log(\sqrt{(2\pi)}\sigma)}{\sqrt{(2\pi)} \sigma} \int_{\mathbb{R}} \exp{ \left( \frac{-x^2}{2\sigma^2} \right)} dx +  \frac{1}{2\sqrt{(2\pi)} \sigma} \int_{\mathbb{R}} \frac{x^2}{2\sigma^2}  \exp{ \left( \frac{-x^2}{\sigma^2} \right)} dx \nonumber \\
&=  log(\sqrt{(2\pi)}\sigma) + \frac{1}{2} \nonumber 
\end{align}
Thus the exact entropy for the normal distribution is given by 
\begin{equation}\label{NormalEnt}
H =  log(\sqrt{(2\pi e)}\sigma) 
\end{equation}

The normal distribution has the properties which automatically satisfy the conditions above.... condition 1 since ... condition 2 since...

I will first explore the 1-dimensional standard normal distribution with mean $\mu = 0$ and variance $\sigma^2 = 1$, $N(0, 1)$. The exact entropy of this distribution is given by;
\begin{equation} \label{normal_exact}
H = log(\sqrt{(2\pi e)}) \approx 1.418939
\end{equation}



\subsubsection{k=1} \label{N_k=1}
I will be considering k=1 for the estimator of entropy; thus, the estimator will take the form;
\begin{equation} 
\hat{H}_{N, 1} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(1),i}^{d} V_{d} (N-1)}{e^{\Psi(1)}} \right] = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(1),i}^{d} V_{d} (N-1)}{e^{-\gamma}} \right] \nonumber
\end{equation}

I will consider $500$ samples of size $N$ from this distribution, find the estimator in each case and take the average of these estimators to find our entropy estimator, shown in table \ref{normal_k=1_table}. We will then consider the relationship show in (\ref{logbias}) for each sample and again work out the average for the values of a and c, shown in \ref{normal_k=1_graph}.

\begin{table}
\caption{1-dimensional normal distribution, $k=1$} \label{normal_k=1_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 1}$ & $|Bias(\hat{H}_{N, 1})|$ & $Var(Bias(\hat{H}_{N, 1}))$ \\
\midrule[1pt]
100     & 1.405890     & 0.01304883282     & 0.0275142086  \\
200     & 1.411070     & 0.00786872927     & 0.0128689734  \\
500     & 1.416666     & 0.00227293433     & 0.0051416433  \\
1000    & 1.419401     & 0.00046261516     & 0.0028127916  \\
5000    & 1.418469     & 0.00046981107     & 0.0005147810  \\
10000   & 1.417998     & 0.00094067533     & 0.0002472848  \\
25000   & 1.418877     & 0.00006147045     & 0.0001088641  \\
50000   & 1.419286     & 0.00034705584     & 0.0000496450  \\
\hline
\end{tabular}
\end{center}
\end{table}

Considering table \ref{normal_k=1_table}, we can see for a larger value of N, the Bias of the estimator becomes much smaller; the bias decreases from $\approx 0.0130$ to $\approx 0.0003$ as $N$ increases from $100 \to 50,000$. This result is to be expected for an estimator to satisfy the consistency condition (\ref{?}). We can also see that the variance of the bias is decreasing as N increases implying that, not only is the average of the estimator getting closer to the actual value of entropy, also the variability between the estimator of different samples is decreasing, making it a consistent and asymptotically unbiased estimator in practice, as well as in theory.

This relationship between the bias $|Bias(\hat{H}_{N, 1})|$ of the estimator and the size of the sample $N$, can be computed for these sample sizes. Figure \ref{normal_k=1_graph}, shows this relationship of $log|Bias(\hat{H}_{N, 1})|$ against $log(N)$ with a fitted regression line. In this graph, I have considered the values of $N$ up to $50,000$ at intervals of size $100$, where each point is calculate $500$ times and the average estimator is plotted. I have also found the corresponding coefficients a and c for the relationship shown in (\ref{bias}); $a = 0.4594$ and $c = 0.0249$;
\begin{equation}
|Bias(\hat{H}_{N, 1})| = \frac{0.0249}{N^{0.4594}} \nonumber
\end{equation}
On their own these coefficients show that there is a negative relationship between $log(|Bias(\hat{H}_{N, 1})|)$ and $log(N)$. This implies that the relationship between the Bias and N is such that as N increases the Bias decreases to 0. Hence, creating a consistent estimator for entropy, when considering the 1-dimensional normal distribution with k=1 in the Kozachenko-Leonenko estimator. However, to understand better the meaning of this relationship we must compare this to coefficients found of the regression relationships for different values of $k$, and for different distributions.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_k=1_plot.png}
  \end{center}
  \caption{Graph of simulations from the 1-dimensional standard normal distribution, considering the logarithm of the Bias of the estimator for entropy, with k=1, against the logarithm of N.} 
  \label{normal_k=1_graph}
\end{figure}


\subsubsection{k=2} \label{N_k=2}

I am now going to examine the case where k=2 in the Kozachenko-Leonenko estimator, to compare the results of simulations from this estimator with that for k=1. Here the estimator will take the form;
\begin{equation}
\hat{H}_{N, 2} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(2),i}^{d} V_{d} (N-1)}{e^{\Psi(2)}} \right] = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(2),i}^{d} V_{d} (N-1)}{e^{-\gamma + 1}} \right] \nonumber
\end{equation}

I wish to explore, in a similar manner as for k=1, the changes in the bias of the estimator depending on a change in N. Additionally, later I will make the comparison between the regression coefficients for different values of k. I will again consider 500 samples of size N from the 1-dimensional standard normal distribution $N(0, 1)$, the results from the analysis is shown in table \ref{normal_k=2_table}.

\begin{table}
\caption{1-dimensional normal distribution, $k=2$} \label{normal_k=2_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 2}$ & $|Bias(\hat{H}_{N, 2})|$ & $Var(Bias(\hat{H}_{N, 2}))$ \\
\midrule[1pt]
100     & 1.408856     & 0.0100827948     & 0.01357417708  \\
200     & 1.411165     & 0.0077730666     & 0.00688250329  \\
500     & 1.419158     & 0.0002199163     & 0.00296693934  \\
1000    & 1.415719     & 0.0032197158     & 0.00141616592  \\
5000    & 1.418236     & 0.0007026416     & 0.00028872533  \\
10000   & 1.418656     & 0.0002824567     & 0.00014348493  \\
25000   & 1.418376     & 0.0005620780     & 0.00005791073  \\
50000   & 1.418681     & 0.0002574343     & 0.00002956529  \\
\hline
\end{tabular}
\end{center}
\end{table}


We can see that, as expected, the Bias of the estimator decreases from $\approx 0.0100$ when $N=100$ to $\approx 0.0002$ when $N=50,000$. This is showing that the consistency condition is being met since as $N \to \infty$ we have $|Bias(\hat{H}_{N, 2})| \to 0$, which is equivalent to saying $\lim_{N \to \infty} \mathbb{E} (\hat{H}_{N, k}) = H$, Theorem (\ref{Paper1Thm1}). We also have that the variance of the bias of these estimators decrease as $N \to \infty$, as expected. In relation to $k=1$, we can see that the bias of this estimator for $k=2$ decreases at a similar pace as $N \to \infty$; $|Bias(\hat{H}_{N, 1})| \approx 0.0130 \to 0.0003$ and $|Bias(\hat{H}_{N, 2})| \approx 0.0101 \to 0.0002$, implying that from this analysis we cannot decide which value of $k$ generates a better estimator.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_k=2_plot.png}
  \end{center}
  \caption{Graph of simulations from the 1-dimensional standard normal distribution, considering the logarithm of the Bias of the estimator for entropy, with k=2, against the logarithm of N.} 
  \label{normal_k=2_graph}
\end{figure}



CONSIDER THE GRAPH...

a
[1] 0.5998

c
[1] 0.0746

$summary
[1] "c = 0.0746 and a = 0.5998. So Bias(H) = 0.0746/(N^0.5998)  for samples from the normal distribution."$


\subsubsection{k=3} \label{N_k=3}

Again, for $k=3$, I will examine $500$ samples of size $N$ from the standard normal distribution considered before; with estimator of the form;
\begin{equation}
\hat{H}_{N, 3} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(3),i}^{d} V_{d} (N-1)}{e^{\Psi(3)}} \right] = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(3),i}^{d} V_{d} (N-1)}{e^{-\gamma + 1 + \frac{1}{2}}} \right] \nonumber
\end{equation}
The results are displayed in table \ref{normal_k=3_table}. 

\begin{table}
\caption{1-dimensional normal distribution, $k=3$} \label{normal_k=3_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 3}$ & $|Bias(\hat{H}_{N, 3})|$ & $Var(Bias(\hat{H}_{N, 3}))$ \\
\midrule[1pt]
100     & 1.398784     & 0.0201546812     & 0.01210622150  \\
200     & 1.412908     & 0.0060302660     & 0.00530612702  \\
500     & 1.414035     & 0.0049035937     & 0.00223855589  \\
1000    & 1.416105     & 0.0028340080     & 0.00107754839  \\
5000    & 1.420184     & 0.0012459298     & 0.00022320970  \\
10000   & 1.418351     & 0.0005874791     & 0.00011630350  \\
25000   & 1.419115     & 0.0001760980     & 0.00004286406  \\
50000   & 1.418853     & 0.0000851863     & 0.00002257717  \\
\hline
\end{tabular}
\end{center}
\end{table}

This shows again, that the Kozachenko-Leonenko estimator for entropy tends to 0, $\hat{H}_{N, 3} \to 0$, as $N \to \infty$. However, comparing these results to those for $k=1, 2$, which had similar bias as $N \to \infty$, we can see that for $k=3$, $|Bias(\hat{H}_{N, 3})| \approx 0.0202 \to 0.00009$. So for larger $N$, the estimator with $k=1$ or $k=2$ would be less appropriate to use, since the bias is slightly larger than for the estimator using $k=3$.

CONSIDER THE GRAPH...

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_k=3_plot.png}
  \end{center}
  \caption{Graph of simulations from the 1-dimensional standard normal distribution, considering the logarithm of the Bias of the estimator for entropy, with k=3, against the logarithm of N.} 
  \label{normal_k=3_graph}
\end{figure}



$"c = 0.1156 and a = 0.6443. So Bias(H) = 0.1156/(N^0.6443)  for samples from the normal distribution."$


\subsubsection{k=5} \label{N_k=5}

Now we consider the estimator, shown in equation \ref{KLest}, for k=5. This takes the form;
\begin{equation}
\hat{H}_{N, 5} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(5),i}^{d} V_{d} (N-1)}{e^{\Psi(5)}} \right] \nonumber
\end{equation}

\begin{table}
\caption{1-dimensional normal distribution, $k=5$} \label{normal_k=5_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 5}$ & $|Bias(\hat{H}_{N, 5})|$ & $Var(Bias(\hat{H}_{N, 5}))$ \\
\midrule[1pt]
100     & 1.391834     & 0.02710439666     & 0.00807261026  \\
200     & 1.405356     & 0.01358205942     & 0.00425419382  \\
500     & 1.411436     & 0.00750282472     & 0.00168848112  \\
1000    & 1.415091     & 0.00384740080     & 0.00091927735  \\
5000    & 1.418150     & 0.00078877480     & 0.00018941496  \\
10000   & 1.418648     & 0.00029099525     & 0.00008767553  \\
25000   & 1.418879     & 0.00005917171     & 0.00003243503  \\
50000   & 1.418644     & 0.00029451951     & 0.00001705529  \\
\hline
\end{tabular}
\end{center}
\end{table}

Comparing this to the exact entropy for the standard normal distribution, (\ref{normal_exact}), gives table \ref{normal_k=5_table}. Here, the $|Bias(\hat{H}_{N, 5})|$ decreases as $N$ goes from $100 \to 25,000$, but at $50,000$ this jumps to a larger number. Up to $25,000$ indicates that the estimator is becoming closer to the actual value, the jump at $50,000$  could be due to a number of reasons. 

Firstly, this could indicate that for $k=5$, this estimator becomes less efficient, and doesn't satisfy the property ...  as strongly as smaller values of $k$ have done so far. Secondly,this could just be an error in the data for $|Bias(\hat{H}_{50000, 5})|$ ; since we are only considering a relative small number of samples, 500, and are taking the average of this, we could just have an outlier. Lastly, there could be an error in the previous two data points, $|Bias(\hat{H}_{25000, 5})|$ and $|Bias(\hat{H}_{10000, 5})|$, causing us to either believe it is decreasing, when it isn't.

To determine the reason for this jump of Bias in the wrong direction, I will examine $|Bias(\hat{H}_{50000, 5})|$ for 3,000 samples and see if this is consistent with the previous findings. I have found this number to be; 
\begin{equation} 
|Bias(\hat{H}_{50000, 5})|  \approx 0.00006034936 \nonumber
\end{equation}
This gives us a much smaller bias than $|Bias(\hat{H}_{50000, 5})|$ shown in table \ref{normal_k=5_table}, however, this is still not smaller than the value of $|Bias(\hat{H}_{25000, 5})|$ shown in the same table. This could mean that for $k=5$, the estimator doesn't satisfy the consistency condition as strongly as the previous estimators for $k=1, 2, 3$.

CONSIDER GRAPH...

$"c = 0.3557 and a = 0.7568. So Bias(H) = 0.3557/(N^0.7568)  for samples from the normal distribution."$


\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_k=5_plot.png}
  \end{center}
  \caption{Graph of simulations from the 1-dimensional standard normal distribution, considering the logarithm of the Bias of the estimator for entropy, with k=5, against the logarithm of N.} 
  \label{normal_k=5_graph}
\end{figure}





\subsubsection{k=10} \label{N_k=10}
The last estimator for the entropy of a sample from the standard normal distribution that I wish to explore is that for $k=10$. Here, the estimator takes the form;
\begin{equation}
\hat{H}_{N, 10} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(10),i}^{d} V_{d} (N-1)}{e^{\Psi(10)}} \right] \nonumber
\end{equation}
The results for the comparison between this estimator and \ref{normal_exact} are displayed in table \ref{normal_k=10_table}.

\begin{table}
\caption{1-dimensional normal distribution, $k=10$} \label{normal_k=10_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 10}$ & $|Bias(\hat{H}_{N, 10})|$ & $Var(Bias(\hat{H}_{N, 10}))$ \\
\midrule[1pt]
100     & 1.375699     & 0.0432399931     & 0.00678770166  \\
200     & 1.391934     & 0.0270050257     & 0.00293164825  \\
500     & 1.407625     & 0.0113137866     & 0.00148669638  \\
1000    & 1.411684     & 0.0072549983     & 0.00067990485  \\
5000    & 1.417306     & 0.0016322988     & 0.00013650841  \\
10000   & 1.418196     & 0.0007429215     & 0.00006783354  \\
25000   & 1.418356     & 0.0005825702     & 0.00003162161  \\
50000   & 1.418790     & 0.0001488755     & 0.00001318863  \\
\hline
\end{tabular}
\end{center}
\end{table}

Here, we can again see that this estimator is satisfying the consistency condition \ref{??}, as $\hat{H}_{N, 10} \approx  0.0432 \to 0.0001$ as $N \approx 100 \to 50,000$. Comparing this to previous values of $k$, we can see that the bias changes decreases in a similar manner to that for $k=1, 2, 3$.

CONSIDER GRAPH...

\subsubsection{Comparison of k}

The above analysis, sections \ref{N_k=1} to \ref{N_k=10} is done to examine the difference in the bias of the estimator for different values of k. Considering the above samples, for $N=25,000$ and $N=50,000$, we can create a table to compare the values of the bias of the estimator for the different values of $k$ considered;

\begin{table}
\caption{1-dimensional normal distribution, comparison of $k$} \label{normal_kcompare_table}
\begin{center}
\begin{tabular}{| l | c c c c|} 
\toprule
$k$ & $|Bias(\hat{H}_{25000, k})|$ & $Var(Bias(\hat{H}_{25000, k}))$ & $|Bias(\hat{H}_{50000, k})|$ & $Var(Bias(\hat{H}_{50000, k}) )$ \\
\midrule[1pt]
1    & 0.00006147045   & 0.0001088641     & 0.00034705584   & 0.0000496450   \\
2    & 0.0005620780     & 0.00005791073   & 0.0002574343     & 0.00002956529 \\
3    & 0.0001760980     & 0.00004286406   & 0.0000851863     & 0.00002257717 \\
5    & 0.00005917171   & 0.00003243503   & 0.00029451951   & 0.00001705529 \\
10  & 0.0005825702     & 0.00003162161   & 0.0001488755     & 0.00001318863 \\
\hline
\end{tabular}
\\[10pt]
\caption*{This table is comparing the values of $|Bias(\hat{H}_{N, k})|$ for the values of $k$ explored in tables \ref{normal_k=1_table}, \ref{normal_k=2_table}, \ref{normal_k=3_table}, \ref{normal_k=5_table} and  \ref{normal_k=10_table} with $N=25,000$ and $N=50,000$, when the estimator is taken over $500$ samples}
\end{center}
\end{table}

The results shown in table \ref{normal_kcompare_table} are inconclusive in determining if larger/smaller values of k generate better estimators, with smaller bias. However, these results are consistent in showing that for the larger value of N, the smaller the variance in the estimator. The results for the Bias are not conclusive; because, for $N=25,000$ we can see that with $k=1, 5$ and possibly $k=3$ have a slight smaller bias than the others. However, when $N=50,000$ we find that for $k=3, 10$ we have the smallest values of bias. These are inconsistent with one and other. To further examine this, I will now generate a table for values $k=1, 2, 3, 5 and 10$ with $N=50,000$ in all cases. Moreover, this time I will consider $3,000$ samples of this size, not the $500$ considered before, and will find the mean and variance of the bias of this estimator.

\begin{table}
\caption{1-dimensional normal distribution, comparison of $k$} \label{normal_kcompare2_table}
\begin{center}
\begin{tabular}{| l | c c |} 
\toprule
$k$ &  $|Bias(\hat{H}_{50000, k})|$ & $Var(Bias(\hat{H}_{50000, k}))$ \\
\midrule[1pt]
1      & 0.00013495546     & 0.00005116758  \\
2      & 0.00012647214     & 0.00002868082  \\
3      & 0.00003478968     & 0.00002299754  \\
5      & 0.00006034936     & 0.00001733369  \\
10    & 0.00022455715     & 0.00001409080  \\
\hline
\end{tabular}
\\[10pt]
\caption*{This table is comparing the values of $|Bias(\hat{H}_{N, k})|$ for the values of $k$ explored before now with only $N=50,000$ and the estimator being taken over $3,000$ samples}
\end{center}
\end{table}

From the results in table \ref{normal_kcompare2_table}, we can see that  $|Bias(\hat{H}_{N, k})|$ is the smallest, for sample size $N=50,000$, when $k=3$, which is consistent with the results found in table \ref{normal_kcompare_table}. So from these simulations, we can conclude that for large $N$, the consistency condition is best satisfied when $k=3$. Interestingly, the $Var(Bias(\hat{H}_{50000, k})) \to 0$ for $k \to 10$, but this is to be expected, as by the definition of the estimator using the nearest neighbour method. Taking a larger $k$ in the nearest neighbour method will produce less varied results, this is because more smoothing takes place for a larger $k$, eventually - if $k$ is made large enough - the output will be constant and the variance negligible regardless of the inputted values. Thus, considering the variance of the bias of the estimator is not necessarily informative.

CONSIDER GRAPHS...

\subsection{1-dimensional Uniform distribution}

I will now explore the entropy of samples from the 1-dimensional uniform distribution, $U[a b]$. This distribution also has an exact formula to work out the entropy for. We can find this formula by considering the density function, $f$, from the uniform distribution, which is given by;
\[
f(x) =  \begin{cases} 
      \frac{1}{b-a} & a \leq x \leq b \\
      0 & otherwise
   \end{cases}
\]
Using the definition of Shannon entropy given in equation (\ref{ShaEnt}), we can find the exact entropy for the uniform distribution;
\begin{align}
H &= - \int_{x : f(x) > 0} f(x) log(f(x)) dx \nonumber \\ 
&= - \int_{a}^{b} \frac{1}{b-a} log \left[ \frac{1}{b-a} \right] dx \nonumber \\
&= - \frac{1}{b-a} log \left[ \frac{1}{b-a} \right]  \int_{a}^{b} dx \nonumber \\
&= -  log  \left[ \frac{1}{b-a} \right] \nonumber
\end{align}
Thus, the actual value of entropy for the uniform distribution is given by;
\begin{equation} \label{UnifEnt}
H = log [ b-a ]
\end{equation}

The uniform distribution automatically satisfies the conditions .... because ... 

In our samples we will be consider the uniform distribution $U[0,100]$; this is because, using the standard uniform, $U[0,1]$, would fail since taking $N=50,000$ samples between 0 and 1 would generate problems as the pdf would be $f(x) = 1 , \quad 0 \leq x \leq 1$, which would incur working on a very small scale; i.e taking a points with distance between them as $\approx 0.00002$ along the x-direction. Thus using the pdf $f(x) = 0.01 , \quad 0 \leq x \leq 100$, which gives the exact entropy to be;
\begin{equation} \label{uniform_exact}
H = log(100) \approx 4.605170
\end{equation}

\subsubsection{k=1} \label{U_k=1}

We will begin by considering 500 samples from the uniform distribution $U[0,100]$ and finding the estimator for $k=1$, which is of the form;
\begin{equation} 
\hat{H}_{N, 1} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(1),i}^{d} V_{d} (N-1)}{e^{\Psi(1)}} \right] = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(1),i}^{d} V_{d} (N-1)}{e^{-\gamma}} \right] \nonumber
\end{equation}
Considering the bias for this estimator against the actual value (\ref{uniform_exact}) for different samples sizes $N$ gives table \ref{uniform_k=1_table}.

\begin{table}
\caption{1-dimensional uniform distribution, $k=1$} \label{uniform_k=1_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 1}$ & $|Bias(\hat{H}_{N, 1})|$ & $Var(Bias(\hat{H}_{N, 1}))$ \\
\midrule[1pt]
100     & 4.596921     & 0.00824926770     & 0.0222674976  \\
200     & 4.616316     & 0.01114602105     & 0.0100263188  \\
500     & 4.601683     & 0.00348766754     & 0.0040618438  \\
1000    & 4.610342     & 0.00517161041     & 0.0017659932  \\
5000    & 4.605226     & 0.00005536281     & 0.0003974555  \\
10000   &     -Inf     &           Inf     &          NaN  \\
25000   &     -Inf     &           Inf     &          NaN  \\
50000   &     -Inf     &           Inf     &          NaN  \\
\hline
\end{tabular}
\end{center}
\end{table}

We can see from this table that for $N \geq 10,000$ the estimator cannot be computed. This is due to the same reason, that we decided to use the distribution $U[0,100]$ instead of the standard normal $U[0,1]$. However, even considering $U[0,100000]$ only gives non infinite values for $N \leq 10,000$. Hence, from now on, we will only consider samples of size $N \leq 5,000$ and we will use the distribution $U[0,100]$. From the above table, we can see obviously, that for the largest non-inifinite value of $N$, we have $|Bias(\hat{H}_{5000, 1})| \approx 0.00006$, which is significantly smaller than $|Bias(\hat{H}_{100, 1})| \approx 0.00825$, confirming the consistency condition.





\subsubsection{k=2} \label{U_k=2}





\begin{table}
\caption{1-dimensional uniform distribution, $k=2$} \label{uniform_k=2_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 2}$ & $|Bias(\hat{H}_{N, 2})|$ & $Var(Bias(\hat{H}_{N, 2}))$ \\
\midrule[1pt]
100     & 4.606725     & 0.0015545396     & 0.00851906912  \\
200     & 4.610785     & 0.0056145269     & 0.00456819154  \\
500     & 4.611599     & 0.0064290000     & 0.00189878458  \\
1000    & 4.603534     & 0.0016366244     & 0.00095980274  \\
5000    & 4.604978     & 0.0001921979     & 0.00017282038  \\
10000   & 4.605488     & 0.0003180407     & 0.00009978981  \\
25000   & 4.604753     & 0.0004176853     & 0.00004003515  \\
50000   & 4.605480     & 0.0003095010     & 0.00001737807  \\
\hline
\end{tabular}
\end{center}
\end{table}




\end{document}