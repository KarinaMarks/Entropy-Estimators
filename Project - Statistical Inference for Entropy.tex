\documentclass{article}
\usepackage[utf8]{inputenc}
\title{Statistical Inference for Entropy}
\author{Karina Marks}

\usepackage{amsmath, amsfonts, graphicx, listings, booktabs, amstext}
\usepackage[format=plain,
            textfont=it]{caption}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Condition}

\begin{document}
\maketitle
\section{Introduction}



\section{Entropies and Properties}

Entropy can be thought of as a representation of the average information content of an observation; sometimes referred to as a measure of unpredictability or disorder. 

\subsection{Shannon Entropy}
The Shannon entropy of a random vector X with density function f is given by;
\begin{align} 
H &= - \mathbb{E} \{log(f(x))\} \nonumber \\
&= - \int_{x : f(x) > 0} f(x) log(f(x)) dx \nonumber \\
&= - \sum_{x \in \mathbb{R}^{d}} f(x) log(f(x)) \label{ShaEnt}
\end{align} 

\subsection{ R\'enyi and Tsallis Entropy}
These entropies are for the order $q \neq 1$ and the construction of them relies upon the generalisation of the Shannon entropy \ref{ShaEnt}. For a random vector $X \in \mathbb{R}^d$ with density function f, we define;

R\'enyi entropy
\begin{align} 
H_{q}^{*} &= \frac{1}{1-q} log \left( \int_{\mathbb{R}^d} f^q (x) dx \right) \quad  \quad (q \neq 1) \label{RenEnt} \\
&=  \frac{1}{1-q} log \left( \sum_{x \in \mathbb{R}^{d}} f^q (x) \right) \nonumber 
\end{align}

Tsallis entropy
\begin{align} 
H_{q} &= \frac{1}{q-1} \left(1 - \int_{\mathbb{R}^d} f^q (x) dx \right)  \quad  \quad (q \neq 1) \label{TsaEnt} \\
&=  \frac{1}{q-1} \left(1 - \sum_{x \in \mathbb{R}^d} f^q (x) \right) \nonumber 
\end{align}

When the order of the entropy $q \to 1$, both the R\'enyi, (\ref{RenEnt}), and Tsallis, (\ref{TsaEnt}), entropies tend to the Shannon entropy, (\ref{ShaEnt}), this is a special case for when $q=1$. There are also other special cases, sometimes the R\'enyi entropy is considered for the special case, $q=2$, and known as the quadratic R\'enyi entropy;
\begin{align} 
H_{2}^{*} &= - log\left( \int_{\mathbb{R}^{d}} f^2(x) dx \right) \label{QuadRenEnt} \\
&= - log \left( \sum_{x \in \mathbb{R}^d} f^2 (x) \right) \nonumber 
\end{align}
As $q \to \infty$, the limit of the R\'enyi entropy exists, and is defined as the minimum entropy, since it's the smallest possible value of $H_{q}^{*}$;
\begin{equation}
H_{\infty}^{*} = - \log \sup_{x \in \mathbb{R}^d} f (x) \nonumber
\end{equation}
Thus, it follows that; $H_{\infty}^{*} \leq H_{2}^{*} \leq 2H_{\infty}^{*}$.

There is also an approximate relationship between the Shannon entropy and the quadratic R\'enyi entropy;
\begin{equation}
H_{2}^{*} \leq H \leq \log(d) + \frac{1}{d} - e^{-H_{2}^{*}} \nonumber
\end{equation}
where $H_{2}^{*}$ is the quadratic R\'enyi entropy (\ref{QuadRenEnt}), H is the Shannon entropy (\ref{ShaEnt}) and d is the dimension of the distribution.

\section{Estimation of Entropy}

\subsection{Kozachenko-Leonenko Estimator}

I now wish to introduce the Kozachenko-Leonenko estimator of the entropy H. Let $X_{1}, X_{2}, ... ,X_{N}$, $N \geq 1$ be independent and identically distributed random vectors in $\mathbb{R}^{d}$, and denote $\|.\|$ the Euclidean norm on $\mathbb{R}^{d}$.
 
\begin{itemize}

\item For $i = 1, 2, ..., N$, let $X_{(1), i}, X_{(2), i}, .., X_{(N-1), i}$ denote an order of the $X_{k}$ for $k = \{1, 2, ..., N\} \setminus \{i\}$, such that $\| X_{(1), i} - X_{i}\| \leq \cdots \leq \|  X_{(N-1), i} - X_{i}\| $. Let the metric $\rho$, defined as;
\begin{equation} \label{Rho}
\rho_{(k), i} = \| X_{(k), i} - X_{i}\|
\end{equation} denote the kth nearest neighbour or $X_{i}$.

\item  For dimension d, the volume of the unit d-dimensional Euclidean ball is defined as;
\begin{equation} \label{Volume}
V_{d} = \frac{\pi^\frac{d}{2}}{\Gamma(1 + \frac{d}{2})}
\end{equation}

\item For the kth nearest neighbour, the digamma function is defined as;
\begin{equation} \label{Psi}
\Psi(k) = -\gamma + \sum_{j=1}^{k-1} \frac{1}{j}
\end{equation}
where $\gamma = 0.577216$ is the Euler-Mascheroni constant (where the digamma function is chosen so that $\frac{e^{\Psi(k)}}{k}\to1$ as $k \to \infty$).

\end{itemize} Then the Kozachenko-Leonenko estimator for entropy, H, is given by;
\begin{equation} \label{KLest}
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(k),i}^{d} V_{d} (N-1)}{e^{\Psi(k)}} \right]
\end{equation} where, $\rho_{(k),i}^{d}$ is defined in (\ref{Rho}), $V_{d}$ is defined in (\ref{Volume}) and $\Psi(k)$ is defined in (\ref{Psi}).This estimator for entropy, when $d \leq 3$, under a wide range of k and some regularity conditions, satisfies some  theorems. Firstly from PAPER 1 (TODO refrence), we have the following;

\begin{theorem} \label{paper1_T1}
For exact entropy $H$, Kozachenko-Leonenko estimator $\hat{H}_{N,k}$, and density function $f(x)$, for some $\epsilon > 0$ if both
\begin{equation} \label{paper1_T1_eq1}
\int_{\mathbb{R}^{d}} | log(f(x))|^{1 + \epsilon} f(x) dx < \infty 
\end{equation}
and
\begin{equation} \label{paper1_T1_eq2}
\int_{\mathbb{R}^{d}} \int_{\mathbb{R}^{d}} | log(\|x-y\|)|^{1+ \epsilon} f(x) f(y) dx dy < \infty
\end{equation}
Then we have that 
\begin{equation} 
\lim_{N \to \infty} \mathbb{E} (\hat{H}_{N, k}) = H \nonumber
\end{equation}
Thus $\hat{H}_{N, k}$ is an asymptotically unbiased estimator of $H$.
\end{theorem}

\begin{theorem} \label{paper1_T2}
For exact entropy $H$, Kozachenko-Leonenko estimator $\hat{H}_{N,k}$, and density function $f(x)$, for some $\epsilon > 0$ if both
\begin{equation} \label{paper1_T2_eq1}
\int_{\mathbb{R}^{d}} | log(f(x))|^{2 + \epsilon} f(x) dx < \infty \nonumber
\end{equation}
and
\begin{equation} \label{paper1_T2_eq2}
\int_{\mathbb{R}^{d}} \int_{\mathbb{R}^{d}} | log(\|x-y\|)|^{2+ \epsilon} f(x) f(y) dx dy < \infty \nonumber
\end{equation}
Then $\hat{H}_{N, k}$ for $N \to \infty$ is a consistent estimator of H.

(An estimator is consistent if the probability that it is in error by more than a given amount tends to zero as the sample become large $\Leftrightarrow $ for error $\delta > 0$, we have $\lim_{N \to \infty} \mathbb{P}(|\hat{H}_{N,k} - H| < \delta) = 1$) 
\end{theorem}

Additionally, from paper 4 (TODO reference), we have the following, stronger conditions which will be used in Theorems \ref{paper4_T4} and \ref{paper4_T5};
\begin{remark} ($\beta$) \label{A1}
For density $f$ bounded, denoting $m := \lfloor \beta \rfloor$ and $\eta := \beta -m$, we have that $f$ is $m$ times continuously differentiable and there exists $r_{*} > 0$ and a Borel measurable function $g_{*}$ such that for each $t = 1, 2, ... , m$ and $\|y-x\| \leq r_{*}$, we have;
\begin{equation}
\| f^{(t)} (x) \| \leq g_{*}(x)f(x) \nonumber
\end{equation},
\begin{equation}
\| f^{(m)} (y) - f^{(m)} (x) \| \leq g_{*}(x)f(x) \|y-x\|^{\eta} \nonumber
\end{equation}
and $sup_{x:f(x)\geq \delta} g_{*}(x) = o(\delta^{-\epsilon})$ as $\delta \downarrow 0$, for each $\epsilon > 0$.
\end{remark}

\begin{remark} ($\alpha$) \label{A2}
For density $f(x)$ and dimension $d$, we have;
\begin{equation}
\int_{\mathbb{R}^{d}} \| x \|^{\alpha} f(x) dx < \infty \nonumber
\end{equation}
\end{remark}

\begin{remark} \label{A3}
Assume that condition \ref{A1} holds for $\beta = 2$ and condition \ref{A2} holds for some $\alpha > d$. Let $k_{0}^{*} = k_{0, N}^{*}$ and $k_{1}^{*} = k_{1, N}^{*}$ denote two deterministic sequences of positive integers with $k_{0}^{*} \leq k_{1}^{*}$, with $\frac{k_{0}^{*}}{\log^{5}{N}} \to \infty$ and with $k_{1}^{*} = O(N^{\tau})$, where
\begin{equation}
\tau < \min \left\{ \frac{2 \alpha}{5 \alpha + 3d} , \frac{\alpha - d}{2 \alpha} , \frac{4}{4 + 3d} \right\} \nonumber
\end{equation}
\end{remark}

\begin{theorem}\label{paper4_T4}
Assume that condition \ref{A1} holds for $\beta = 2$ and condition \ref{A2} holds for some $\alpha > d$, then by condition \ref{A3}, for the estimator $\hat{H}_{N, k}$ we have;
\begin{equation}
Var(\hat{H}_{N, k}) = \frac{Var(\log f(x))}{N} + o(\frac{1}{N}) \nonumber
\end{equation}
as $N \to \infty$, uniformly for $k \in \{ k_{0}^{*}, ...,  k_{1}^{*} \}$.
\end{theorem}

\begin{theorem} \label{paper4_T5}
Assume that $d \leq 3$ and the conditions of Theorem \ref{paper4_T4} are satisfied (where if $d=3$, we additionally assume $k_{1}^{*} = o(n^{\frac{1}{4}})$. Then, for the estimator $\hat{H}_{N, k}$ we have;
\begin{equation} \label{est_dist}
\sqrt{N}(\hat{H}_{N, k} - H) \xrightarrow{d} N(0, \sigma^2)
\end{equation}
and 
\begin{equation} \label{est_consist}
N \mathbb{E}{(\hat{H}_{N, k} - H)^2} \xrightarrow{} \sigma^2
\end{equation}
as $N \to \infty$ uniformly for $k \in \{ k_{0}^{*}, ...,  k_{1}^{*} \}$, where $\sigma^2 = Var(log(f(x))$, for density function $f(x)$.

(This combines Theorem \ref{paper1_T1} with stronger conditions; hence we can now say that $\hat{H}_{N, k}$ is an consistent and asymptotically unbiased estimator of exact entropy $H$.)
\end{theorem}

Theorem \ref{paper4_T5} holds, according to the central limit theorem, on the estimator for entropy $\hat{H}_{N, k}$;
\begin{equation}
\frac{\hat{H}_{N, k} - \mathbb{E}{\hat{H}_{N, k}}}{\sqrt{Var(\hat{H}_{N, k})}} \xrightarrow{d} N(0, \sigma^2) \nonumber
\end{equation}
By Theorem \ref{paper4_T4}, we can assume that $Var(\hat{H}_{N, k}) = \frac{Var(\log f(x))}{N} \approx \frac{1}{N}$, as for large $N$, the variance of the logarithm of the density function stays constant. Thus, the left side of the central limit theorem above can be written as;
\begin{align*}
\frac{\hat{H}_{N, k} - \mathbb{E}{\hat{H}_{N, k}}}{\sqrt{Var(\hat{H}_{N, k})}} &= \sqrt{N}(\hat{H}_{N, k} - \mathbb{E}{\hat{H}_{N, k}}) \\
&= \sqrt{N}[(\hat{H}_{N, k} - H) + (H - \mathbb{E}{\hat{H}_{N, k}})] \\
&= \sqrt{N}(\hat{H}_{N, k} - H) + \sqrt{N}(H - \mathbb{E}{\hat{H}_{N, k}})
\end{align*}
and as $N \to \infty$ this tends to the normal distribution, $N(0, \sigma^2)$. So we can say that $\sqrt{N}(\hat{H}_{N, k} - H) \xrightarrow{d} N(0, \sigma^2)$ while $\sqrt{N} (H - \mathbb{E}{\hat{H}_{N, k}}) \to \sigma^2$, which is equivalent to the properties stated in Theorem \ref{paper4_T5}.

Later, I will further discuss this estimator for the specific dimensions $d=1$ and $d=2$; however, it is important to note that for larger dimensions this estimator is not accurate. When $d=4$, equations (\ref{est_dist}) and (\ref{est_consist}) no longer hold but the estimator $\hat{H}_{N, k}$, defined by (\ref{KLest}), is still root-N consistent, provided k is bounded. Also, when $d \geq 5$ there is a non trivial bias, regardless of the choice of $k$. There is a new proposed estimator, formed as a weighted average of $\hat{H}_{N, k}$ for different values of $k$, where $k$ depends on the choice of $N$, explored in PAPER 4 (TODO reference). 

Moreover, this paper focuses only on distributions for $d \leq 3$, more specifically, I will first be considering samples from 1-dimensional distributions, $d=1$. Therefore, the volume of the 1-dimensional Euclidean ball is given by $V_{1} = \frac{\pi^{\frac{1}{2}}}{\Gamma (\frac{3}{2})} = \frac{\sqrt{\pi}}{\frac{\sqrt{\pi}}{2}} = 2$. Hence the Kozachenko-Leonenko estimator is of the form;
\begin{equation} \label{KLest_d=1}
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(k),i}(N-1)}{e^{\Psi(k)}} \right]
\end{equation}
 Later, I will be considering samples from 2-dimensional distributions; thus, $d=2$ and the volume of the 2-dimensional Euclidean ball is given by $V_{2} = \frac{\pi^{\frac{2}{2}}}{\Gamma (2)} = \frac{\pi}{1} = \pi$. Hence, the estimator takes the form;
\begin{equation} \label{KLest_d=2}
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\pi \rho_{(k),i}^{2} (N-1)}{e^{\Psi(k)}} \right]
\end{equation}



\subsection{Other Estimators}
\begin{itemize}
\item The estimator for $H_{2}^{*}$ from paper 5
\item The estimator for higher dimensions $d$, from paper 4
\end{itemize}



\section{Monte-Carlo Simulations}

In this section I will explore simulations of the bias of estimator (\ref{KLest}) in comparison to the size of the sample estimated from, with respect to different values of k; by exploring 1-dimensional distributions and then progressing onto 2-dimensional. Firstly, the distributions considered will be analysed to determine if they satisfy the conditions stated for Theorems \ref{paper1_T1}, \ref{paper1_T2}, \ref{paper4_T4} and \ref{paper4_T5} to hold. Then, I will explore the estimator of entropy for simulations of samples  from certain distributions, for different values of $k$.

The motivation for these simulations is to explore the consistency of this estimator for different values of $k$; the relationship between the size of the bias of the estimator $\hat{H}_{N, k}$, $Bias(\hat{H}_{N, k})$,  and the sample size, $N$. Throughout this analysis we will be considering the absolute value of this bias, since when considering its logarithm, we need a positive value. Using Theorem \ref{paper4_T5}, we can write that the bias of the estimator approaches 0 as $N \to \infty$. This is because we can write $Bias(\hat{H}_{N, k} ) = \mathbb{E}(\hat{H}_{N, k}) - H$, which in equation (\ref{est_consist}) implies $Bias(\hat{H}_{N, k}) \to 0$ as $N \to \infty$. Thus, there must be a type of inverse relationship between the modulus of the bias of the estimator, $|Bias(\hat{H}_{N, k})|$, and $N$. We believe this relationship is of the form;
\begin{equation} \label{bias}
|Bias(\hat{H}_{N, k})| = \frac{c}{N^a}
\end{equation}
for $a, c > 0$. By taking the logarithm of this, we can generate a linear relationship, which is easier to analyse, and is given by;
\begin{equation} \label{logbias}
log|Bias(\hat{H}_{N, k})| = log(c) - a [log(N)]
\end{equation}
I will investigate the consistency of this estimator for a sample from the normal distribution, dependent on the value of $k$, this mean finding the optimum value of $k$ for which $|Bias(\hat{H}_{N, k})| \to 0$ for $N \to \infty$. For the relationship in equation (\ref{bias}), this will happen for large values of $a$ and relatively small $c$, as $N \to \infty$. Moreover, I wish to also examine the dependence of $c$ on the value of $k$. As I wish to consider the difference in accuracy of the estimator when using different values of k, let us denote the approximate values for $a$ and $c$ dependent on $k$ as $a_{k}$ and $c_{k}$.




\subsection{1-dimensional Normal Distribution} \label{Normal_d=1}

I will begin by exploring entropy of samples from the normal distribution $N(0, \sigma^2)$, where without loss of generality we can use the mean $\mu = 0$ and change the variance $\sigma^2$ as needed. The normal distribution has an exact formula to work out the entropy, given the variance $\sigma^2$. Using equation (\ref{ShaEnt}) and the density function for the normal distribution $f(x) = \frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)}$ for $x \in \mathbb{R}$, given $\mu = 0$. We can write the exact entropy for the normal distribution, using equation (\ref{ShaEnt});
\begin{align}
H &= - \int_{x : f(x) > 0} f(x) log(f(x)) dx \nonumber \\
&= - \int_{\mathbb{R}} \frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)} log \left[\frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)} \right] dx \nonumber \\
&=  \int_{\mathbb{R}} \frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)} \left( log(\sqrt{(2\pi)}\sigma) +  \frac{x^2}{2\sigma^2} \right) \nonumber \\
&= \frac{log(\sqrt{(2\pi)}\sigma)}{\sqrt{(2\pi)} \sigma} \int_{\mathbb{R}} \exp{ \left( \frac{-x^2}{2\sigma^2} \right)} dx +  \frac{1}{2\sqrt{(2\pi)} \sigma} \int_{\mathbb{R}} \frac{x^2}{2\sigma^2}  \exp{ \left( \frac{-x^2}{\sigma^2} \right)} dx \nonumber \\
&=  log(\sqrt{(2\pi)}\sigma) + \frac{1}{2} \nonumber 
\end{align}
Thus the exact entropy for the normal distribution is given by 
\begin{equation}\label{NormalEnt}
H =  log(\sqrt{(2\pi e)}\sigma) 
\end{equation}
I will first explore samples from 1-dimensional standard normal distribution with mean $\mu = 0$ and variance $\sigma^2 = 1$, $N(0, 1)$, to consider the behavior of the Kozachenko-Leonenko estimator. The exact entropy of this distribution is given by equation (\ref{NormalEnt}), with $\sigma^2=1$;
\begin{equation} \label{normal_exact}
H = log(\sqrt{(2\pi e)}) \approx 1.418939
\end{equation}

Since, I am first considering the 1-dimensional normal distribution, the estimator takes the form in equation (\ref{KLest_d=1}), which is given by;
\begin{equation}
\hat{H}_{N, k} =  \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(k),i}(N-1)}{e^{\Psi(k)}} \right]\nonumber
\end{equation}




\subsubsection{Estimator Conditions} \label{N_Conditions}
The normal distribution satisfies Theorem \ref{paper1_T1}, using that the density function is given by $f(x) = C \exp{ \left( \frac{-x^2}{2\sigma^2} \right)}$ for $x \in \mathbb{R}$, given $\mu = 0$ and where $C:= \frac{1}{\sqrt{(2\pi)} \sigma} > 0$. Then by equation (\ref{paper1_T1_eq1}), considering that here $d=1$, taking some $\epsilon >0$, the first condition of the Theorem is satisfied;
\begin{align} \nonumber
\int_{\mathbb{R}} | log(f(x))|^{1 + \epsilon} f(x) dx  &= C \int_{\mathbb{R}} \left| log \left( C \right) -  \left( \frac{x^2}{2\sigma^2} \right) \right|^{1 + \epsilon} \exp{ \left( \frac{-x^2}{2\sigma^2} \right)} dx \\ \nonumber
&< \int_{\mathbb{R}} \frac{|x|^{1 + \epsilon}}{ \exp{x^2}} dx \\ \nonumber
&< \int_{-\infty}^{\infty} \frac{|x|}{ \exp{x^2}} dx < \infty \nonumber
\end{align}
Also, the second condition, equation (\ref{paper1_T1_eq2}), of Theorem \ref{paper1_T1} is satisfied;
\begin{align} \nonumber
\int_{(\mathbb{R})^2} | log(\|x-y\|)|^{1+ \epsilon} f(x) f(y) dx dy  &= C^2 \int_{(\mathbb{R})^2} | log(\|x-y\|)|^{1+ \epsilon} \exp{\left(\frac{-(x^2 + y^2)}{2 \sigma}\right)} dx dy \\ \nonumber
&< \int_{\mathbb{R}} \int_{\mathbb{R}} \frac{| log(\|x\| + \|y\|)|^{1+ \epsilon}}{\exp{(x^2 + y^2)}} dx dy \\ \nonumber
&< \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{| log(\|x\| + \|y\|)|}{\exp{(x^2 + y^2)}} dx dy < \infty  \nonumber
\end{align}
Thus we can say that for the normal distribution, $\hat{H}_{N,k}$ is an asymptotically unbiased estimator for entropy. 

Moreover, the normal distribution satisfies Theorem \ref{paper1_T2}, as it fulfills the first condition shown in equation (\ref{paper1_T2_eq1});
\begin{align} \nonumber
\int_{\mathbb{R}} | log(f(x))|^{2 + \epsilon} f(x) dx  &= C \int_{\mathbb{R}} \left| log \left( C \right) -  \left( \frac{x^2}{2\sigma^2} \right) \right|^{2 + \epsilon} \exp{ \left( \frac{-x^2}{2\sigma^2} \right)} dx \\ \nonumber
&< \int_{\mathbb{R}} \frac{|x|^{2 + \epsilon}}{ \exp{x^2}} dx \\ \nonumber
&< \int_{-\infty}^{\infty} \frac{|x|^2}{ \exp{x^2}} dx < \infty \nonumber
\end{align}
and the second condition, equation (\ref{paper1_T2_eq2});
\begin{align} \nonumber
\int_{(\mathbb{R})^2} | log(\|x-y\|)|^{2+ \epsilon} f(x) f(y) dx dy  &= C^2 \int_{(\mathbb{R})^2} | log(\|x-y\|)|^{2+ \epsilon} \exp{\left(\frac{-(x^2 + y^2)}{2 \sigma}\right)} dx dy \\ \nonumber
&< \int_{\mathbb{R}} \int_{\mathbb{R}} \frac{| log(\|x\| + \|y\|)|^{2+ \epsilon}}{\exp{(x^2 + y^2)}} dx dy \\ \nonumber
&< \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{| log(\|x\| + \|y\|)|^2}{\exp{(x^2 + y^2)}} dx dy < \infty  \nonumber
\end{align}
Henceforth, the estimator $\hat{H}_{N,k}$, for a sample from the normal distribution is a consistent estimator by Theorem \ref{paper1_T2}. 

For Theorems \ref{paper4_T4} and \ref{paper4_T5} to be satisfied by the estimators generated by samples from the normal distribution, this distribution must meet the Conditions \ref{A1}, \ref{A2} and \ref{A3}. 
Firstly, to satisfy Condition \ref{A1}, for density function $f(x) = \frac{1}{\sqrt{(2\pi)}} \exp{ \left( \frac{-x^2}{2} \right)}$ for $x \in \mathbb{R}$, given $\mu = 0$ and $\sigma^2 = 1$, it must be such that;
\begin{itemize}
\item $f$ is bounded - obvious, since for any probability distribution we always have $f(x) \geq 0$, additionally for the normal distribution we have that $f(x) = \frac{1}{\sqrt{(2\pi)}} \exp{ \left( \frac{-x^2}{2} \right)} < 0.4$, $\forall x \in \mathbb{R}$. Hence, $f$ is bounded above and below; so bounded.

\item $f$ is m-times differentiable - using Hermite polynomials, defined as;
\begin{equation}
H_{m}(x) = (-1)^m e^{\frac{x^2}{2}} \frac{d^m}{dx^m} \left(e^{\frac{-x^2}{2}} \right) \nonumber
\end{equation}
multiplying this by the coefficient in the distribution of $f(x)$, $\frac{1}{\sqrt(2 \pi}$, we then get;
\begin{align*}
 \frac{d^m}{dx^m} f(x) &= \frac{H_{m}(x)}{(-1)^m} \frac{1}{\sqrt{2 \pi}} e^{\frac{-x^2}{2}} \\ 
&= \frac{H_{m}(x)}{(-1)^m} f(x) 
\end{align*}
where $\frac{H_{m}(x)}{(-1)^m}$ is a polynomial; thus $f$ is m-times differentiable.

\item $\exists r_{*} > 0$ and a Borel measurable function $g_{*}$, with $\|y-x\| \leq r_{*}$ so that $\|f^{(t)}(x)\| \leq g_{*}(x) f(x)$ and $\|f^{(m)}(x) - f^{(m)}(x)\| \leq g_{*}(x) f(x)\|y - x\|^{\eta}$, for some $g_{*}$ such that $\sup_{\{x : f(x) < \delta\}} g_{*}(x) = O(\delta^{-\epsilon})$ as $\delta \searrow 0$ for some $\epsilon >0$. 

Since we are considering a 1-dimensional distribution, we can write the norms $\| \cdotp \|$ as  $| \cdotp |$. Moreover, considering that for Theorems \ref{paper4_T4} and \ref{paper4_T5}, we have the value of $\beta =2$ and since $m = \lfloor \beta \rfloor =  \lfloor 2 \rfloor = 2 = \beta$ and $\eta = \beta -m$, we have that $\eta =0$. Thus we need  $|f^{(t)}(x)| \leq g_{*}(x) f(x)$, which is obvious by above, in view of writing $|\frac{d^t}{dx^t} f(x)| = g_{*}(x) f(x)$, where we choose $ g_{*}(x) = |\frac{H_{t}(x)}{(-1)^t}| = |H_{t}(x)|$, for $t=1,2,...,m$, and $|f(x)| = f(x)$, since $f(x) >0$. Also, $g_{*}$ is a polynomial and is hence Borel measurable over $\mathbb{R}$, and for any polynomial we obviously have $\sup_{\{x : f(x) < \delta\}} g_{*}(x) = O(\delta^{-\epsilon})$ as $\delta \searrow 0$ for some $\epsilon >0$. Additionally, we need $|f^{(m)}(x) - f^{(m)}(x)| \leq g_{*}(x) f(x)|y - x|^{0} = g_{*}(x) f(x)$. We currently have;
\begin{align*}
|f^{(m)}(x) - f^{(m)}(x)| &= \left| \frac{H_{m}(x)}{(-1)^m} f(x) - \frac{H_{m}(y)}{(-1)^m} f(y) \right| \\
&\leq \left| \frac{H_{m}(x)}{(-1)^m} f(x) \right| + \left| \frac{H_{m}(y)}{(-1)^m} f(y) \right| \\
&= g_{*}(x)f(x) + g_{*}(y)f(y) \\
&\leq g_{*}(x)f(x)
\end{align*}
since we know that $f(x) >0$ for all $x \in \mathbb{R}$, and $g_{*}(x) = |H_{m}(x)| >0$, which is similar to the $g_{*}$ before; thus satisfies the conditions for it.
\end{itemize}

Next, to satisfy Condition \ref{A2}, for the density function $f$ of the normal distribution, must fulfill that;
\begin{itemize}
\item The $\alpha$-moment of $f$ must be finite, so $\int_{\mathbb{R}^{d}} \| x \|^{\alpha} f(x) dx < \infty$ - this is always true for the normal distribution, all of its moments are finite, since they are defined with respect to $\sigma^n$, for some $n$, and $\sigma < \infty$.

\end{itemize}

Lastly, to satisfy Condition \ref{A3}, we must find the values of $k$ for which the estimator provides a uniform convergence for Theorems \ref{paper4_T4} and \ref{paper4_T5}. To do this we must have, for some $\alpha > d = 1$, let $k_{0}^{*}$ and $k_{1}^{*}$ denote two deterministic sequences of positive integers with $k_{0}^{*} \leq k_{1}^{*}$. Taking $\alpha := 2$, we must have;

\begin{itemize}
\item $k_{1}^{*} = O(N^{\tau})$, where $\tau < \min \left\{ \frac{2 \alpha}{5 \alpha + 3d} , \frac{\alpha - d}{2 \alpha} , \frac{4}{4 + 3d} \right\} = \min \left\{ \frac{4}{13}, \frac{1}{4}, \frac{4}{7} \right\} = \frac{1}{4}$, so we can choose $\tau := \frac{2}{9} < \frac{1}{4}$ so that we have $k_{1}^{*} = O(N^{\frac{2}{9}})$

\item $\frac{k_{0}^{*}}{\log^{5}{N}} \to \infty$ - for this to be true we need to choose $k_{0}^{*} := N^A$ for some $A>0$. Considering that $k_{0}^{*} \leq k_{1}^{*}$ and $ k_{1}^{*} = O(N^{\frac{2}{9}})$, thus $A \in (0, \frac{2}{9})$. So we can choose $A := \frac{1}{16}$, which gives that $k_{0}^{*} = O(N^{\frac{1}{16}})$.
\end{itemize}

Thus, on account of the values of $N$ being considered in the simulations; $N=100, 200, ..., 50000$, we have that for the smallest $N=100$, the values of $k$ for which Theorem \ref{paper4_T4} and \ref{paper4_T5} both hold for are $k \in \{1, 2, 3\}$. Moreover, for the largest $N=50,000$, we must consider $k \in \{2, 3, ..., 11\}$.

Overall, due to the above conditions for Theorems \ref{paper1_T1}, \ref{paper1_T2}, \ref{paper4_T4} and \ref{paper4_T5} being met, we can say that the Kozachenko-Leonenko estimator, of a sample from the 1-dimensional normal distribution is an asymptotically unbiased and consistent estimator for entropy, for certain values of $k$. We will explore values of $k \in \{1, 2, ..., 11\}$, more specifically focusing on $k=1, 2, 3, 5, 10$ and exploring the other values in this range if needed to help make a conclusion.




\subsubsection{k=1} \label{N_k=1}
I will be considering k=1 for the estimator of entropy; thus, the estimator will take the form;
\begin{equation} 
\hat{H}_{N, 1} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(1),i} (N-1)}{e^{\Psi(1)}} \right] = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(1),i} (N-1)}{e^{-\gamma}} \right] \nonumber
\end{equation}

I will consider $500$ samples of size $N$ from this distribution, find the estimator in each case and take the average of these estimators to find our entropy estimator, shown in table \ref{normal_k=1_table}. We will then consider the relationship show in (\ref{logbias}) for each sample and again work out the average for the values of a and c, shown in \ref{normal_k=1_graph}.

\begin{table}
\caption{1-dimensional normal distribution, $k=1$} \label{normal_k=1_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 1}$ & $|Bias(\hat{H}_{N, 1})|$ & $Var(Bias(\hat{H}_{N, 1}))$ \\
\midrule[1pt]
100     & 1.405890     & 0.01304883282     & 0.0275142086  \\
200     & 1.411070     & 0.00786872927     & 0.0128689734  \\
500     & 1.416666     & 0.00227293433     & 0.0051416433  \\
1000    & 1.419401     & 0.00046261516     & 0.0028127916  \\
5000    & 1.418469     & 0.00046981107     & 0.0005147810  \\
10000   & 1.417998     & 0.00094067533     & 0.0002472848  \\
25000   & 1.418877     & 0.00006147045     & 0.0001088641  \\
50000   & 1.419286     & 0.00034705584     & 0.0000496450  \\
\hline
\end{tabular}
\end{center}
\end{table}

Considering table \ref{normal_k=1_table}, we can see for a larger value of N, the Bias of the estimator becomes much smaller; the bias decreases from $\approx 0.0130$ to $\approx 0.0003$ as $N$ increases from $100 \to 50,000$. This result is to be expected for an estimator to be asymptotically unbiased, shown in Theorem \ref{paper1_T1}. We can also see that the variance of the bias is decreasing as N increases implying that, not only is the average of the estimator getting closer to the actual value of entropy, also the variability between the estimator of different samples is decreasing, making it a consistent and asymptotically unbiased estimator in practice, as well as in theory.

This relationship between the bias $|Bias(\hat{H}_{N, 1})|$ of the estimator and the size of the sample $N$, can be computed for these sample sizes. Figure \ref{normal_k=1_graph}, shows this relationship of $log|Bias(\hat{H}_{N, 1})|$ against $log(N)$ with a fitted regression line. In this graph, I have considered the values of $N$ up to $50,000$ at intervals of size $100$, where each point is calculate $500$ times and the average estimator is plotted. I have also found the corresponding coefficients $a_{1}$ and $c_{1}$ for the relationship shown in (\ref{bias}). These coefficients tend towards $a_{1} = 0.4594$ and $c_{1} = 0.0249$ for a large $N$, as shown in table \ref{normal_k=1_reg_coeff_table}. 
 
\begin{table}
\caption{1-dimensional normal distribution, $k=1$, coefficients of regression for large $N$} \label{normal_k=1_reg_coeff_table}
\begin{center}
\begin{tabular}{| l | c c|}
\toprule
$N$ & $a_{1}$ & $c_{1}$ \\
\midrule[1pt]
35000 &  0.5047   &   0.0389   \\
40000 &  0.4732   &   0.0315  \\
45000 &  0.4514   &   0.0240  \\
50000 &  0.4954   &   0.0249   \\
\end{tabular}
\end{center}
\end{table}


Thus we have the relationship between the bias and $N$, for $k=1$, to be of the form;
\begin{equation}
|Bias(\hat{H}_{N, 1})| \approx \frac{0.0249}{N^{0.4594}} \nonumber
\end{equation}
On their own these coefficients show that there is a negative relationship between $log(|Bias(\hat{H}_{N, 1})|)$ and $log(N)$. This implies that the relationship between the Bias and N is such that as N increases the Bias decreases to 0. Hence, creating a consistent estimator for entropy, when considering the 1-dimensional normal distribution with k=1 in the Kozachenko-Leonenko estimator. However, to understand better the meaning of this relationship we must compare this to coefficients found of the regression relationships for different values of $k$, and for different distributions.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_k=1_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 1})|$ against $\log(N)$}
  \label{normal_k=1_graph}
\end{figure}




\subsubsection{k=2} \label{N_k=2}
I am now going to examine the case where k=2 in the Kozachenko-Leonenko estimator, to compare the results of simulations from this estimator with that for k=1. Here the estimator will take the form;
\begin{equation}
\hat{H}_{N, 2} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(2),i} (N-1)}{e^{\Psi(2)}} \right] = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(2),i} (N-1)}{e^{-\gamma + 1}} \right] \nonumber
\end{equation}

I wish to explore, in a similar manner as for k=1, the changes in the bias of the estimator depending on a change in N. Additionally, later I will make the comparison between the regression coefficients for different values of k. I will again consider 500 samples of size N from the 1-dimensional standard normal distribution $N(0, 1)$, the results from the analysis is shown in table \ref{normal_k=2_table}.

\begin{table}
\caption{1-dimensional normal distribution, $k=2$} \label{normal_k=2_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 2}$ & $|Bias(\hat{H}_{N, 2})|$ & $Var(Bias(\hat{H}_{N, 2}))$ \\
\midrule[1pt]
100     & 1.408856     & 0.0100827948     & 0.01357417708  \\
200     & 1.411165     & 0.0077730666     & 0.00688250329  \\
500     & 1.419158     & 0.0002199163     & 0.00296693934  \\
1000    & 1.415719     & 0.0032197158     & 0.00141616592  \\
5000    & 1.418236     & 0.0007026416     & 0.00028872533  \\
10000   & 1.418656     & 0.0002824567     & 0.00014348493  \\
25000   & 1.418376     & 0.0005620780     & 0.00005791073  \\
50000   & 1.418681     & 0.0002574343     & 0.00002956529  \\
\hline
\end{tabular}
\end{center}
\end{table}

We can see that, as expected, the Bias of the estimator decreases from $\approx 0.0100$ when $N=100$ to $\approx 0.0002$ when $N=50,000$. This is showing that the consistency condition is being met since as $N \to \infty$ we have $|Bias(\hat{H}_{N, 2})| \to 0$, which is equivalent to saying $\lim_{N \to \infty} \mathbb{E} (\hat{H}_{N, k}) = H$, Theorem \ref{paper1_T1}. We also have that the variance of the bias of these estimators decrease as $N \to \infty$, as expected. In relation to $k=1$, we can see that the bias of this estimator for $k=2$ decreases at a similar pace as $N \to \infty$; $|Bias(\hat{H}_{N, 1})| \approx 0.0130 \to 0.0003$ and $|Bias(\hat{H}_{N, 2})| \approx 0.0101 \to 0.0002$, implying that from this analysis we cannot decide which value of $k$ generates a better estimator.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_k=2_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 2})|$ against $\log(N)$}
  \label{normal_k=2_graph}
\end{figure}

We have found the coefficients for the equation (\ref{bias}), for $k=2$, which are given by $a_{2} = 0.5998$ and $c_{2} = 0.0746$, thus;
\begin{equation}
|Bias(\hat{H}_{N, 2})| \approx \frac{0.0746}{N^{0.5998}} \nonumber
\end{equation}

Again, this shows the relationship one would expect, that $|Bias(\hat{H}_{N, 2})| \to 0$ as $N \to \infty$. This relationship for $k=2$ is stronger than that for $k=1$ since $a_{1} \leq a_{2}$. A full comparison of the relationship of $|Bias(\hat{H}_{N, 1})|$ to $N$ and $|Bias(\hat{H}_{N, 2})|$ to $N$ is given in section \ref{N_compare_k}.




\subsubsection{k=3} \label{N_k=3}
Again, for $k=3$, I will examine $500$ samples of size $N$ from the standard normal distribution considered before; with estimator of the form;
\begin{equation}
\hat{H}_{N, 3} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(3),i} (N-1)}{e^{\Psi(3)}} \right] = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(3),i} (N-1)}{e^{-\gamma + 1 + \frac{1}{2}}} \right] \nonumber
\end{equation}

\begin{table}
\caption{1-dimensional normal distribution, $k=3$} \label{normal_k=3_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 3}$ & $|Bias(\hat{H}_{N, 3})|$ & $Var(Bias(\hat{H}_{N, 3}))$ \\
\midrule[1pt]
100     & 1.398784     & 0.0201546812     & 0.01210622150  \\
200     & 1.412908     & 0.0060302660     & 0.00530612702  \\
500     & 1.414035     & 0.0049035937     & 0.00223855589  \\
1000    & 1.416105     & 0.0028340080     & 0.00107754839  \\
5000    & 1.420184     & 0.0012459298     & 0.00022320970  \\
10000   & 1.418351     & 0.0005874791     & 0.00011630350  \\
25000   & 1.419115     & 0.0001760980     & 0.00004286406  \\
50000   & 1.418853     & 0.0000851863     & 0.00002257717  \\
\hline
\end{tabular}
\end{center}
\end{table}

The results of the comparison between the actual value and the estimated value of entropy, for different values of $N$, are displayed in table \ref{normal_k=3_table}. This shows again, that the Kozachenko-Leonenko estimator for entropy, $\hat{H}_{N, 3} \to 0$, as $N \to \infty$, as more specifically $\hat{H}_{50000, 3} \approx 0.00009$. However, comparing these results to those for $k=1, 2$, which had similar bias as $N \to \infty$, we can see that for $k=3$, $|Bias(\hat{H}_{N, 3})| \approx 0.0202 \to 0.00009$. So for larger $N$, the estimator with $k=1$ or $k=2$ would be less appropriate to use, since the bias is slightly larger than for the estimator using $k=3$.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_k=3_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 3})|$ against $\log(N)$}
  \label{normal_k=3_graph}
\end{figure}

The graph showing the relationship given by \ref{logbias} is shown in figure \ref{normal_k=3_graph}. The have found the coefficients for the formula \ref{bias}, for the graph shown with $k=3$ are given by $a_{3} = 0.6443$ and $c_{3} = 0.1156$, thus;
\begin{equation}
|Bias(\hat{H}_{N, 3})| \approx \frac{0.1156}{N^{0.6443}} \nonumber
\end{equation}
So for $N=50,000$, this implies that $|Bias(\hat{H}_{50000, 3})| \approx 0.000176$, which is close to the value found in table \ref{normal_k=3_table}, that $|Bias(\hat{H}_{50000, 3})| \approx 0.000085$. We here have that $a_{3} \geq a_{2} \geq a_{1}$, hence, according to this analysis, when $k=3$ we have a stronger negative relationship between the bias and $N$. A full comparison of the regression analysis for each $k$ is conducted in section \ref{N_compare_k}.




\subsubsection{k=5} \label{N_k=5}
Now we consider the estimator, shown in equation \ref{KLest}, for k=5. This takes the form;
\begin{equation}
\hat{H}_{N, 5} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(5),i}(N-1)}{e^{\Psi(5)}} \right] \nonumber
\end{equation}

\begin{table}
\caption{1-dimensional normal distribution, $k=5$} \label{normal_k=5_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 5}$ & $|Bias(\hat{H}_{N, 5})|$ & $Var(Bias(\hat{H}_{N, 5}))$ \\
\midrule[1pt]
100     & 1.391834     & 0.02710439666     & 0.00807261026  \\
200     & 1.405356     & 0.01358205942     & 0.00425419382  \\
500     & 1.411436     & 0.00750282472     & 0.00168848112  \\
1000    & 1.415091     & 0.00384740080     & 0.00091927735  \\
5000    & 1.418150     & 0.00078877480     & 0.00018941496  \\
10000   & 1.418648     & 0.00029099525     & 0.00008767553  \\
25000   & 1.418879     & 0.00005917171     & 0.00003243503  \\
50000   & 1.418644     & 0.00029451951     & 0.00001705529  \\
\hline
\end{tabular}
\end{center}
\end{table}

Comparing this to the exact entropy for the standard normal distribution, (\ref{normal_exact}), gives table \ref{normal_k=5_table}. Here, the $|Bias(\hat{H}_{N, 5})|$ decreases as $N$ goes from $100 \to 25,000$, but at $50,000$ this jumps to a larger number. Up to $25,000$ indicates that the estimator is becoming closer to the actual value, the jump at $50,000$  could be due to a number of reasons. 

Firstly, this could indicate that for $k=5$, this estimator becomes less efficient, and doesn't satisfy the property ...  as strongly as smaller values of $k$ have done so far. Secondly,this could just be an error in the data for $|Bias(\hat{H}_{50000, 5})|$ ; since we are only considering a relative small number of samples, 500, and are taking the average of this, we could just have an outlier. Lastly, there could be an error in the previous two data points, $|Bias(\hat{H}_{25000, 5})|$ and $|Bias(\hat{H}_{10000, 5})|$, causing us to either believe it is decreasing, when it isn't.

To determine the reason for this jump of Bias in the wrong direction, I will examine $|Bias(\hat{H}_{50000, 5})|$ for 3,000 samples and see if this is consistent with the previous findings. I have found this number to be; 
\begin{equation} 
|Bias(\hat{H}_{50000, 5})|  \approx 0.00006034936 \nonumber
\end{equation}
This gives us a much smaller bias than $|Bias(\hat{H}_{50000, 5})|$ shown in table \ref{normal_k=5_table}, however, this is still not smaller than the value of $|Bias(\hat{H}_{25000, 5})|$ shown in the same table. This could mean that for $k=5$, the estimator doesn't satisfy the consistency condition as strongly as the previous estimators for $k=1, 2, 3$.

However, if we consider the graph in figure \ref{normal_k=5_graph}, we can see an obvious negative relationship between the logarithm of $|Bias(\hat{H}_{N, 5})|$ and the logarithm of $N$. Henceforth, I would expect that the numbers above are within the standard error range, so that for $k=5$, we do have an estimator which is asymptotically unbiased, as required for Theorem \ref{paper1_T1}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_k=5_plot.png}
  \end{center} 
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 5})|$ against $\log(N)$}
  \label{normal_k=5_graph}
\end{figure}

By plotting the graph in figure \ref{normal_k=5_graph}, we have found the coefficients for the formula \ref{bias}, for $k=5$, which are given by $a_{5} = 0.7568$ and $c_{5} = 0.3557$ , thus;
\begin{equation}
|Bias(\hat{H}_{N, 5})| \approx \frac{0.3557}{N^{0.7568}} \nonumber
\end{equation}
For these coefficients we have that $a_{5} \geq a_{3} \geq a_{2} \geq a_{1}$, thus according to this analysis, we have a stronger consistency of our estimator for $k=5$, in comparison to $k=1, 2, 3$. This comparison will be considered in more detail in section \ref{N_compare_k}.




\subsubsection{k=10} \label{N_k=10}
The last estimator for the entropy of a sample from the standard normal distribution that I wish to explore is that for $k=10$. Here, the estimator takes the form;
\begin{equation}
\hat{H}_{N, 10} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(10),i}(N-1)}{e^{\Psi(10)}} \right] \nonumber
\end{equation}
The results for the comparison between this estimator and \ref{normal_exact} are displayed in table \ref{normal_k=10_table}.

\begin{table}
\caption{1-dimensional normal distribution, $k=10$} \label{normal_k=10_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 10}$ & $|Bias(\hat{H}_{N, 10})|$ & $Var(Bias(\hat{H}_{N, 10}))$ \\
\midrule[1pt]
100     & 1.375699     & 0.0432399931     & 0.00678770166  \\
200     & 1.391934     & 0.0270050257     & 0.00293164825  \\
500     & 1.407625     & 0.0113137866     & 0.00148669638  \\
1000    & 1.411684     & 0.0072549983     & 0.00067990485  \\
5000    & 1.417306     & 0.0016322988     & 0.00013650841  \\
10000   & 1.418196     & 0.0007429215     & 0.00006783354  \\
25000   & 1.418356     & 0.0005825702     & 0.00003162161  \\
50000   & 1.418790     & 0.0001488755     & 0.00001318863  \\
\hline
\end{tabular}
\end{center}
\end{table}

Here, we can again see that this estimator is asymptotically unbiased, satisfying Theorem \ref{paper1_T1}, as $\hat{H}_{N, 10} \approx  0.0432 \to 0.0001$ as $N \approx 100 \to 50,000$. Comparing this to previous values of $k$, we can see that the bias changes decreases in a similar manner to that for $k=1, 2, 3$.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_k=10_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 10})|$ against $\log(N)$}
  \label{normal_k=10_graph}
\end{figure}

From the graph in Figure \ref{normal_k=10_graph}, we have the relationship between the Bias and $N$ taking the form;
\begin{equation}
|Bias(\hat{H}_{N, 5})| \approx \frac{5.5942}{N^{1.0055}} \nonumber
\end{equation}
So we have the coefficients of the regression formula (\ref{bias}) as $a_{10} = 1.0055$ and $c_{k} = 5.5942$. These numbers are very contrasting to the previous coefficients that we have seen for the other values of $k$ in this distribution. This could indicate, either an error in the simulation, or that for $k=10$, the results are very different to that for the smaller values of $k$. I will explore this difference in more details in section \ref{N_compare_k}.




\subsubsection{Comparison of k} \label{N_compare_k}
The above analysis, sections \ref{N_k=1} to \ref{N_k=10} is done to examine the difference in the bias of the estimator for different values of k. Considering the above samples, for $N=25,000$ and $N=50,000$, we can create a table to compare the values of the bias of the estimator for the different values of $k$ considered.

\begin{table}
\caption{1-dimensional normal distribution, comparison of $k$} \label{normal_kcompare_table}
\begin{center}
\begin{tabular}{| l | c c c c|} 
\toprule
$k$ & $|Bias(\hat{H}_{25000, k})|$ & $Var(Bias(\hat{H}_{25000, k}))$ & $|Bias(\hat{H}_{50000, k})|$ & $Var(Bias(\hat{H}_{50000, k}) )$ \\
\midrule[1pt]
1    & 0.00006147045   & 0.0001088641     & 0.00034705584   & 0.0000496450   \\
2    & 0.0005620780     & 0.00005791073   & 0.0002574343     & 0.00002956529 \\
3    & 0.0001760980     & 0.00004286406   & 0.0000851863     & 0.00002257717 \\
5    & 0.00005917171   & 0.00003243503   & 0.00029451951   & 0.00001705529 \\
10  & 0.0005825702     & 0.00003162161   & 0.0001488755     & 0.00001318863 \\
\hline
\end{tabular}
\\[10pt]
\caption*{This table is comparing the values of $|Bias(\hat{H}_{N, k})|$ for the values of $k$ explored in tables \ref{normal_k=1_table}, \ref{normal_k=2_table}, \ref{normal_k=3_table}, \ref{normal_k=5_table} and  \ref{normal_k=10_table} with $N=25,000$ and $N=50,000$, when the estimator is taken over $500$ samples}
\end{center}
\end{table}

The results shown in table \ref{normal_kcompare_table} are inconclusive in determining which value of k generates the best estimator, with the smallest bias. However, these results are consistent in showing that for the larger value of N, the smaller the variance in the estimator. The results for the bias are not conclusive; because, for $N=25,000$ we can see that with $k=1, 5$ and possibly $k=3$ have a slight smaller bias than the others. However, when $N=50,000$ we find that for $k=3, 10$ we have the smallest values of bias. These are inconsistent with one and other. To further examine this, I will now generate a table for values $k=1, 2, 3, 5, 10$ with $N=50,000$ in all cases. Moreover, this time I will consider $3,000$ samples of this size, not the $500$ considered before, and will find the mean and variance of the bias of this estimator.

\begin{table}
\caption{1-dimensional normal distribution, comparison of $k$} \label{normal_kcompare2_table}
\begin{center}
\begin{tabular}{| l | c c |} 
\toprule
$k$ &  $|Bias(\hat{H}_{50000, k})|$ & $Var(Bias(\hat{H}_{50000, k}))$ \\
\midrule[1pt]
1      & 0.00013495546     & 0.00005116758  \\
2      & 0.00012647214     & 0.00002868082  \\
3      & 0.00003478968     & 0.00002299754  \\
5      & 0.00006034936     & 0.00001733369  \\
10    & 0.00022455715     & 0.00001409080  \\
\hline
\end{tabular}
\\[10pt]
\caption*{This table is comparing the values of $|Bias(\hat{H}_{N, k})|$ for the values of $k$ explored before now with only $N=50,000$ and the estimator being taken over $3,000$ samples}
\end{center}
\end{table}

The results in table \ref{normal_kcompare2_table}, consider the scenario set out above, and we can see that $|Bias(\hat{H}_{N, k})|$ is the smallest, for sample size $N=50,000$, when $k=3$, which is consistent with the results found in table \ref{normal_kcompare_table}. So from these simulations, we can conclude that for large $N$, the consistency condition is best satisfied when $k=3$. 
Interestingly, the $Var|Bias(\hat{H}_{50000, k})| \to 0$ for $k \to 10$, but this is to be expected, as by the definition of the estimator using the nearest neighbour method. Taking a larger $k$ in the nearest neighbour method will produce less varied results, this is because more smoothing takes place for a larger $k$, eventually - if $k$ is made large enough - the output will be constant and the variance negligible regardless of the inputted values. Thus, considering the variance of the bias of the estimator in comparison to $k$ is not necessarily informative. 
However, considering the variance of the bias of the estimator in comparison to $N$ is informative. Theorem \ref{paper4_T4} states that the variance of the estimator becomes $\approx \frac{Var(log(f(x))}{N}$ as $N \to \infty$, and previously I have stated that, if $Var(log(f(x))$ is constant, this becomes 0 for large $N$. This is consistent with the results found considering the variance of the bias, $Var|\hat{H}_{N, k} - H|$, instead of just variance of the estimator, $Var(\hat{H}_{N, k})$. But, since variance is not linear; $Var(X+a)=Var(X)$ for some $a \in \mathbb{R}$, we can say that $Var(\hat{H}_{N, k} - H) = Var(\hat{H}_{N, k})$, thus  $Var|\hat{H}_{N, k} - H| \leq Var(\hat{H}_{N, k} - H) = Var(\hat{H}_{N, k})$. So if $Var(\hat{H}_{N, k}) \to 0$ as $N \to \infty$, then we also must have $Var|\hat{H}_{N, k} - H| \to 0$ as $N \to \infty$, which we have confirmed to be true numerically throughout this analysis.

\begin{table}
\caption{Comparison of coefficients of regression $a_{k}$ and $c_{k}$ from equation \ref{bias}, for 1-dimensional normal distribution} \label{normal_a_c_compare_table}
\begin{center}
\begin{tabular}{| l | l l |} 
\toprule
$k$ &  $a_{k}$ & $c_{k}$ \\
\midrule[1pt]
1      & 0.4594     & 0.0249  \\
2      & 0.5998     & 0.0746  \\
3      & 0.6443     & 0.1156  \\
5      & 0.7568     & 0.3557  \\
10    & 1.0055     & 5.5942  \\
\hline
\end{tabular}
\\[10pt]
\end{center}
\end{table}

Table \ref{normal_a_c_compare_table}, shows that as $k$ runs from $1 \to 10$, we have that $a_{k}$ and $c_{k}$ both increase, with a large jump between $k=5$ and $k=10$. The higher the value of $a_{k}$, the stronger the negative relationship is between the two variables in question, so for a larger values of $a_{k}$, we have that $|Bias(\hat{H}_{N, k})| \to 0$ for large $N$ faster than smaller values of $a_{k}$. This is due to the relationship between $|Bias(\hat{H}_{N, k})|$ and $a_{k}$ shown in equation (\ref{bias}). Thus, considering a large sample size, say $N=100,000$, we can find the bias of the Kozachenko-Leonenko estimator according to the regressional relationship for each $k$; $|Bias(\hat{H}_{N, k})| = \frac{c_{k}}{N^{a_{k}}}$. We find that;
\begin{gather*}
|Bias(\hat{H}_{100000, 1})| \approx  \frac{0.0249}{100000^{0.4594}}   \approx 0.00012566 \\
|Bias(\hat{H}_{100000, 2})| \approx  \frac{0.0746}{100000^{0.5998}}   \approx 0.00007477 \\
|Bias(\hat{H}_{100000, 3})| \approx  \frac{0.1156}{100000^{0.6443}}   \approx 0.00006942 \\
|Bias(\hat{H}_{100000, 5})| \approx  \frac{0.3557}{100000^{0.7568}}   \approx 0.00005949 \\
|Bias(\hat{H}_{100000, 10})| \approx  \frac{5.5942}{100000^{1.10055}}   \approx 0.00005251 
\end{gather*}

This shows that the bias decreases slightly faster for a higher values of $k$. I have also compared these relationships through a graph of the regression lines found from plotting the simulations above, shown in Figure \ref{normal_comparison_graph}. From this we can see obviously that for $k=10$, the regression line is steepest, indicating the strongest relationship between the logarithm of the bias and the logarithm of $N$. This implies that for a larger value of $k$, the estimator is stronger, to explore this I will plot the regression lines for some larger values of $k$ to see if this is true.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Normal_comparison.png}
  \end{center}
\caption{Plot of regression lines for $\log|Bias(\hat{H}_{N, k})|$ against $\log(N)$, for $k=1, 2, 3, 5, 10$, for samples from the normal distribution}
  \label{normal_comparison_graph}
\end{figure}

I have also considered a plot of the values of $a_{k}$ and $c_{k}$ against $k$, to see if there is a clear relationship between the value of $k$ and the coefficients,  $a_{k}$ and $c_{k}$. This relationship is depicted in figure \ref{}.

what does this show? ... TODO







\subsection{1-dimensional Uniform distribution} \label{Uniform_d=1}

I will now explore the entropy of samples from the 1-dimensional uniform distribution, $U[a, b]$. This distribution also has an exact formula to work out the entropy for. We can find this formula by considering the density function, $f$, from the uniform distribution, which is given by;
\[
f(x) =  \begin{cases} 
      \frac{1}{b-a} & a \leq x \leq b \\
      0 & otherwise
   \end{cases}
\]
Using the definition of Shannon entropy given in equation (\ref{ShaEnt}), we can find the exact entropy for the uniform distribution;
\begin{align*}
H &= - \int_{x : f(x) > 0} f(x) log(f(x)) dx \\ 
&= - \int_{a}^{b} \frac{1}{b-a} log \left[ \frac{1}{b-a} \right] dx  \\
&= - \frac{1}{b-a} log \left[ \frac{1}{b-a} \right]  \int_{a}^{b} dx  \\
&= -  log  \left[ \frac{1}{b-a} \right] 
\end{align*}
Thus, the actual value of entropy for the uniform distribution is given by;
\begin{equation} \label{UnifEnt}
H = log [ b-a ]
\end{equation}

 Similarly to the 1-dimensional normal distribution, we have for the 1-dimensional uniform distribution that $d=1$ so $V_{1} = 2$, thus our estimator takes the form of equation (\ref{KLest_d=1});
\begin{equation}
\hat{H}_{N, k} =  \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(k),i}(N-1)}{e^{\Psi(k)}} \right]\nonumber
\end{equation}

Moreover, the samples considered will not be from the standard uniform, but from the the uniform distribution $U[0,100]$. This is because, using the standard uniform, $U[0,1]$, would fail since taking $N=50,000$ samples between 0 and 1 would generate problems as the pdf would be $f(x) = 1 , \quad 0 \leq x \leq 1$, which would incur working on a very small scale; i.e taking a points with distance between them as $\approx 0.00002$ along the x-direction. Thus, I will be using the pdf $f(x) = 0.01 , \quad 0 \leq x \leq 100$, which is from the $U[0,100]$ distribution and gives the exact entropy to be;
\begin{equation} \label{uniform_exact}
H = log(100) \approx 4.605170
\end{equation}




\subsubsection{Estimator Conditions} \label{U_Conditions}

The uniform distribution satisfies Theorem \ref{paper1_T1}, firstly by considering equation (\ref{paper1_T1_eq1}) with $d=1$ and the distribution $f(x)=C$ for $a \leq x \leq b$, where $C = \frac{1}{b-a} >0$, we have for some $\epsilon >0$;
\begin{align} \nonumber
\int_{\mathbb{R}} | log(f(x))|^{1 + \epsilon} f(x) dx  &= C \int_{a}^{b} | log (C) |^{1 + \epsilon} dx \\ \nonumber 
&<  \int_{a}^{b} | \hat{C} |^{1 + \epsilon}  dx  < \infty \nonumber
\end{align}
Also, the second condition, equation (\ref{paper1_T1_eq2}), of Theorem \ref{paper1_T1} is satisfied;
\begin{align} \nonumber
\int_{(\mathbb{R})^2} | log(\|x-y\|)|^{1+ \epsilon} f(x) f(y) dx dy  &= C^2 \int_{a}^{b} \int_{a}^{b} | log(\|x-y\|)|^{1+ \epsilon} dx dy \\ \nonumber
&< \int_{a}^{b} \int_{a}^{b} | log(\|x\| + \|y\|)| dx dy < \infty  \nonumber
\end{align}
Thus we can say that for the normal distribution, $\hat{H}_{N,k}$ is an asymptotically unbiased estimator for entropy. 

Moreover, the uniform distribution also satisfies Theorem \ref{paper1_T2}, as it fulfills the first condition shown in equation (\ref{paper1_T2_eq1});
\begin{align} \nonumber
\int_{\mathbb{R}} | log(f(x))|^{2 + \epsilon} f(x) dx  &= C \int_{a}^{b} | log  (C)|^{2 + \epsilon} dx \\ \nonumber
&< \int_{a}^{b} \hat{C}^{2 + \epsilon} dx < \infty \nonumber
\end{align}
and the second condition, equation (\ref{paper1_T2_eq2});
\begin{align} \nonumber
\int_{(\mathbb{R})^2} | log(\|x-y\|)|^{2+ \epsilon} f(x) f(y) dx dy  &= C^2 \int_{a}^{b} \int_{a}^{b}| log(\|x-y\|)|^{2+ \epsilon} dx dy \\ \nonumber
&< \int_{a}^{b} \int_{a}^{b} | log(\|x\| + \|y\|)|^2 dx dy < \infty \nonumber
\end{align}

For Theorems \ref{paper4_T4} and \ref{paper4_T5} to be satisfied by the estimators generated by samples from the uniform distribution, this distribution must meet the Conditions \ref{A1}, \ref{A2} and \ref{A3}. 
Firstly, to satisfy Condition \ref{A1}, for the density function $f(x) = 0.01$ for $0 \leq x \leq 100$, it must be such that;
\begin{itemize}
\item f is bounded - obviously, since the density function for the normal distribution is constant for $x \in [a, b]$ and $0$ otherwise; hence is bounded.

\item f is m-times differentiable - TODO

\item $\exists r_{*} > 0$ and a Borel measurable function $g_{*}$, with $\|y-x\| \leq r_{*}$ so that $\|f^{(t)}(x)\| \leq g_{*}(x) f(x)$ and $\|f^{(m)}(x) - f^{(m)}(x)\| \leq g_{*}(x) f(x)\|y - x\|^{\eta}$, for some $g_{*}$ such that $\sup_{\{x : f(x) < \delta\}} g_{*}(x) = O(\delta^{-\epsilon})$ as $\delta \searrow 0$ for some $\epsilon >0$

TODO

\end{itemize}

Next, to satisfy Condition \ref{A2}, for the density function $f$ of the uniform distribution, must fulfill that;
\begin{itemize}
\item The $\alpha$-moment of $f$ must be finite, so $\int_{\mathbb{R}^{d}} \| x \|^{\alpha} f(x) dx < \infty$ - this is true, since for the 1-dimensional uniform distribution, $f(x)$ is constant; thus we would be integrating a polynomial $|x|^{\alpha}$, over a finite interval $a \leq x \leq b$, which is always finite.
\end{itemize}

Lastly, to satisfy Condition \ref{A3}, we must find the values of $k$ for which the estimator provides a uniform convergence for Theorems \ref{paper4_T4} and \ref{paper4_T5}. These values are independent of the distribution that the sample is from, and only depends on the size of the sample, the dimension of the distribution that sample is taken from and the value chosen for $\alpha$, where we have chosen $\alpha = 2$. Thus, the values of $k$ found in section \ref{N_Conditions} are $\{1, 2, ..., 11\}$.

Due to the above conditions for Theorems \ref{paper1_T1}, \ref{paper1_T2}, \ref{paper4_T4} and \ref{paper4_T5} being met, we can say that the Kozachenko-Leonenko estimator, of a sample from the uniform distribution is an asymptotically unbiased and consistent estimator for entropy. 




\subsubsection{k=1} \label{U_k=1}
We will begin by considering 500 samples from the uniform distribution $U[0,100]$, of size $N=100 \to 50,000$ and finding the estimator for $k=1$, which is of the form;
\begin{equation} 
\hat{H}_{N, 1} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(1),i} (N-1)}{e^{-\gamma}} \right] \nonumber
\end{equation}
Considering the bias for this estimator against the actual value (\ref{uniform_exact}) for different samples sizes $N$ gives Table \ref{uniform_k=1_table}.

\begin{table}
\caption{1-dimensional uniform distribution, $k=1$} \label{uniform_k=1_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 1}$ & $|Bias(\hat{H}_{N, 1})|$ & $Var(Bias(\hat{H}_{N, 1}))$ \\
\midrule[1pt]
100     & 4.600031     & 0.0051387799     & 0.02082714731  \\
200     & 4.610892     & 0.0057214665     & 0.01051097236  \\
500     & 4.606304     & 0.0011339740     & 0.00464501181  \\
1000    & 4.604414     & 0.0007562145     & 0.00197979118  \\
5000    & 4.606068     & 0.0008976195     & 0.00041910068  \\
10000   & 4.604871     & 0.0002993139     & 0.00021349464  \\
25000   & 4.605529     & 0.0003587332     & 0.00008599342  \\
50000   & 4.605547     & 0.0003764863     & 0.00004437685  \\
\hline
\end{tabular}
\end{center}
\end{table}

From table \ref{uniform_k=1_table} there is an obvious decrease in value of the Bias, for larger $N$, with $|Bias(\hat{H}_{N, 1})|$ decreasing from $\approx 0.005$ to $\approx 0.0004$. This decrease is similar to that considered in the normal distribution for $k=1$, where $|Bias(\hat{H}_{N, 1})|$ went from $\approx 0.01$ to $\approx 0.0003$; however, the difference is that for this distribution, the estimator seems to be more accurate for smaller $N$. Another thing to notice from this table is that, even through there is a general decrease, considering each rows in the table in comparison to the next does not necessarily show a decrease. The smallest value of bias occurs for $N=10,000$, which does not correspond with the findings for the normal distribution. This could indicate a number of features;

\begin{itemize}
\item The values in table \ref{uniform_k=1_table} contains outliers - this could be the case, since the numbers seem to jump around more ion this occasion than any others seen before. However, for $k=1$ the bias decreases from $0.0057 \to 0.00029$ in the uniform distribution and decreases from $0.013 \to 0.00034$ in the normal distribution (not necessarily as $N$ gets larger); these values of bias are not too dissimilar from one and other. Also, the tables are made from considering $500$ samples of each $N$, finding the estimator in all cases, then taking the average of these as the actual estimator; this makes an outlier much less likely, as they would have been smoothed out from the averaging process.

\item The actual value of entropy is significantly smaller itself, so the bias of the estimator is accordingly small - this is again unlikely, since the actual value of entropy for the uniform distribution is given by $\approx 4.605170$ (\ref{uniform_exact}), and the for the normal distribution is $\approx 1.418939$ (\ref{normal_exact}). These values are not significantly different to one and other, also under this reasoning, one would expect the normal distribution to have an accordingly smaller bias than the uniform - but we are experiencing values the other way around.

\item The estimator works better for samples from uniform than the normal distributions - this should not be true since the uniform distribution satisfies the conditions under which this estimator can be used in exactly the same manner as the normal distribution, so one would not expect samples from a specific distribution to yield a more accurate estimator for entropy. However, this is the most likely reason for the difference occurring between the two distributions. This is due to, as mentioned before, the nature of the uniform distribution, that by using $U[0, 100]$, for $N=50,000$ each sample would be $\approx 0.002$ distance apart. So using the nearest neighbour method; all of the data in the samples will have close neighbours, which could be the reason for the unreliable values shown in table \ref{uniform_k=1_table}.
\end{itemize}

To understand what is occurring in table \ref{uniform_k=1_table}, I have plotted the results of the approximate correlation between the bias of the estimator against $N$, as shown in equation \ref{logbias}. This is shown in Figure \ref{uniform_k=1_graph}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Uniform_k=1_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 1})|$ against $\log(N)$}
  \label{uniform_k=1_graph}
\end{figure}

 From this analysis, I have found the coefficients of the regression to be $a_{1} = 0.3698$ and $c_{1}=0.0103$. On their own these coefficients show that the bias of the Koazchenko-Leonenko estimator for entropy has the following relationship with $N$;
\begin{equation}
|Bias(\hat{H}_{N, 2})| \approx \frac{0.0103}{N^{0.3698}}\nonumber
\end{equation}
This is not what we would like to see, as for the estimator to be asymptotically unbiased, we would like to have $a_{k} > 0.5$, which here is not true. This could possibly be due to outliers in the samples, or could be due to the fact that for $k=1$, the estimator for the entropy of a sample from the uniform distribution, is not as strong as expected. If we compare this to the values from the normal distribution; $a_{1} = 0.4594$ and $c_{1} = 0.0249$,we can see that for the normal distribution, the value for $a_{k}$ is closer to that desired, but they have only an $\approx 0.09$ difference between them. To further understand the meaning of these coefficients we must compare them with those for higher values of $k$, which will be done in more detail in section \ref{U_compare_k}.





\subsubsection{k=2} \label{U_k=2}
We now wish to consider the estimator for $k=2$, which takes the form;
\begin{equation} 
\hat{H}_{N, 1} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(1),i} (N-1)}{e^{-\gamma + 1}} \right] \nonumber
\end{equation}

\begin{table}
\caption{1-dimensional uniform distribution, $k=2$} \label{uniform_k=2_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 2}$ & $|Bias(\hat{H}_{N, 2})|$ & $Var(Bias(\hat{H}_{N, 2}))$ \\
\midrule[1pt]
100     & 4.606725     & 0.0015545396     & 0.00851906912  \\
200     & 4.610785     & 0.0056145269     & 0.00456819154  \\
500     & 4.611599     & 0.0064290000     & 0.00189878458  \\
1000    & 4.603534     & 0.0016366244     & 0.00095980274  \\
5000    & 4.604978     & 0.0001921979     & 0.00017282038  \\
10000   & 4.605488     & 0.0003180407     & 0.00009978981  \\
25000   & 4.604753     & 0.0004176853     & 0.00004003515  \\
50000   & 4.605480     & 0.0003095010     & 0.00001737807  \\
\hline
\end{tabular}
\end{center}
\end{table}

Using the same parameters as before; taking $500$ samples of size $N=100 \to 50,000$ from the uniform, $U[0, 100]$, distribution, we get the results in Table \ref{uniform_k=2_table}. These show that there is a general decrease in bias for a larger $N$ from $|Bias(\hat{H}_{500, 2})| \approx  0.006430$ to $|Bias(\hat{H}_{50000, 2})| \approx 0.00031$; however, if we look closely, in a similar fashion to for $k=1$, the bias is not always decreasing as $N$ gets larger. This is shown in it increasing for the first 3 data points; $N=100, 200 ,500$, then decreasing for a bit, then increasing again for $N = 10000, 25000$. The smallest value of Bias actually occurs at $N=5000$, which does not correspond with the results one would expect from this analysis. Moreover, the size of the bias does begin smaller for this case than it has done previously for values from the normal distribution, section \ref{Normal_d=1}. 

To understand better what is occurring in table \ref{uniform_k=2_table}, I have plotted the results of the approximate correlation between the bias of the estimator against $N$, as shown in equation \ref{bias}. This is shown in Figure \ref{uniform_k=2_graph}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Uniform_k=2_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 2})|$ against $\log(N)$}
  \label{uniform_k=2_graph}
\end{figure}

This graph has the regression line plotted of the form \ref{logbias}, with $a_{2} \approx 0.5857$ and $c_{2} \approx 0.00503$. From this analysis, I would expect the bias of the Koazchenko-Leonenko estimator for entropy to have the following relationship with $N$;
\begin{equation}
|Bias(\hat{H}_{N, 2})| \approx \frac{0.0503}{N^{0.5857}}\nonumber
\end{equation}
As we can see from the graph, the relationship is obviously a negative correlation, and the values around the line are sparsely located. So I believe the reason for table \ref{uniform_k=2_table} not looking as expected, is just due to bad luck in the values of $N$ that I have chosen to be numerically represented in it. The graph plotted and the regression coefficients, align well with the normal distribution; whose coefficients $a_{2} \approx 0.5998$ and $c_{2} \approx 0.0746$ were found in section \ref{N_k=2}. Thus removing any uncertainty that we have about the estimator of entropy for a sample from the uniform distribution acting differently to that from the normal distribution. A comparison of the values of $a_{2}$ and $c_{2}$, with other values of $k$, will be further explored in section \ref{U_compare_k}.



\subsubsection{k=3} \label{U_k=3}
We now wish to consider the estimator for $k=3$, which takes the form;
\begin{equation} 
\hat{H}_{N, 1} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(1),i} (N-1)}{e^{-\gamma + \frac{3}{2}}} \right] \nonumber
\end{equation}

Using the same parameters as before; taking $500$ samples of size $N=100 \to 50,000$ from the uniform, $U[0, 100]$, distribution, we get the results in Table \ref{uniform_k=3_table}. These results are again inconclusive, to showing a the consistency condition that $|Bias(\hat{H}_{N, 3})| \to 0$ as $N \to \infty$, since the numbers jump around and increase between, say $N=500$ and $1000$, which we would not expect to happen.

\begin{table}
\caption{1-dimensional uniform distribution, $k=3$} \label{uniform_k=3_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 3}$ & $|Bias(\hat{H}_{N, 3})|$ & $Var(Bias(\hat{H}_{N, 3}))$ \\
\midrule[1pt]
100     & 4.610386     & 0.00521552744     & 0.00697968570  \\
200     & 4.611047     & 0.00587685467     & 0.00316173901  \\
500     & 4.605035     & 0.00013506468     & 0.00121563168  \\
1000    & 4.606418     & 0.00124808620     & 0.00054151215  \\
5000    & 4.605270     & 0.00009934301     & 0.00011448612  \\
10000   & 4.604869     & 0.00030102035     & 0.00007028042  \\
25000   & 4.605341     & 0.00017121288     & 0.00002543334  \\
50000   & 4.605123     & 0.00004761182     & 0.00001155187  \\
\hline
\end{tabular}
\end{center}
\end{table}

I believe that the best way to show this consistency condition is to just consider the graphical representation, and to not worry about the tabulated values, as they're inconsistent due to the reasons stated in section \ref{U_k=2}. From this plot, Figure \ref{uniform_k=3_graph}, I have found that $a_{3} \approx 0.6291$ and $c_{3} \approx 0.0737$. This implies the relationship;
\begin{equation}
|Bias(\hat{H}_{N, 3})| \approx \frac{0.0737}{N^{0.6291}}\nonumber
\end{equation}

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Uniform_k=3_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 3})|$ against $\log(N)$}
  \label{uniform_k=3_graph}
\end{figure}

In comparison to this, for $k=2$, we had that $a_{2} \approx 0.5857$ and $a_{1} \approx 0.3698$, so $a{1} < a_{2} < a_{3}$, which implies that the relationship for $k=3$ is stronger than that for $k=1, 2$. We can also compare the values found in section \ref{N_k=3}, where samples from the normal distribution were considered and an estimator for $k=3$ was found. The coefficients here were given by $a_{3} \approx 0.6443$ and $c_{3} \approx 0.1156$, which is a $\approx 0.015$ difference in $a_{3}$, and a $\approx 0.04$ difference in $c_{3}$, in comparison to the uniform distribution. This implies a very similar relationship is shown for each distribution; moreover, a more detailed comparison will be further explored in section \ref{U_compare_k}.





\subsubsection{k=5} \label{U_k=5}
Now we consider the estimator, shown in equation \ref{KLest}, for k=5. This takes the form;
\begin{equation}
\hat{H}_{N, 5} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(5),i}(N-1)}{e^{\Psi(5)}} \right] \nonumber
\end{equation}

Comparing this estimator to the exact value of entropy, shown in equation (\ref{uniform_exact}), for 500 samples of size $N$, as before, we get an inconclusive result from table \ref{uniform_k=5_table}. This is again due to the fact that taking a larger number of samples around a relatively small interval, will give similar results; hence, the relationship is again not as obvious from this. 

\begin{table}
\caption{1-dimensional uniform distribution, $k=5$} \label{uniform_k=5_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 5}$ & $|Bias(\hat{H}_{N, 5})|$ & $Var(Bias(\hat{H}_{N, 5}))$ \\
\midrule[1pt]
100     & 4.621001     & 0.0158306351     & 0.003899909770  \\
200     & 4.612364     & 0.0071935784     & 0.001744887954  \\
500     & 4.609793     & 0.0046227604     & 0.000768266967  \\
1000    & 4.607499     & 0.0023291487     & 0.000381576396  \\
5000    & 4.605980     & 0.0008102069     & 0.000068805987  \\
10000   & 4.606053     & 0.0008829085     & 0.000035434958  \\
25000   & 4.605339     & 0.0001689148     & 0.000015449454  \\
50000   & 4.605333     & 0.0001629615     & 0.000007555981  \\
\hline
\end{tabular}
\end{center}
\end{table}

Furthermore, I have considered a plot to represent equation (\ref{logbias}), shown in Figure \ref{uniform_k=5_graph}, where I have also found the coefficients of regression to be; $a_{5} \approx 0.7501$ and $c_{5} \approx 0.1889$. 

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Uniform_k=5_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 5})|$ against $\log(N)$}
  \label{uniform_k=5_graph}
\end{figure}

Thus, here we get the relationship;
\begin{equation}
|Bias(\hat{H}_{N, 5})| \approx \frac{0.1889}{N^{0.7501}}\nonumber
\end{equation}
In comparison to the coefficients from the previous value of $k$, we find that $a_{1} < a_{2} < a_{3} < a_{5}$, indicating that as $k$ increases the strength of the relationship between the $|Bias(\hat{H}_{N, k})|$ and $N$ also increases. This implies that $|Bias(\hat{H}_{N, k})| \to 0$ as $N \to \infty$ faster for $k=5$, over a smaller $k$. We can also see that $c_{k}$, so far, has also been increasing for larger $k \leq 5$.
We can also compare this to the information found form the analysis of the normal distribution, with $k=5$, explored in section \ref{N_k=5}. Here we found that $a_{5}=0.7568$ and $c_{5}=0.3557$, which shows an $\approx 0.06$ difference in the value of $a_{5}$ and a $\approx 0.17$ difference in the values of $c_{5}$. These show small differences between the two distributions, which will be further explored in section \ref{U_compare_k}.






\subsubsection{k=10} \label{U_k=10}
The last estimator for the entropy of a sample from the 1-dimensional uniform distribution that I wish to explore is that for $k=10$. Here, the estimator takes the form;
\begin{equation}
\hat{H}_{N, 10} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(10),i}(N-1)}{e^{\Psi(10)}} \right] \nonumber
\end{equation}
The results for the comparison between this estimator and \ref{uniform_exact} are displayed in table \ref{uniform_k=10_table}.

\begin{table}
\caption{1-dimensional uniform distribution, $k=10$} \label{uniform_k=10_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 10}$ & $|Bias(\hat{H}_{N, 10})|$ & $Var(Bias(\hat{H}_{N, 10}))$ \\
\midrule[1pt]
100     & 4.639476     & 0.03430601671     & 0.002521145015  \\
200     & 4.621455     & 0.01628474750     & 0.000951343553  \\
500     & 4.611200     & 0.00602956738     & 0.000380048902  \\
1000    & 4.609219     & 0.00404887162     & 0.000186210819  \\
5000    & 4.605507     & 0.00033726494     & 0.000037485008  \\
10000   & 4.605341     & 0.00017035096     & 0.000018679424  \\
25000   & 4.605138     & 0.00003191065     & 0.000007044257  \\
50000   & 4.605190     & 0.00002025184     & 0.000003429152  \\
\hline
\end{tabular}
\end{center}
\end{table}

In comparison to what we have seen before in tables \ref{uniform_k=2_table}, \ref{uniform_k=3_table} and \ref{uniform_k=5_table}, we here have more of a obvious comparison, shown numerically, between the size of the sample, $N$, and the size of the bias. This fits in with the condition of asymptotic unbiasedness from Theorem \ref{paper1_T1}, that $|Bias(\hat{H}_{N, 10})| \to 0$ as $N \to \infty$. 

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Uniform_k=10_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 10})|$ against $\log(N)$}
  \label{uniform_k=10_graph}
\end{figure}

From plotting the logarithm of the bias of the estimator against the logarithm of $N$ I have found the coefficients of regression to be $a_{10} = 1.0357$ and $c_{10}=3.8217$. Again these values are both larger than recorded previously for smaller values of $k$, for samples from the uniform distribution. Most importantly; $a_{10} > a_{5} > a_{3} > a_{2} > a_{1}$. These coefficients imply a relationship between the bias of the estimator and $N$ of the form;
\begin{equation}
|Bias(\hat{H}_{N, 10})| \approx \frac{3.8217}{N^{1.0357}}\nonumber
\end{equation}
This implies that the value of $|Bias(\hat{H}_{N, 10})| \to 0$ for large $N$, in a faster manner than that for smaller $k$, which again is fitting with Theorem \ref{paper1_T1} on the asymptotic unbiasedness of the estimator. I can also make the comparison between the regressional relationship between the uniform and the normal distribution for $k=10$, where for the normal distribution I found the coefficients to be $a_{10}=1.0055$ and $c_{10}=5.5942$, in section \ref{N_k=10}. We can see that these regression lines would be close to one and other, which is to be expected. For both distributions the value of $a_{10}$ is only $\approx 0.03$ in difference, but the value of $c_{10}$ has a larger difference of $\approx 1.8$, which is a much larger distance and gives the impression of the two lines being parallel to one and other; the uniform regression line is just shifted down slightly from the normal. A more detailed comparison of this will be shown in section \ref{U_compare_k}.





\subsubsection{Comparison of k} \label{U_compare_k}
In sections \ref{U_k=1} to \ref{U_k=10}, I have explored the Koazchenko-Leonenko estimator for samples from the 1-dimensional uniform distribution. In the most part, the tables of information from this estimator were inconsistent and inconclusive, due to the nature of the uniform distribution; that the samples are very close to one and other. Because of this, I will not be going into more detail of that comparison; thus, will be focusing solely on the relationship shown in equation (\ref{bias});
\begin{equation}
|Bias(\hat{H}_{N, k})| = \frac{c_{k}}{N^{a_{k}}} \nonumber
\end{equation}
The results from the investigation above have been condensed into table \ref{uniform_k_comparison_table}, showing the change in values of $a_{k}$ and $c_{k}$ for different {k}.

\begin{table}
\caption{1-dimensional uniform distribution, a comparison of $k$} \label{uniform_k_comparison_table}
\begin{center}
\begin{tabular}{| l | c c|} 
\toprule
k & $a_{k}$ & $c_{k}$ \\
\midrule[1pt]
1 & 0.3698 & 0.0103 \\
2 & 0.5857 & 0.0503 \\
3 & 0.6291 & 0.0737 \\
5 & 0.7501 & 0.1889 \\
10 & 1.0357 &  3.8217 \\
\hline
\end{tabular}
\end{center}
\end{table}

From table \ref{uniform_k_comparison_table}, we can see that as $k$ in creases from $1$ to $10$, that both $a_{k}$ and $c_{k}$ also increase. The value of $a_{k}$ increasing implies an increase in strength of the asymptotic unbiasedness of the estimator, Theorem \ref{paper1_T1};  $N (H - \mathbb{E}{\hat{H}_{N, k}})^2 \to 0 \quad  (N \to \infty)$, which is equivalent to saying that $|Bias(\hat{H}_{N, k})| \to 0 \quad (N \to \infty)$. Thus, considering a large sample size, say $N=100,000$, we can find the bias of the Kozachenko-Leonenko estimator according to the regressional relationships found in sections \ref{U_k=1} to \ref{U_k=10} for each $k$;
\begin{gather*}
|Bias(\hat{H}_{100000, 1})| \approx  \frac{0.0103}{100000^{0.3698}}   \approx 0.000145826 \\
|Bias(\hat{H}_{100000, 2})| \approx  \frac{0.0503}{100000^{0.5857}}   \approx 0.000059301 \\
|Bias(\hat{H}_{100000, 3})| \approx  \frac{0.0737}{100000^{0.6291}}   \approx 0.000052719 \\
|Bias(\hat{H}_{100000, 5})| \approx  \frac{0.1889}{100000^{0.7501}}   \approx 0.000033553\\
|Bias(\hat{H}_{100000, 10})| \approx  \frac{3.8217}{100000^{1.0357}}  \approx 0.000025337
\end{gather*}
These values confirm our original thoughts that the larger value of $k \leq 10$ gives a more consistent estimator. Moreover, we can compare these values to those found for the standard normal distribution, along with comparing the values of $a_{k}$ and $c_{k}$, to see if the estimator has a similar accuracy for both distributions, this comparison is shown in table \ref{uniform_normal_comparison_table}.

\begin{table}
\caption{Comparison between 1-dimensional Uniform and Normal distribution} \label{uniform_normal_comparison_table}
\begin{center}
\begin{tabular}{| l | c c c | c c c |}
\toprule
{ |} &  \multicolumn{3}{c |}{Normal} & \multicolumn{3}{c |}{Uniform}\\
\hline
$k$   &  $a_{k}$  &  $c_{k}$  &  $|Bias(\hat{H}_{100000, k})|$  &  $a_{k}$  &  $c_{k}$  &  $|Bias(\hat{H}_{100000, k})|$  \\
\midrule[1pt]
1      & 0.4594     & 0.0249 &  0.00012566  &  0.3698  &  0.0103  &  0.000145826 \\
2      & 0.5998     & 0.0746 &  0.00007477  &  0.5857  &  0.0503  &  0.000059301 \\
3      & 0.6443     & 0.1156 &  0.00006942  &  0.6291  &  0.0737  &  0.000052719 \\
5      & 0.7568     & 0.3557 &  0.00005949  &  0.7501  &  0.1889  &  0.000033553 \\
10    & 1.0055     & 5.5942 &  0.00005251  &  1.0357  &  3.8217  &  0.000025337 \\
\hline
\end{tabular}
\end{center}
\end{table}

In this comparison we can see that, the values of $a_{k}$ and $|Bias(\hat{H}_{100000, k})|$ are similar for both distributions, with $a_{k}$ varying by less that $\approx 0.015$ for $k=2,3,5,10$ and $|Bias(\hat{H}_{100000, k})|$ varying by less than $\approx 0.00002$ for all $k$. Both of these confirm that the approximation to the relationship between the estimator and the actual value of entropy, is a good approximation to make; one which is consistent through both distributions considered so far.

Another comparison that we can make, is by considering the plot of the regression lines for the logarithm of  $|Bias(\hat{H}_{N, k})|$ against the logarithm of $N$, for all values of $k$. This relationship is shown in Figure \ref{uniform_comparison_graph}. This graph shows obviously that the larger the $k$ that we have considered the stronger the relationship between the bias and $N$, since the regression line for $k=10$ is the steepest out of them all.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Uniform_comparison.png}
  \end{center}
\caption{Plot of regression lines for $\log|Bias(\hat{H}_{N, k})|$ against $\log(N)$, for $k=1, 2, 3, 5, 10$, for samples from the uniform distribution}
  \label{uniform_comparison_graph}
\end{figure}





\subsection{1-dimensional Exponential Distribution} \label{Expo_d=1}

I will now be looking at the entropy of samples from the exponential distribution $exp(\lambda)$, where $\lambda > 0$ is the rate or inverse scale parameter. In a similar fashion to the previous distributions, the exponential also has an exact formula for the entropy, given the rate parameter $\lambda$. Using equation (\ref{ShaEnt}) and the density function for the exponential distribution $f(x) = \lambda e^{-\lambda x}$ for $x \in [0, \infty)$ and for $\lambda >0$, we can write the exact entropy;
\begin{align*}
H &= - \int_{x : f(x) > 0} f(x) log(f(x)) dx \\ 
&= - \int_{0}^{\infty} \lambda e^{-\lambda x} log [ \lambda e^{-\lambda x} ] dx  \\
&= - \lambda \int_{0}^{\infty} \lambda e^{-\lambda x} [log(\lambda) - \lambda x] dx  \\
&= \lambda \int_{0}^{\infty} \lambda e^{-\lambda x} - log(\lambda) e^{-\lambda x} dx \\
&= - \lambda \left[x e^{-\lambda x}\right]_{0}^{\infty} + \int_{0}^{\infty}\lambda e^{-\lambda x} dx + log(\lambda) \left[ e^{-\lambda x}\right]_{0}^{\infty} \\
&= 0 + (log(\lambda) - 1) \left[e^{-\lambda x} \right]_{0}^{\infty} \\
&= -(log(\lambda) - 1)
\end{align*}

Thus we have the the exact value of entropy for the exponential distribution, given the rate parameter $\lambda > 0$, is;
\begin{equation} \label{ExpEnt}
H = 1 - log(\lambda)
\end{equation}

Moreover, I am again considering a 1-dimensional distribution; thus $V_{d} = V_{1} = 2$, and;
\begin{equation}
\hat{H}_{N, k} =  \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(k),i}(N-1)}{e^{\Psi(k)}} \right]\nonumber
\end{equation}
is the form of the Kozachenko-Leonenko estimator that I will be considering here, equation (\ref{KLest_d=1}).

I have decided to choose to explore the exponential distribution with rate parameter $\lambda = 0.5$, this is because, we know that for the exponential distribution we must have the rate parameter $\lambda >0$ and if I choose $\lambda > e \approx 2.7183$ we get a negative values of entropy, $H < 0$. This will introduce problems when considering the modulus of the bias; hence, for this analysis it will be more beneficial to consider a positive value of entropy, $H$. Also, for $\lambda \geq 1$, we have a very small value of entropy, $H \leq 1$, which would cause problems when calculating large samples and their entropy. Therefore, I have chosen a random number for the rate parameter such that $\lambda \in (0, 1)$, so for this value of $\lambda=0.5$, the exact entropy is given by;
\begin{equation} \label{exponential_exact}
H = 1 - log(0.5) \approx 1.693147
\end{equation}



\subsubsection{Estimator Conditions} \label{E_Conditions}

Samples from the exponential distribution must satisfy the conditions of Theorem \ref{paper1_T1} and \ref{paper1_T2}, to  be an asymptotically unbiased and consistent estimator. For Theorem \ref{paper1_T1} to be satisfied, it must first satisfy equation (\ref{paper1_T1_eq1}), thus using that the density here is $f(x) = \lambda e^{-\lambda x}$ for $x \in [0, \infty)$, where $\lambda >0$, and considering some $\epsilon >0$;
\begin{align} \nonumber
\int_{\mathbb{R}} | log(f(x))|^{1 + \epsilon} f(x) dx  &= \lambda \int_{0}^{\infty} | log (\lambda) - \lambda x |^{1 + \epsilon} e^{- \lambda x} dx \\ \nonumber
&<  \int_{0}^{\infty} \frac{| - \lambda x |^{1 + \epsilon}}{ e^{\lambda x}} dx \\ \nonumber
&<  \int_{0}^{\infty} \frac{|x|}{ e^{\lambda x}} dx < \infty \nonumber
\end{align}
Also, the second condition, equation (\ref{paper1_T1_eq2}), of Theorem \ref{paper1_T1} is satisfied;
\begin{align} \nonumber
\int_{(\mathbb{R})^2} | log(\|x-y\|)|^{1+ \epsilon} f(x) f(y) dx dy  &= \lambda^2 \int_{0}^{\infty} \int_{0}^{\infty} | log(\|x-y\|)|^{1+ \epsilon} e^{- \lambda (x + y)} dx dy \\ \nonumber
&<  \int_{0}^{\infty} \int_{0}^{\infty}  \frac{| log(\|x\| + \|y\|)|^{1+ \epsilon}}{e^{\lambda (x + y)}} dx dy \\ \nonumber
&<  \int_{0}^{\infty} \int_{0}^{\infty}  \frac{| log(\|x\| + \|y\|)|}{e^{(x + y)}} dx dy < \infty  \nonumber
\end{align}
Thus we can say that for the exponential distribution, $\hat{H}_{N,k}$ is an asymptotically unbiased estimator for entropy. Moreover, the exponential distribution also satisfies Theorem \ref{paper1_T2}, as it fulfills the first condition shown in equation (\ref{paper1_T2_eq1});
\begin{align} \nonumber
\int_{\mathbb{R}} | log(f(x))|^{2 + \epsilon} f(x) dx  &= \lambda \int_{0}^{\infty} | log (\lambda) - \lambda x |^{2 + \epsilon} e^{- \lambda x} dx \\ \nonumber
&<  \int_{0}^{\infty} \frac{| - \lambda x |^{2 + \epsilon}}{ e^{\lambda x}} dx \\ \nonumber
&<  \int_{0}^{\infty} \frac{|x|^2}{ e^{\lambda x}} dx < \infty \nonumber
\end{align}
and the second condition, equation (\ref{paper1_T2_eq2});
\begin{align} \nonumber
\int_{(\mathbb{R})^2} | log(\|x-y\|)|^{2+ \epsilon} f(x) f(y) dx dy  &= \lambda^2 \int_{0}^{\infty} \int_{0}^{\infty} | log(\|x-y\|)|^{2+ \epsilon} e^{- \lambda (x + y)} dx dy \\ \nonumber
&<  \int_{0}^{\infty} \int_{0}^{\infty}  \frac{| log(\|x\| + \|y\|)|^{2+ \epsilon}}{e^{\lambda (x + y)}} dx dy \\ \nonumber
&<  \int_{0}^{\infty} \int_{0}^{\infty}  \frac{| log(\|x\| + \|y\|)|^2}{e^{(x + y)}} dx dy < \infty  \nonumber
\end{align}
Thus, the estimator $\hat{H}_{N,k}$, for a sample from the exponential distribution is a consistent estimator by Theorem \ref{paper1_T2}. 

For Theorems \ref{paper4_T4} and \ref{paper4_T5} to be satisfied by the estimators generated by samples from the exponential distribution, this distribution must meet the Conditions \ref{A1}, \ref{A2} and \ref{A3}. 
Firstly, to satisfy Condition \ref{A1}, the density function $f(x) =  \frac{e^{\frac{-x}{2}}}{2}$ for $x \in [0, \infty)$, where we have chosen $\lambda = 0.5$, must be such that;
\begin{itemize}
\item $f$ is bounded - this is true, since for any probability distribution we have $f(x) \geq 0$, also for the exponential distribution we always have for $x \in [0, \infty)$ that $f(x) \leq 1$, so $f$ is a bounded function.

\item $f$ is m-times differentiable - using Laguerre Polynomials;
\begin{equation}
L_{m}(x) = \frac{1}{m!} e^x \frac{d^m}{dx^m}(e^{-x} x^m) \nonumber
\end{equation}
we can see that by taking $x := \frac{x}{2}$, and multiplying the equation through by $\lambda = \frac{1}{2}$ we can formulate an equation containing the m-th derivative of the exponential distribution;
\begin{align*}
\frac{d^m}{dx^m}\left(\frac{e^{\frac{-x}{2}}}{2} \left(\frac{x}{2}\right)^m\right) &= L_{m}\left(\frac{x}{2}\right) m! \frac{e^{\frac{-x}{2}}}{2} \\
\frac{d^m}{dx^m}(f(x) x^m) &=  2^m L_{m}\left(\frac{x}{2}\right) m! f(x)
\end{align*}
TODO .. use Leibniz rule? rearrange and write something like $\frac{d^m}{dx^m}(f(x)) = G(x) f(x)$..

\item $\exists r_{*} > 0$ and a Borel measurable function $g_{*}$, with $\|y-x\| \leq r_{*}$ so that $\|f^{(t)}(x)\| \leq g_{*}(x) f(x)$ and $\|f^{(m)}(x) - f^{(m)}(x)\| \leq g_{*}(x) f(x)\|y - x\|^{\eta}$, for some $g_{*}$ such that $\sup_{\{x : f(x) < \delta\}} g_{*}(x) = O(\delta^{-\epsilon})$ as $\delta \searrow 0$ for some $\epsilon >0$

TODO.. can be done once previous point has been written out correctly
\end{itemize}

Next, to satisfy Condition \ref{A2}, for the density function $f$ of the exponential distribution, must fulfill that;
\begin{itemize}
\item The $\alpha$-moment of $f$ must be finite, so $\int_{\mathbb{R}^{d}} \| x \|^{\alpha} f(x) dx < \infty$ - this is true since the moments of the exponential distribution are given by;
\begin{align*}
\int_{\mathbb{R}^{d}} \| x \|^{\alpha} f(x) dx &= \int_{0}^{\infty} | x |^{\alpha} \lambda e^{-\lambda x} dx\\
&= \frac{\alpha !}{\lambda ^{\alpha}} < \infty
\end{align*}
for all $\alpha \in \mathbb{N}$, which is obviously finite.
\end{itemize}

Lastly, to satisfy Condition \ref{A3}, we must find the values of $k$ for which the estimator provides a uniform convergence for Theorems \ref{paper4_T4} and \ref{paper4_T5}. As previously, these values are independent of the distribution that the sample is from, and only depends on the size of the sample, the dimension of the distribution that sample is taken from and the value chosen for $\alpha$, where we have chosen $\alpha = 2$. Thus, the values of $k$ found in section \ref{N_Conditions} are $\{1, 2, ..., 11\}$.

Due to the above conditions for Theorems \ref{paper1_T1}, \ref{paper1_T2}, \ref{paper4_T4} and \ref{paper4_T5} being met, we can say that the Kozachenko-Leonenko estimator, of a sample from the exponential distribution is an asymptotically unbiased and consistent estimator for entropy. 




\subsubsection{k=1} \label{E_k=1}
In a similar fashion to how the previous distributions were explored in sections \ref{N_k=1} to \ref{N_k=10} and \ref{U_k=1} and \ref{U_k=10}, I will be considering $500$ samples of size $N$ from the exponential distribution, find the estimator in each case and take the average of these estimators to find our entropy estimator, using the equation;
\begin{equation}
\hat{H}_{N, 1} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(1),i} (N-1)}{e^{-\gamma}} \right] \nonumber
\end{equation}
Considering the bias for this estimator against the actual value of entropy for the exponential distribution, with $\lambda =0.5$, shown in equation (\ref{expo_exact}), for different samples sizes $N$ gives Table \ref{expo_k=1_table}.

\begin{table}
\caption{1-dimensional exponential distribution, $k=1$} \label{expo_k=1_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 1}$ & $|Bias(\hat{H}_{N, 1})|$ & $Var(Bias(\hat{H}_{N, 1}))$ \\
\midrule[1pt]
100     & 1.682163     & 0.01098459356     & 0.03000921515  \\
200     & 1.692696     & 0.00045124177     & 0.01814679310  \\
500     & 1.690508     & 0.00263961497     & 0.00634492739  \\
1000    & 1.687560     & 0.00558717255     & 0.00319726793  \\
5000    & 1.695081     & 0.00193336147     & 0.00058924735  \\
10000   & 1.693198     & 0.00005051942     & 0.00032789400  \\
25000   & 1.694373     & 0.00122575211     & 0.00012790693  \\
50000   & 1.693321     & 0.00017399576     & 0.00005779114  \\
\hline
\end{tabular}
\end{center}
\end{table}

These results show an overall decreasing trend in the bias, and a strong decreasing variance of the bias. However, this is not a decreasing sequence of numbers, as in many places (i.e. from $N=10,000$ to $N=25,000$) the bias is increasing, which disagrees with Theorem \ref{paper1_T1}, that the estimator is asymptotically unbiased; since the smallest value of bias occurs at $N=10,000$. There could be a number of reasons for the behaviour shown;
\begin{itemize}
\item The estimator for the exponential distribution is not asymptotically unbiased - we know that this is not true since in section \ref{Expo_d=1}, I have shown how samples from this distribution satisfies the conditions to imply that the Theorems \ref{paper1_T1} and \ref{paper1_T2} hold. Thus the estimator for samples from the exponential distribution must be asymptotically unbiased and consistent, hence we should see the trend that $|Bias(\hat{H}_{N, 1})| \to 0$ as $N \to \infty$.
\item The relationship isn't smooth but does overall show asymptotic unbias -  this could be true, since we are not looking at every data point from $N = 1 \to 50,000$, we have only considered 8 values of $N$. Between each $N$ we would expect variability and not an exact smoothness; this is because we are considering simulations so are not expecting to have every point of bias against $N$ in exactly the correct place. Thus considering each point could confirm Theorem \ref{paper1_T1}; this will be done in the graph of the $log |Bias(\hat{H}_{N, 1})|$ against $log(N)$.
\item The table could just contain outliers - the amount that the values of bias jumps around would indicate that the majority of them being outliers is not feasible. Additionally, the estimator for each value $N$ is computed $500$ times, and then the average is recorded in table \ref{expo_k=1_table}, so this should smooth out any outliers that could occur from the computation.
\end{itemize}

To further examine the relationship between bias and $N$, which was inconclusive in table \ref{expo_k=1_table}, I have depicted Figure \ref{expo_k=1_graph}, shows the relationship of $log|Bias(\hat{H}_{N, 1})|$ against $log(N)$ with a fitted regression line. In this graph, I have considered the values of $N$ up to $50,000$ at intervals of size $100$, where each point is calculate $500$ times and the average estimator is plotted. I have also found the corresponding coefficients $a_{1} = 0.4174$ and $c_{1} = 0.0198$ for the relationship shown in (\ref{bias});
\begin{equation}
|Bias(\hat{H}_{N, 1})| \approx \frac{0.0198}{N^{0.4174}}\nonumber
\end{equation}

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Expo_k=1_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 1})|$ against $\log(N)$}
  \label{expo_k=1_graph}
\end{figure}

To say that this estimator is asymptotically unbiased, we would like to have the value of $a_{k} > 0.5$, which is not the case here. This is similar to the relationship shown in sections \ref{N_k=1} and \ref{U_k=1}, where we also had $a_{1} = 0.4594, 0.3698 < 0.5$. Although this is not what we would like in our estimator, it is consistent throughout the different distribution considered so far. Also, it still implies a negative relationship between the bias and $N$, as $N$ increases the bias will decrease, just not a relationship as strong one would like. These three regression lines can be plotted together to further show their similarities, depicted in figure \ref{E_U_N_k=1_graph}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/EUN_k=1_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 1})|$ against $\log(N)$, for the 1-dimensional distributions; exponential, uniform and normal}
  \label{E_U_N_k=1_graph}
\end{figure}

This graph shows that for each distribution, with $k=1$, has a similar trend between the logarithm of the bias and the logarithm of $N$. It appears that the normal has the strongest negative correlation, but only by a small amount; considering the graph for larger values of $log(N)$ show the regression lines being particularly close to one and other for large $N$. A more detailed comparison will be further explored in section \ref{E_compare_k}.





\subsubsection{k=2} \label{E_k=2}
We now wish to consider the estimator for $k=2$, which takes the form;
\begin{equation} 
\hat{H}_{N, 1} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(1),i} (N-1)}{e^{-\gamma + 1}} \right] \nonumber
\end{equation}

\begin{table}
\caption{1-dimensional exponential distribution, $k=2$} \label{expo_k=2_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 2}$ & $|Bias(\hat{H}_{N, 2})|$ & $Var(Bias(\hat{H}_{N, 2}))$ \\
\midrule[1pt]
100     & 1.692627     & 0.00051996867     & 0.01705110058  \\
200     & 1.686513     & 0.00663429438     & 0.00942333921  \\
500     & 1.691328     & 0.00181927748     & 0.00371778428  \\
1000    & 1.692399     & 0.00074835082     & 0.00203062072  \\
5000    & 1.692660     & 0.00048693059     & 0.00039750041  \\
10000   & 1.693193     & 0.00004543429     & 0.00019553001  \\
25000   & 1.693504     & 0.00035725960     & 0.00007458969  \\
50000   & 1.693084     & 0.00006338711     & 0.00004293387  \\
\hline
\end{tabular}
\end{center}
\end{table}

Using the same parameters as before; taking $500$ samples of size $N=100 \to 50,000$ from the exponential distribution, with rate parameter $\lambda =0.5$, and consider the estimator of these samples against the actual value of entropy, equation \ref{exponential_exact}, we get the results in table \ref{expo_k=2_table}. These results are inconclusive in showing the relationship desired between the $|Bias(\hat{H}_{N, 2})|$ and $N$, from Theorem \ref{paper1_T1}, this is similar to what was found for $k=1$ in section \ref{E_k=1}, and a more convincing conclusion was made by examining the relationship in equation (\ref{logbias}). Thus, I have also plotted the regression graph again, this time for $k=2$, the results are shown in Figure \ref{expo_k=1_graph} and the regression coefficients are given by $a_{2} = 0.6480$ and $c_{2} = 0.1482$.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Expo_k=2_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 2})|$ against $\log(N)$}
  \label{expo_k=2_graph}
\end{figure}

Using equation (\ref{bias}), we have the relationship between the bias and $N$ given by;
\begin{equation}
|Bias(\hat{H}_{N, 1})| \approx \frac{0.1482}{N^{0.6480}}\nonumber
\end{equation}
where here we have the $a_{2} > a_{1}$, which is the same relationship that we have found in the previous two distributions. We can also compare th value of $a_{2}$ in these other distributions and we have; $a_{2} = 0.5857$ for the normal distribution and $a_{3}= 0.5998$ for the uniform distribution (from sections \ref{N_k=2} and \ref{U_k=2} respectively). All of these values vary by only $\approx 0.06$, showing that the information found from the analysis done is consistent throughout the distributions considered so far. I have also plotted these regression lines alongside each other, in Figure \ref{E_U_N_k=2_graph}, to depict the differences/similarities, between the regression line for each distribution.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/EUN_k=2_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 2})|$ against $\log(N)$, for the 1-dimensional distributions; exponential, uniform and normal}
  \label{E_U_N_k=2_graph}
\end{figure}

This graph confirms the closeness of the regression lines for each distribution, for k=2, we can see that the uniform distribution has the lowest regression line; indicating that at the largest $N$ considered, we would have the smallest bias for this distribution. However, the exponential distribution has the steepest slope to its line; demonstrating that for a large $N$ (larger than what has been considered so far), we would have that the bias of samples from this distribution tends to 0 the fastest. Moreover, the regression lines are very similar between these distributions for k=2, suggesting that the results here are as to be expected.




\subsubsection{k=3} \label{E_k=3}
Again, for $k=3$, I will examine $500$ samples of size $N$ from the exponential distribution considered before, with estimator of the form;
\begin{equation}
\hat{H}_{N, 3} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(3),i} (N-1)}{e^{\Psi(3)}} \right] = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(3),i} (N-1)}{e^{-\gamma + 1 + \frac{1}{2}}} \right] \nonumber
\end{equation}

\begin{table}
\caption{1-dimensional exponential distribution, $k=3$} \label{expo_k=3_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
$N$ & $\hat{H}_{N, 3}$ & $|Bias(\hat{H}_{N, 3})|$ & $Var(Bias(\hat{H}_{N, 3}))$ \\
\midrule[1pt]
100     & 1.687045     & 0.0061025449     & 0.01703802128  \\
200     & 1.689520     & 0.0036271927     & 0.00726556895  \\
500     & 1.690834     & 0.0023135057     & 0.00293535162  \\
1000    & 1.696420     & 0.0032728456     & 0.00150595517  \\
5000    & 1.693846     & 0.0006989676     & 0.00030230568  \\
10000   & 1.692206     & 0.0009408184     & 0.00018257444  \\
25000   & 1.692290     & 0.0008569323     & 0.00006568172  \\
50000   & 1.693392     & 0.0002444838     & 0.00003130878  \\
\hline
\end{tabular}
\end{center}
\end{table}

The results of the comparison between the actual value and the estimated value of entropy, for different values of $N$, are displayed in table \ref{expo_k=3_table}. This table is beginning to indicate a strong trend between the bias of the estimator and the sample size, $N$. The previous tables from section \ref{E_k=1} and \ref{E_k=2}, had inconclusive values in their tables; however this table does seem to suggest that the estimator agrees with Theorem \ref{paper1_T1}, that the estimator is asymptotically unbiased. To further examine the relationship between the values of bias and $N$, I have created the graph depicted in Figure \ref{expo_k=3_graph}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Expo_k=3_plot2.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 3})|$ against $\log(N)$}
  \label{expo_k=3_graph}
\end{figure}

From this analysis, I have found that for this distribution we have the coefficients of regression to be $a_{3} = 0.5711$ and $c_{3}=0.0585$, which implies a relationship of the form;
\begin{equation}
|Bias(\hat{H}_{N, 1})| \approx \frac{0.0585}{N^{0.5711}}\nonumber
\end{equation}
In comparison to the regression coefficients found for smaller values of $k$, for samples also from the exponential distribution, I have found that this relationship is unlike expected. This is because, for this distribution, we have; $a_{1} < a_{3} < a_{2}$, whereas for the 1-dimensional normal and uniform distribution I have consistently found that  $a_{1} < a_{2} < a_{3}$. There is also similar behaviour for the values of $c_{k}$, where here we have  $c_{1} < c_{3} < c_{2}$ in contrast to  $c_{1} < c_{2} < c_{3}$. This could mean a number of things;
\begin{itemize}
\item When $k=2$, we have a stronger asymptotic unbias than when $k=1, 3$ - this could be true, since we do not know what value of $k$ will produce the best estimator, also in the previous distributions, we only had a difference of $\approx 0.045$ for normal and $\approx 0.043$ for uniform, between the values of $a_{2}$ and $a_{3}$. However, since the previous analysis done in sections \ref{Uniform_d=1} and \ref{Normal_d=1} implies otherwise, I would not encourage this solution. To confirm this either way, it would be beneficial to take into account the regression relationship for higher values of $k$ for samples fro the exponential distribution.

\item There are outliers in the solution - this is a possibility, since the regression relationship for $k=2$ and $k=3$, have similar values of the gradient $a_{2} = 0.6480$ and $a_{3}=0.5711$, they differ by $\approx 0.077$. Thus, if the value of $a_{2}$ is slightly higher than it should be and if $a_{3}$ is slightly lower than it should be, both due to outliers; then we could actually have the relationship already assumed, that $a_{2} < a_{3}$. On the other hand, one would not expect this to happen, since any outliers should have been smoothed out in the process of considering a large number of samples (300) for a lot of different sample sizes (100, 200, 300, ... , 50000).

\end{itemize}

We can also compare the found values for $a_{3}$ and $c_{3}$ between distributions; normal - $a_{3}=0.6443$ and $c_{3}=0.1156$, uniform - $a_{3}=0.6291$ and $c_{3}=0.0737$ and exponential - $a_{3}=0.5711$ and $c_{3}=0.0585$. Thus the values of the slope and intercept differ by only $\approx 0.073$ and $\approx 0.057$ respectively, which are not especially large values of difference. I have plotted the regression lines for each distribution together in figure \ref{E_U_N_k=3_graph} to hopefully more easily view the differences.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/EUN_k=3_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 3})|$ against $\log(N)$, for the 1-dimensional distributions; exponential, uniform and normal}
  \label{E_U_N_k=3_graph}
\end{figure}

From this plot, we can see that the regression lines for each distribution are actually very similar to one and other, where at the largest value of $N$ considered here we have that the regression lines implies that the uniform distribution has the smallest bias at this point. However, the normal distribution has the steepest slope, thus for a larger $N$ than what is considered in the graph, one would have that the estimator for samples from the normal distribution has the strongest asymptotic unbias. A more detailed comparison of all values of $k$ and all distributions will be explored in section \ref{E_compare_k}.



\subsubsection{k=5} \label{E_k=5}
Now we consider the estimator, shown in equation \ref{KLest}, for k=5. This takes the form;
\begin{equation}
\hat{H}_{N, 5} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(5),i}(N-1)}{e^{\Psi(5)}} \right] \nonumber
\end{equation}

Comparing this estimator to the exact value of entropy, shown in equation (\ref{exponential_exact}), for 500 samples of size $N$, we get the results in table \ref{expo_k=5_table}. 

\begin{table}
\caption{1-dimensional exponential distribution, $k=5$} \label{expo_k=5_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 5}$ & $|Bias(\hat{H}_{N, 5})|$ & $Var(Bias(\hat{H}_{N, 5}))$ \\
\midrule[1pt]
100     & 1.671551     & 0.02159648310     & 0.01379216507  \\
200     & 1.679168     & 0.01397964263     & 0.00670466906  \\
500     & 1.692568     & 0.00057877793     & 0.00231523099  \\
1000    & 1.694130     & 0.00098238978     & 0.00134446983  \\
5000    & 1.692592     & 0.00055538663     & 0.00029049727  \\
10000   & 1.693190     & 0.00004331284     & 0.00014609571  \\
25000   & 1.692837     & 0.00031015885     & 0.00005897936  \\
50000   & 1.692901     & 0.00024589568     & 0.00002504615  \\
\hline
\end{tabular}
\end{center}
\end{table}

From this table we can see an overall decrease in bias, from $\approx 0.02 \to 0.0002$ as $N$ goes from $100 \to 50,000$. This is fitting with Theorem \ref{paper1_T1}, that the estimator is asymptotically unbiased for this distribution. However, on closer inspection we can see, as in the previous values of $k$ for this distribution, that the size of the bias does not decrease consistently as the sample size, $N$, increases in the table. For example from $N=10,000$ to $N=25,000$, the size of the bias actually increases by $\approx 0.00027$, which one would not expect. However, we should allow for some variability in the results since they are found from simulations. 
Furthermore, I have considered a plot to represent equation (\ref{logbias}), shown in Figure \ref{expo_k=5_graph}, where I have also found the coefficients of regression to be; $a_{5} = 0.4793$ and $c_{5} = 0.0241$. 

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Expo_k=5_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 5})|$ against $\log(N)$}
  \label{expo_k=5_graph}
\end{figure}

This implies the relationship;
\begin{equation}
|Bias(\hat{H}_{N, 5})| \approx \frac{0.0241}{N^{0.4793}}\nonumber
\end{equation}
In comparison to the coefficients from the previous value of $k$, we find that these coefficients also do not fit with the trend that as $k \to 10$ that the values of $a_{k}$ and $c_{k}$ also increase. For the exponential distribution we have found, so far, that $a_{1} < a_{5} < a_{3} < a_{2}$; this is atypical to the normal and uniform distributions where the results are consistently $a_{1} < a_{2} < a_{3} < a_{5} < a_{10}$. To compare the values of $a_{5}$ for these distributions, I have plotted their regression lines together, depicted in Figure \ref{E_U_N_k=5_graph}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/EUN_k=5_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N,5})|$ against $\log(N)$, for the 1-dimensional distributions; exponential, uniform and normal}
  \label{E_U_N_k=5_graph}
\end{figure}

This graph shows that, the normal and uniform have almost parallel to one and other, with uniform lowest at the largest $N$ considered. On the other hand, we have the regression line for the bias of samples from the exponential distribution, which is a more gradual decline that those for the other two distributions. Indicating that the asymptotic unbais of this distribution is not as strong as that for the other two. The reasoning behind this is currently unclear, but a detailed analysis into this will take place in section \ref{E_compare_k}.





\subsubsection{k=10} \label{E_k=10}

The last estimator for the entropy of a sample from the standard normal distribution that I wish to explore is that for $k=10$. Here, the estimator takes the form;
\begin{equation}
\hat{H}_{N, 10} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{2\rho_{(10),i}(N-1)}{e^{\Psi(10)}} \right] \nonumber
\end{equation}
The results for the comparison between this estimator and the exact value of entropy, equation (\ref{exponential_exact}), are displayed in table \ref{expo_k=10_table}.

\begin{table}
\caption{1-dimensional exponential distribution, $k=10$} \label{expo_k=10_table}
\begin{center}
\begin{tabular}{| l | c c c|} 
\toprule
N & $\hat{H}_{N, 10}$ & $|Bias(\hat{H}_{N, 10})|$ & $Var(Bias(\hat{H}_{N, 10}))$ \\
\midrule[1pt]
100     & 1.681136     & 0.01201069876     & 0.01103729658  \\
200     & 1.679907     & 0.01323996238     & 0.00586663681  \\
500     & 1.687144     & 0.00600360695     & 0.00209594189  \\
1000    & 1.692988     & 0.00015893618     & 0.00118121417  \\
5000    & 1.693597     & 0.00044935080     & 0.00025313033  \\
10000   & 1.692565     & 0.00058186611     & 0.00012048276  \\
25000   & 1.693052     & 0.00009528961     & 0.00004392069  \\
50000   & 1.693243     & 0.00009553753     & 0.00002392945  \\
\hline
\end{tabular}
\end{center}
\end{table}

This values in this table, now begin to show more what we are used to; that as $N$ increases the bias of the estimator decreases, which is fitting with Theorem \ref{paper1_T1}. Additionally, the variance of the bias is decreasing for a larger $N$, again something to be expected by Theorem \ref{paper4_T4}. To view this relationship in a more desirable way, we can consider the equation (\ref{logbias}), as previously done, and generate 500 samples, for each size $N = 100, 200, ... ,50000$, and find out the estimator $\hat{H}_{N,k}$ each time, to then work out the average and plot the logarithm of the modulus of the Bias for each $N$, $log|Bias(\hat{H}_{N,k})|$, against the logarithm of $N$, $log(N)$. This is shown in figure \ref{expo_k=5_graph}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/Expo_k=5_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 5})|$ against $\log(N)$}
  \label{expo_k=5_graph}
\end{figure}

From plotting these points and fitting a regression line, we get the relationship, shown in equation (\ref{bias}), here with the coefficients $a_{10}=0.7170$ and $c_{10}=0.2292$, thus;
\begin{equation}
|Bias(\hat{H}_{N, 5})| \approx \frac{0.2292}{N^{0.7170}}\nonumber
\end{equation}

We wish to have $a_{k}>0.5$, which is true for this particular realisation. In comparison to the other coefficients for different $k$ in the exponential distribution,we have that; $a_{1} < a_{5} < a_{3} < a_{2} < a_{10}$. This does show that now there is a general trend, since $a_{1} < a_{5} < a_{10}$, but this is still not the same as what was shown for the other distributions. Considering $k=5$ and the values of $a_{5}$ and $c_{5}$, for the different distributions, I have plot the regression lines together in figure \ref{E_U_N_k=1_graph}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{./Graphs/EUN_k=10_plot.png}
  \end{center}
\caption{Regression plot of $\log|Bias(\hat{H}_{N, 10})|$ against $\log(N)$, for the 1-dimensional distributions; exponential, uniform and normal}
  \label{E_U_N_k=10_graph}
\end{figure}

This graph shows ... TODO 





\subsubsection{Comparison of k} \label{E_compare_k}

In sections \ref{E_k=1} to \ref{E_k=10}, I have explored the Koazchenko-Leonenko estimator for samples from the 1-dimensional exponential distribution. 

smth about tables and bias with N

Additionally, it is important to note that in all the tables for the exponential distribution, we consistently have $Var(Bias(\hat{H}_{N, k}))$ decreases as $N \to \infty$. By Theorem \ref{paper_T4}, we know that $Var(\hat{H}_{N, k}) \approx \frac{Var(\log f(x))}{N}$, and for $N=50,000$ we have for this distribution that the variance of $Bias(\hat{H}_{N, k})$, for $k=1,2,3,5,10$ are given by $\approx 0.000058, 0.000043, 0.000031, 0.000025, 0.000024$ respectively. By the non-linearity of the variance we have that  $Var(Bias(\hat{H}_{N, k})) = Var(\hat{H}_{N,k} - H) = Var(\hat{H}_{N,k})$. For this distribution we have;
\begin{align}
Var(\log f(x)) = TODO... 
\end{align}



I will be now focusing on the results of the coefficients of equation (\ref{bias}), found from the simulations above;
\begin{equation}
|Bias(\hat{H}_{N, k})| = \frac{c_{k}}{N^{a_{k}}} \nonumber
\end{equation}
The results from the investigation above have been condensed into table \ref{expo_k_comparison_table}, showing the change in values of $a_{k}$ and $c_{k}$ for different $k$.

\begin{table}
\caption{1-dimensional exponential distribution, a comparison of $k$} \label{expo_k_comparison_table}
\begin{center}
\begin{tabular}{| l | c c|} 
\toprule
k & $a_{k}$ & $c_{k}$ \\
\midrule[1pt]
1 &  0.4147  &  0.0198  \\
2 &  0.6480  &  0.1482  \\
3 &  0.5711  &  0.0585  \\
5 &  0.4793  &  0.0241  \\
10 & 0.7170 &  0.2292 \\
\hline
\end{tabular}
\end{center}
\end{table}

This table doesn't show the same as the other distributions; previously, in sections \ref{U_compare_k} and \ref{N_compare_k}, we have found that $a_{1} < a_{2} < a_{3} < a_{5} < a_{10}$, similarly with the intercept coefficient we have found that $c_{1} < c_{2} < c_{3} < c_{5} < c_{10}$. However, here the coefficients at $k=3$ and $k=5$ are not as expected, they are smaller than those for $k=2$. To examine this more, I will find the coefficients of regression for estimators found when $k=4,6,7,8,9,11$, these are displayed in table \ref{expo_more_coeffs}.

\begin{table}
\caption{1-dimensional exponential distribution, a comparison of more $k$} \label{expo_more_coeffs}
\begin{center}
\begin{tabular}{| l | c c|} 
\toprule
k & $a_{k}$ & $c_{k}$ \\
\midrule[1pt]
1 &  0.4147  &  0.0198  \\
2 &  0.6480  &  0.1482  \\
3 &  0.5711  &  0.0585  \\
4 &  0.5914  &  0.0731 \\
5 &  0.4793  &  0.0241  \\
6 &  0.6515  &  0.1333  \\
7 &  0.6023  &  0.0772  \\
8 &  0.5384  &  0.0392  \\
9 &  0.7139  &  0.2202  \\
10 & 0.7170 &  0.2292 \\
\hline
\end{tabular}
\end{center}
\end{table}

This tables appears to show that the values of $a_{k}$ and $c_{k}$ for $k=2, 5, 8$ seem to be different to the trend shown with the other values of $k$ for this distribution and also for the other distributions; normal and exponential. To try and smooth out any of these errors, I will now consider $3,000$ (instead of the previous $500$) samples of size $N$, to then find the estimator and compute the relationship in equation (\ref{logbias}). This will be done too find the coefficients shown in table \ref{expo_more_coeffs} when taken as an average over a larger number of samples, for $k=2,5,8$, the results are as follows; TODO...

I can also compare the values found for $a_{k}$ and $c_{k}$ for the exponential distribution, to those found from the other 1-dimensional distributions previously considered, I have condensed this information into table \ref{expo_uniform_normal_comparison_table}.



\begin{table}
\caption{Comparison between 1-dimensional Exponential, Uniform and Normal distribution} \label{expo_uniform_normal_comparison_table}
\begin{center}
\begin{tabular}{| l | c c | c c | c c |}
\toprule
{ |} &  \multicolumn{2}{c |}{Normal} & \multicolumn{2}{c |}{Uniform} & \multicolumn{2}{c |}{Exponential}\\
\hline
$k$   &  $a_{k}$  &  $c_{k}$  &  $a_{k}$  &  $c_{k}$  &  $a_{k}$  &  $c_{k}$   \\
\midrule[1pt]
1      & 0.4594     & 0.0249 &  0.3698  &  0.0103  &  0.4147  &  0.0198  \\
2      & 0.5998     & 0.0746 &  0.5857  &  0.0503  &  0.6480  &  0.1482  \\
3      & 0.6443     & 0.1156 &  0.6291  &  0.0737  &  0.5711  &  0.0585 \\
5      & 0.7568     & 0.3557 &  0.7501  &  0.1889  &  0.4793  &  0.0241 \\
10    & 1.0055     & 5.5942 &  1.0357  &  3.8217  &  0.7170  &  0.2292 \\
\hline
\end{tabular}
\end{center}
\end{table}



\end{document}