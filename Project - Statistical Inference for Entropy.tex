\documentclass{article}
\usepackage[utf8]{inputenc}
\title{Statistical Inference for Entropy}
\author{Karina Marks}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{listings}

\begin{document}
\maketitle
\section{Introduction}



\section{Entropies and Properties}

Entropy can be thought of as a representation of the average information content of an observation; sometimes referred to as a measure of unpredictability or disorder. 

\subsection{Shannon Entropy}
The Shannon entropy of a random vector X with density function f is given by;
\begin{equation} \label{ShaEnt}
H = - \mathbb{E} \{log(f(x))\} = - \int_{x : f(x) > 0} f(x) log(f(x)) dx
\end{equation}

\subsection{ R\'enyi and Tsallis Entropy}
These entropies are for the order $q \neq 1$ and the construction of them relies upon the generalisation of the Shannon entropy \ref{ShaEnt}. For a random vector $X \in \mathbb{R}^d$ with density function f, we define;

R\'enyi entropy
\begin{equation} \label{RenEnt}
H_{q}^{*} = \frac{1}{1-q} log \left( \int_{\mathbb{R}^d} f^q (x) dx \right) \quad  \quad (q \neq 1)
\end{equation}

Tsallis entropy
\begin{equation} \label{TsaEnt}
H_{q} = \frac{1}{q-1} \left(1 - \int_{\mathbb{R}^d} f^q (x) dx \right)  \quad  \quad (q \neq 1)
\end{equation}
When the order of the entropy $q \to 1$, both the Renyi, (\ref{RenEnt}), and Tsallis, (\ref{TsaEnt}), entropies tend to the Shannon entropy, (\ref{ShaEnt}). 

\section{Estimation of Entropy}

\subsection{Kozachenko-Leonenko Estimator}

We now wish to introduce the Kozachenko-Leonenko estimator of the entropy H. Let $X_{1}, X_{2}, ... ,X_{N}$, $N \geq 1$ be independent and identically distributed random vectors in $\mathbb{R}^{d}$, and denote $\|.\|$ the Euclidean norm on $\mathbb{R}^{d}$.
 
\begin{itemize}

\item For $i = 1, 2, ..., N$, let $X_{(1), i}, X_{(2), i}, .., X_{(N-1), i}$ denote an order of the $X_{k}$ for $k = \{1, 2, ..., N\} \setminus \{i\}$, such that $\| X_{(1), i} - X_{i}\| \leq \cdots \leq \|  X_{(N-1), i} - X_{i}\| $. Let the metric $\rho$, defined as;
\begin{equation}
\rho_{(k), i} = \| X_{(k), i} - X_{i}\|
\end{equation} denote the kth nearest neighbour or $X_{i}$.

\item  For dimension d, the volume of the unit d-dimensional Euclidean ball is defined as;
\begin{equation}
V_{d} = \frac{\pi^\frac{d}{2}}{\Gamma(1 + \frac{d}{2})}
\end{equation}

\item For the kth nearest neighbour, the digamma function is defined as;
\begin{equation}
\Psi(k) = -\gamma + \sum_{j=1}^{k-1} \frac{1}{j}
\end{equation}
where $\gamma = 0.577216$ is the Euler-Mascheroni constant (where the digamma function is chosen so that $\frac{e^{\Psi(k)}}{k}\to1$ as $k \to \infty$).

\end{itemize} Then the Kozachenko-Leonenko estimator for entropy, H, is given by;
\begin{equation} \label{KLest}
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(k),i}^{d} V_{d} (N-1)}{e^{\Psi(k)}} \right]
\end{equation} This estimator for entropy, when $d \leq 3$, under a wide range of k and some regularity conditions, satisfies;
\begin{equation} \label{efficiency}
N \mathbb{E} {(\hat{H}_{N, k} - H)^2} \to 0 \quad  (N \to \infty)
\end{equation} so $\hat{H}_{N, k}$ is efficient in the sense that the asymptotic variance is the best attainable; $N^{\frac{1}{2}}(\hat{H}_{N, k} - H) \xrightarrow{d} N(0, Var[log(f(x))])$, the normal distribution with 0 mean and variance as shown.

Later, I will further discuss this estimator for the specific dimensions $d=1$ and $d=2$; however, it is important to note that for larger dimensions this estimator is not accurate. When $d=4$, equation( \ref{efficiency}) no longer holds but the estimator $\hat{H}_{N, k}$, defined by (\ref{KLest}), is still root-N consistent, provided k is bounded. Also, when $d \geq 5$ there is a non trivial bias, regardless of the choice of k. 

There is a new proposed estimator, formed as a weighted average of $\hat{H}_{N, k}$ for different values of k, explored in ...SOMEONE... . Moreover, this will not be examined here as this paper focuses only on the 1-dimensional samples;
\begin{equation} \label{KLestd=1}
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(k),i} V_{1} (N-1)}{e^{\Psi(k)}} \right]
\end{equation}
and the 2-dimensional;
\begin{equation} \label{KLestd=2}
\hat{H}_{N, k} = \frac{1}{N} \sum_{i=1}^{N} log \left[ \frac{\rho_{(k),i}^{2} V_{2} (N-1)}{e^{\Psi(k)}} \right]
\end{equation}



\subsection{Bias of the K-L estimator}
$\hat{H}_{N, k}$ is approximately an unbiased estimator for H; we wish to explore how approximate this is, by considering the bias of the estimator for entropy;
\begin{equation} \label{Bias}
Bias(\hat{H}_{N, k} ) = \mathbb{E}(\hat{H}_{N, k}) - H = \mathbb{E}(\hat{H}_{N, k} - H)
\end{equation}
To do this we consider the consistency and asymptotic bias of the estimator $\hat{H}_{N, k}$, ...SOMEONE... has explored this in detail thus the following theorems hold.



\section{Monte-Carlo Simulations}

In this section I will explore simulations of the bias of estimator (\ref{KLest}) in comparison to the size of the sample estimated from, with respect to different values of k; firstly exploring 1-dimensional distributions and then progressing onto 2-dimensional.

I will begin by exploring entropy of samples from the normal distribution $N(0, \sigma^2)$, where without loss of generality we can use the mean $\mu = 0$ and change the variance $\sigma^2$ as needed. The normal distribution has an exact formula to work out the entropy, given the variance $\sigma^2$. Using equation (\ref{ShaEnt}) and the density function for the normal distribution $f(x) = \frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)}$ for $x \in \mathbb{R}$, given $\mu = 0$, we can write the exact entropy for the normal distribution;
\begin{equation}
H = log(\sqrt{(2\pi e)}\sigma)
\end{equation}

The motivation for these simulations is to explore the consistency of this estimator for different values of $k$; the relationship between the size of the bias of the estimator $\hat{H}_{N, k}$, $Bias(\hat{H}_{N, k})$,  and the sample size, $N$. We believe this relationship is of the form;
\begin{equation}
|Bias(\hat{H}_{N, k})| = \frac{c}{N^a}
\end{equation}
for $a, c > 0$. By taking the logarithm of this, we can see that this relationship is in fact linear;
\begin{equation}
log|Bias(\hat{H}_{N, k})| = log(c) - a [log(N)]
\end{equation}
We will investigate the consistency of this estimator for a sample from the normal distribution, dependent on the value of $k$.

\subsection{1-dimensional Normal Distribution}

\subsubsection{k=1}


\subsubsection{k=2}




\end{document}
