\documentclass{article}
\usepackage[utf8]{inputenc}
\title{Statistical Inference for Entropy}
\author{Karina Marks}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{listings}

\begin{document}
\maketitle
\section{Introduction}



\section{Entropies and Properties}

Entropy can be thought of as a representation of the average information content of an observation; sometimes referred to as a measure of unpredictability or disorder. 

\subsection{Shannon Entropy}
The Shannon entropy of a random vector X with density function f is given by;
\begin{equation} \label{ShaEnt}
H = - \mathbb{E} \{log(f(x))\} = - \int_{x : f(x) > 0} f(x) log(f(x)) dx
\end{equation}

\subsection{ R\'enyi and Tsallis Entropy}
These entropies are for the order $q \neq 1$ and the construction of them relies upon the generalisation of the Shannon entropy \ref{ShaEnt}. For a random vector $X \in \mathbb{R}^d$ with density function f, we define;
\begin{itemize}
\item R\'enyi entropy
\begin{equation} \label{RenEnt}
H_{q}^{*} = \frac{1}{1-q} log \left( \int_{\mathbb{R}^d} f^q (x) dx \right) \quad  \quad (q \neq 1)
\end{equation}

\item Tsallis entropy
\begin{equation} \label{TsaEnt}
H_{q} = \frac{1}{q-1} \left(1 - \int_{\mathbb{R}^d} f^q (x) dx \right)  \quad  \quad (q \neq 1)
\end{equation}
\end{itemize}



When the order of the entropy $q \to 1$, both the Renyi, (\ref{RenEnt}), and Tsallis, (\ref{TsaEnt}), entropies tend to the Shannon entropy, (\ref{ShaEnt}). 

\section{Estimation of Entropy}

\subsection{Kozachenko-Leonenko Estimator}

We now wish to introduce the Kozachenko-Leonenko estimator of the entropy H. Let $X_{1}, X_{2}, ... ,X_{n}$, $N \geq 1$ be independent and identically distributed random vectors in $\mathbb{R}^{d}$, and denote $\|.\|$ the Euclidean norm on $\mathbb{R}^{d}$.
 
\begin{itemize}

\item For $i = 1, 2, ..., n$, let $X_{(1), i}, X_{(2), i}, .., X_{(n-1), i}$ denote an order of the $X_{k}$ for $k = \{1, 2, ..., n\} \setminus \{i\}$, such that $\| X_{(1), i} - X_{i}\| \leq \cdots \leq \|  X_{(n-1), i} - X_{i}\| $. Let the metric $\rho$, defined as;
\begin{equation}
\rho_{(k), i} = \| X_{(k), i} - X_{i}\|
\end{equation} denote the kth nearest neighbour or $X_{i}$.

\item  For dimension d, the volume of the unit d-dimensional Euclidean ball is defined as;
\begin{equation}
V_{d} = \frac{\pi^\frac{d}{2}}{\Gamma(1 + \frac{d}{2})}
\end{equation}

\item For the kth nearest neighbour, the digamma function is defined as;
\begin{equation}
\Psi(k) = -\gamma + \sum_{j=1}^{k-1} \frac{1}{j}
\end{equation}
where $\gamma = 0.577216$ is the Euler-Mascheroni constant (where the digamma function is chosen so that $\frac{e^{\Psi(k)}}{k}\to1$ as $k \to \infty$).

\end{itemize} Then the Kozachenko-Leonenko estimator for entropy, H is given by;
\begin{equation} \label{KLest}
\hat{H}_{n} = \frac{1}{n} \sum_{i=1}^{n} log \left[ \frac{\rho_{(k),i}^{d} V_{d} (n-1)}{e^{\Psi(k)}} \right]
\end{equation} This estimator for entropy, when $d \leq 3$, under a wide range of k and some regularity conditions, this estimator satisfies;
\begin{equation} \label{efficiency}
n \mathbb{E} {(\hat{H}_{n} - H)^2} \to 0 \quad  (n \to \infty)
\end{equation} so $\hat{H}_{n}$ is efficient in the sense that the asymptotic variance is the best attainable; $n^{\frac{1}{2}}(\hat{H}_{n} - H) \xrightarrow{d} N(0, Var[log(f(x))])$.

Later, I will further discuss this estimator for the specific dimensions $d=1$ and $d=2$; however, it is important to note that for larger dimensions this estimator is not accurate. When $d=4$, equation \ref{efficiency} no longer holds but the estimator $\hat{H}_{n}$, defined by \ref{KLest}, is still root-n consistent, provided k is bounded. Also, when $d \geq 5$ there is a non trivial bias, regardless of the choice of k. 

There is a new proposed estimator, formed as a weighted average of $\hat{H}_{n}$ for different values of k, explored in ... . Moreover, as this paper only considers $d=1, 2$, this will not be examined here.

\section{Monte-Carlo Simulations}

In this section I will explore simulations of the bias of estimator \ref{KLest} in comparison to the size of the sample estimated from, with respect to different values of k; firstly exploring 1-dimensional distributions and then progressing onto 2-dimensions.

I will begin by exploring entropy of samples from the normal distribution $N(0, \sigma^2)$, where without loss of generality we can use the mean $\mu = 0$ and change the variance $\sigma^2$ as needed. The normal distribution has an exact formula to work out the entropy, given the variance $\sigma^2$. Using equation \ref{ShaEnt} and the density function for the normal distribution $f(x) = \frac{1}{\sqrt{(2\pi)} \sigma}\exp{ \left( \frac{-x^2}{2\sigma^2} \right)}$ for $x \in \mathbb{R}$, given $\mu = 0$, we can write the exact entropy for the normal distribution;
\begin{equation}
H = log(\sqrt{(2\pi e)}\sigma)
\end{equation}





\end{document}
